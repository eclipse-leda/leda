[{"body":"Release artifacts: https://github.com/eclipse-leda/leda-distro/releases/tag/v0.1.0-M1\nArtifact Download Size Leda Raspberry Pi 4 eclipse-leda-raspberrypi.tar.xz 578 MB Leda QEMU ARM64 eclipse-leda-qemu-arm64.tar.xz 381 MB Leda QEMU x86_64 eclipse-leda-qemu-x86_64.tar.xz 465 MB Note: You need to uncompress eclipse-leda-raspberrypi.tar.xz multiple times until the plain .wic file is extracted, which can be flashed.\nAfter download, continue with Getting Started\nRelease Notes First pre-release of Eclipse Leda quickstart images, based on Yocto LTS release Kirkstone with Long Term Support until at least April 2024.\nMinimal feature set to run software-defined-vehicle applications (such as Eclipse Velocitas apps) using COVESA Vehicle Signal Specification and the Eclipse Kuksa.VAL Databroker on virtual devices (QEMU), in Docker, or on physical consumer-grade devices (Raspberry Pi 4 64-Bit) for development, demonstration and prototyping purposes.\nIncludes example applications from Kuksa:\nExample Seat Service Example HVAC Service DBC Feeder replays a Tesla Model 3 CAN-Dump and feeds it into Kuksa.VAL Databroker Change log Replaced k3s with Eclipse Kanto Container Management Replaced packages licensed under GPLv3 and similar licenses with alternatives Replaced Grub with U-Boot Replaced nano with kibi Removed readline library Removed bash RAUC Updates Bundles available for each image type (Full, Minimal, Rescue) Enabled verity format by default Fixed the compatible configuration string Added and updated Leda utilities Build infrastructure improvements Builds for Dockerized images Switched to a BitBake build using kas for layer management Added automated system tests using Robot framework and Dockerized environments General cleanup of recipes, dependencies and structuring of the meta-leda sublayers to improve reusability Automatic deployment of containers based on container manifests with ad-hoc updates (filewatcher in kanto-auto-deployer) Preparation for AirGap installation of containers General improvements, such as Wifi network management Known Issues The following issues were known to the development team before starting the 0.1.0-M1 build cycle. They have been prioritized as non-critical and may be fixed for later releases.\nRAUC can not install in stream-mode on Raspberry Pi sdv-motd is confused by wlan interfaces growdisk on RPi4 with sfdisk instead of parted (signaling kernel) Partition sizes are not multiple of 4096 byte for Rauc block-hash-index sdv-provision produces an empty Device ID Missing/Outdated Kuksa databroker-cli sdv-image-minimal missing SDV edge components Home/End key navigation does not work with putty OSS IP Compliance Report Report scan-report-web-app_0.1.0-M1.html\nIncorrectly detected license “biosl-4.0”\nRule: OCaaS Policy A9 License with no classification Message: The license LicenseRef-scancode-biosl-4.0 found for package ‘Unmanaged::leda-distro-fork:0716b55ff8f57319263d67ee16d90e64588b391d’ is not categorized and / or evaluated for usage. Evaluation: This license seems to be detected incorrectly by the tool being used, as it is an internal, proprietary license which is not used in the Eclipse Leda project. Incorrectly detected license “GPL-1.0” for ORT configuration file\nRule: OCaaS Policy C1 Strict Copyleft Message: License GPL-1.0-only found for package ‘Unmanaged::leda-distro-fork:0716b55ff8f57319263d67ee16d90e64588b391d’ is categorized as strict-copyleft which must not be used for BT11 Open Source Development service applications. Evaluation: The scan tool incorrectly detects its own configuration file (.ort.original.yml) as being licensed under GPL-v1.0 Incorrectly detected license “GPL-2.0-only” for standard Leda license header (which is Apache Software License)\nRule: OCaaS Policy C1 Strict Copyleft Message: License GPL-2.0-only found for package ‘Unmanaged::leda-distro-fork:0716b55ff8f57319263d67ee16d90e64588b391d’ is categorized as strict-copyleft which must not be used for BT11 Open Source Development service applications. Evaluation: The scan tool incorrectly detects the Apache License header as GPL-2.0 license text Incorrectly detected license “proprietary”\nRule: OCaaS Policy C3 Commercial Message: License LicenseRef-scancode-proprietary-license found for package ‘Unmanaged::leda-distro-fork:0716b55ff8f57319263d67ee16d90e64588b391d’ is categorized as commercial and requires special handling. Evaluation: The scan tool incorrectly detects its own configuration file (.ort.original.yml) as being licensed under proprietary licenses. ","categories":"","description":"","excerpt":"Release artifacts: …","ref":"/leda/docs/about/releases/0.1.0/leda-0.1.0-m1/","tags":"","title":"Milestone 0.1.0-M1"},{"body":"Kanto container management binds to a unix socket (default: /run/container-management/container-management.sock) and exposes a gRPC interface which can be used to obtain all the functionality of the kanto-cm cli programatically.\nThe easiest way to access this API through Rust is by creating a new Rust crate:\n$ cargo new talk-to-kanto Dependencies The most feature-rich gRPC library for Rust right now is tonic. Add the following to your Cargo.toml to make tonic and the tokio async runtime available to your crate. Tower and hyper are needed to be able to bind to the unix socket.\n[dependencies] prost = \"0.11\" tokio = { version = \"1.0\", features = [ \"rt-multi-thread\", \"time\", \"fs\", \"macros\", \"net\",] } tokio-stream = { version = \"0.1\", features = [\"net\"] } tonic = {version = \"0.8.2\" } tower = { version = \"0.4\" } http = \"0.2\" hyper = { version = \"0.14\", features = [\"full\"] } serde = { version = \"1.0.147\", features = [\"derive\"] } serde_json = { version = \"1.0.89\", default-features = false, features = [\"alloc\"] } [build-dependencies] tonic-build = \"0.8.2\" Compiling protobufs The easiest way to obtain the kanto-cm .proto files is to add the container management repo in your project root as a git submodule:\n$ git submodule init $ git submodule add https://github.com/eclipse-kanto/container-management.git $ git submodule update --init --recursive You should now have the container-management repository available.\nTo build the .proto files during compile time, define a custom build.rs in the project root\n$ touch build.rs Add the following main function to the build.rs:\nfn main() -\u003e Result\u003c(), Box\u003cdyn std::error::Error\u003e\u003e { tonic_build::configure() .build_server(false) .include_file(\"mod.rs\") .type_attribute(\".\", \"#[derive(serde::Serialize, serde::Deserialize)]\") .compile( \u0026[\"api/services/containers/containers.proto\"], \u0026[\"container-management/containerm/\"], )?; Ok(()) } Here it is important to know that tonic does not like deeply nested protobufs such as those for kanto-cm. That is why the line .include_file(\"mod.rs\") re-exports everything in a seperate module which can later be included in the main.rs file.\n\"#[derive(serde::Serialize, serde::Deserialize)]\" makes all structures (de-)serializable via serde.\nImporting generated Rust modules Now in src/main.rs add the following to import the generated Rust modules:\npub mod cm { tonic::include_proto!(\"mod\"); } use cm::github::com::eclipse_kanto::container_management::containerm::api::services::containers as cm_services; use cm::github::com::eclipse_kanto::container_management::containerm::api::types::containers as cm_types; Now all kanto-cm services as namespaced under cm_services.\nObtaining a unix socket channel To obtain a unix socket channel:\nuse tokio::net::UnixStream; use tonic::transport::{Endpoint, Uri}; use tower::service_fn; let socket_path = \"/run/container-management/container-management.sock\"; let channel = Endpoint::try_from(\"http://[::]:50051\")? .connect_with_connector(service_fn(move |_: Uri| UnixStream::connect(socket_path))) .await?; This is a bit of a hack, because currently, tonic+tower don’t support binding directly to an unix socket. Thus in this case we attemp to make an http connection to a non-existent service on port 5051. When this fails, the fallback method connect_with_connector() is called where a tokio UnixStream is returned and the communication channel is generated from that.\nMaking a simple gRPC call to kanto-cm All that is left is to use the opened channel to issue a simple “list containers” request to kanto.\n// Generate a CM client, that handles containers-related requests (see protobufs) let mut client = cm_services::containers_client::ContainersClient::new(channel); let request = tonic::Request::new(cm_services::ListContainersRequest {}); let response = client.list(request).await?; Since we made all tonic-generated structures (de-)serializable we can use serde_json::to_string() to print the response as a json string.\nprintln!(\"{}\", serde_json::to_string(\u0026response)?); ","categories":"","description":"","excerpt":"Kanto container management binds to a unix socket (default: …","ref":"/leda/docs/build/dev-and-maintenance/rust/notes-on-kanto-grpc/","tags":"","title":"Communicating with Кanto-CM via gRPC"},{"body":"The Leda project aims to provide a BitBake meta-layer for building custom images within the SDV context. The Leda quickstart image serves as an experimentation and development platform but is not production-ready. The focus of the meta-leda meta-layer is to offer reusable components for custom SDV images.\nThis documentation section is aimed at anyone who wishes to build such a custom image.\nHardware The Leda Quickstart image directly provides BSP-recipes for the following platforms:\nQEMUx86-64 (64 bit) QEMUARM64 (64 bit) QEMUARM (32 bit) Raspberry Pi 4B (64 bit) Other hardware would require customizing your image with additional BSP-layers.\nkas and meta-layers We recommend (and use for the quickstart image) the kas tool developed by Siemens as a way to help with reproducible builds. It is, however, not required and you would still need to define your own distro meta-layer on top of meta-leda. The Leda quickstart image kas-files can be found here: leda-distro/kas. Most of the distro-configuration can be found in the common-kirkstone.yaml file.\nOther than meta-leda you will need the following external meta-layers:\nPoky - The base poky distro meta-openembedded - Busybox, systemd, and other core packages meta-rauc - RAUC integration meta-kanto - Kanto container management meta-virtualization - containerd From here on we will assume that you are working with kas as your main build tool.\nSetting up your custom image Create a new repository for your distro. Create a top-level container meta-layer that would contain your distro-specific configurations which will be explained below.\nIn the root of the repository create a .config.yaml that would be used by kas to build your image. The minimal kas-config file can be found in the tool’s official documentation Project Configuration.\nReferencing leda-distro/kas/common-kirkstone.yaml, add to this minimal configuration all required meta-layers from above and sub-layers from meta-leda you will need.\nAdd your custom container layer (you can reference meta-layers by path in kas) to the kas-config. Reference: meta-leda/kas/.config-components.yaml.\nFrom this point on you can use kas build from the root of your repository or kas build /path/to/.config.yaml from anywhere to start the build process.\nGeneral distro configuration The place you should start is by configuring your custom distribution. The BitBake mechanism for doing so can be found in the official BitBake documentation: 22 Creating Your Own Distribution.\nAs a working example, you can use the Leda-distro config file: meta-leda/meta-leda-distro/conf/distro/leda.conf. We will now go through some of the lines in the leda.conf:\nrequire conf/distro/include/buildinfo.inc # -\u003e Defines entries for /etc/build require conf/distro/include/leda-distro-features.inc # -\u003e Defines the quickstart image features. Use it as a guide for your distro. require conf/distro/include/leda-package-blacklist.inc # -\u003e Defines blacklisted packages (e.g. alsa) include conf/machine/${MACHINE}-extra.conf # -\u003e Defines ${MACHINE}-specific configuration files FSTAB, wks, etc. Would be elaborated on further down. QB_KERNEL_CMDLINE_APPEND = \"net.ifnames=0 panic=5 ip=dhcp ip=192.168.7.2::192.168.7.1:255.255.255.0::eth0:off rootwait\" # -\u003e QEMU-specific kernel command line with which all images/partitions are booted The only required contents of your distro-config file are the following (all other includes and distro features are optional):\n# in the main leda.conf DISTRO=\"\u003cdistro_name\u003e\" # should be the same as the \u003cdistro_name\u003e.conf DISTRO_NAME=\"\u003clong_distro_name\u003e\" DISTRO_VERSION=\"\u003cversion\u003e\" DISTRO_CODENAME=\"\u003cdistro_codename\u003e\" # imported with require conf/distro/include/leda-distro-features.inc in Leda Quickstart DISTRO_FEATURES:append = \" virtualization\" # required for containerd DISTRO_FEATURES:append = \" sdv\" # required by the sdv-packagegroups DISTRO_FEATURES:append = \" rauc\" # required for the rauc integration Machine-specific configs The “extra” machine specific-configs the meta-leda-bsp layer provides can be found here: meta-leda-bsp/conf/machine. These files here provide examples on how to configure machine specific options such as the Linux Kernel image type, bootloader configuration, etc. E.g. if you need a working example of how to set up U-Boot for your machine, you might reference common-qemu-arm.inc since the Leda quickstart QEMUARM (64bit and 32bit) images use U-Boot as a bootloader.\nPartitioning your storage The Leda quickstart image uses RAUC as a way to do A/B partition type updates. We also add a third “rescue” partition and a persistent “data” partition. Generally, the Leda quickstart image partitions its storage as so:\n---------------------------------------------------------------------------------- | | | | | | | | | | | | | | | VFAT | NO-FS | EXT4 | EXT4 | EXT4 | EXT4 | | (BOOT) | (GRUBENV) | (RESCUE) | (SDV_A) | (SDV_B) | (DATA) | | | | | | | | | | | | | | | ---------------------------------------------------------------------------------- Where the partitions SDV_A and SDV_B are managed/updated by RAUC. They are read-only (SDV_A has the sdv-image-full image installed, SDV_B - sdv-image-minimal). All container/user data is in the last DATA partition (grows dynamically to fill out the storage). The boot partition contains the U-Boot/Grub bootloaders and their environment.\nGRUBENV is reserved for future boot-partition updates and saving the GRUB/U-Boot environment. Currently unused in the Leda quickstart image.\nIn the RESCUE partition the sdv-image-rescue image is installed, which is a “fallback” image for recovering the system if both partitions SDV_A and SDV_B get corrupted.\nThis partition scheme has to be seen, as of course, a suggestion (you are free to use any RAUC-compatible one) but is important for understanding the later sections on this page.\nWIC/WKS To partition your storage you will need to define your custom .wks-file that defines the partition table type, the bootloader, and the order and size of partitions.\nMore details on OpenEmbedded Kickstart files can be found in the official documentation for Yocto: OpenEmbedded Kickstart (.wks) Reference. In short, .wks-files provide a reproducible way to partition your storage.\nIMPORTANT: the path to your .wks-file should be provided via the WKS_FILES= BitBake variable. For example, in the quickstart image for QEMUARM64 this variable is set in qemuarm64-extra.conf.\nAgain, example .wks/.wks.in files are provided in meta-leda/meta-leda-distro/wic. We will now take a look at the qemuarm64 .wks-file:\nbootloader --ptable gpt part --source bootimg-partition --ondisk vda --fstype=vfat --label BOOT --active --align 4096 --size 100 --use-uuid # Second Bootloader Partition (empty, for future updates) part --fixed-size 10M --ondisk vda --align 4096 --no-table # Empty partition (on x86, this is grubenv - we store RAUC Status here as well) part --fixed-size 10M --ondisk vda --align 4096 part --source rootfs --rootfs-dir=sdv-image-rescue --ondisk vda --fstype=ext4 --label rescue --align 1024 part --source rootfs --rootfs-dir=sdv-image-full --ondisk vda --fstype=ext4 --label root_a --align 4096 part --source rootfs --rootfs-dir=sdv-image-minimal --ondisk vda --fstype=ext4 --label root_b --align 4096 part --fixed-size 2048M --source rootfs --rootfs-dir=sdv-image-data --ondisk vda --fstype=ext4 --label data --align 4096 As you can see it directly implements the partition scheme described above. The partition table type is defined with the bootloader --ptable gpt line and every line after that corresponds to a partition from the diagram above. Please note, that the order in which the part lines in the .wks-file appear would be the order of the partitions.\nHere it is important to note that for the VFAT boot partition the label should be uppercase and you should add the --use-uuid option. Otherwise, it might not be mounted properly in the final Linux distro.\nFSTAB .wks-files partition your storage, define partition tables, filesystem types and install images, but do nothing about actually mounting said partitions in the Linux userspace. This is the job of the FSTAB file. More information about writing a FSTAB file can be found in the Linux FSTAB manpage.\nFSTAB files are installed through the OE-core recipe base-files_\u003cversion\u003e.bb. To install a custom FSTAB in your image create a recipes-core/base-files directory in your container meta-layer and create a base-files_%.bbappend inside. Use this .bbappend to prepend the directory containing your custom FSTAB to the FILESEXTRAPATHS list for that recipe.\nAn example can be found in meta-leda/meta-leda-distro/recipes-core/base-files/ where you would find the needed .bbappend file and the meta-leda/meta-leda-distro/recipes-core/base-files/base-files/\u003cmachine\u003e directory containing the actual FSTAB files.\nLet’s investigate the Leda qemuarm64 FSTAB as an example:\n# Eclipse Leda - fstab for qemuarm64 RAUC redundant boot setup /dev/root / auto defaults,noatime 1 1 proc /proc proc defaults 0 0 devpts /dev/pts devpts mode=0620,ptmxmode=0666,gid=5 0 0 tmpfs /run tmpfs mode=0755,nodev,nosuid,strictatime 0 0 tmpfs /var/volatile tmpfs defaults 0 0 # Add mount for boot, grubenv and data partition LABEL=BOOT /boot vfat defaults,nofail,noatime 0 0 LABEL=grubenv /grubenv auto defaults,nofail,noatime 0 0 LABEL=data /data auto defaults,nofail,noatime 0 2 Here only the BOOT, grubenv, and data partitions are mounted by label.\nThe root partition depends on the image (sdv-image-full/minimal/rescue) that has been currently booted. Here /dev/root is a “special variable” that is set from the root= option in the kernel command line. This is important to be able to boot from the different partitions via a custom bootloader script (and as a consequence these partitions are to be managed by RAUC).\nIntegrating RAUC Whether you are using GRUB or U-Boot, integrating RAUC requires custom bootloader scripting. The official documentation for integrating RAUC can be found here: 6. Integration.\nThis documentation can, however, be quite complicated. Concise, self-contained examples for proper integration of the most popular targets can be found in the meta-rauc-community repository.\nThe RAUC system config file Irrespective of your bootloader, you need to provide a RAUC system.conf file describing the slots, the bootloader, certificates, etc. This is best explained via an example: meta-leda/meta-leda-bsp/recipes-bsp/rauc/files/qemuarm64/system.conf\nGRUB The only Leda quickstart image that uses GRUB as a bootloader is for qemux86_64. It directly uses the meta-rauc-community/meta-rauc-qemux86/ meta-layer.\nU-Boot Note: The following sub-sections are based on the official U-Boot source which the OE recipes use. Your BSP may provide another hardware-specific fork of U-Boot that may or may not be entirely compatible with this integration guide. (e.g. fw_utils not being available)\nThe rpi4-64 target, similar to qemux86_64, directly uses meta-rauc-community/meta-rauc-raspberrypi which also has a great README, explaining the details of the integration. This meta-layer includes quite a lot of rpi4-64 specific recipes and depends on the fact that the meta-raspberrypi layer provides a recipe for custom boot scripts. Such a recipe, in general, for U-Boot-based targets is not available and has to be manually created. That is why, meta-rauc-community/meta-rauc-sunxi might provide a better example in a more general case.\nThe Leda quickstart images that use U-boot as a bootloader are based on the ideas in meta-rauc-sunxi. We will now go into more detail on how to integrate such a U-Boot target that requires a more “from-scratch” integration.\nThe U-Boot environment and “slot-counting” RAUC “talks” to the bootloader and tracks how many times you have tried to boot a slot via the bootloader environment variables “BOOT_ORDER” and “BOOT_\u003cSLOT_NAME\u003e_LEFT”. Thus, U-Boot should be able to save its environment in a file (uboot.env) somewhere that can be read by RAUC from userspace (the booted Linux image). This can be the VFAT BOOT partition, a flash chip, EEPROM, etc. which can be mounted in userspace and read by RAUC.\nNote that while most of the steps for each of these storage options are the same, they might need some specific configuration. For example, the build for the Leda Quickstart QEMUARM64 patches the U-Boot defconfig file to point U-Boot to save its environment in the VFAT BOOT partition env_in_fat_qemuarm.patch. You can instead directly provide a custom defconfig in your machine-specific config (ref: qemuarm64-extra.conf).\nuboot_%.bbappend After ensuring that the device containing the U-Boot environment is correctly mounted in userspace (e.g. the BOOT partition as /boot). Create a custom uboot_%.bbappend file and as an example, you can use meta-leda/meta-leda-bsp/recipes-bsp/uboot/uboot-targets/qemuarm64.inc. This .bbappend should, generally do two things:\nTake your custom boot.cmd.in script and compile it to a boot.scr (or boot.scr.uimg) file with mkimage.\nInstall the fw_env.config file in /etc/fw_env.config.\nOptional: Apply any patches, custom settings, etc.\nNote: if you are building for more than one U-Boot-based target extensive use of machine-specific overrides is recommended.\nCustom boot.scr If a custom boot.scr that was compiled with the mkimage tool is in the same partition as the U-Boot binary, U-Boot will run that on boot. This is the point of integration of RAUC and U-Boot. All concepts behind setting up such a script are explained in the rauc documentation.\nGenerally, as a starting point, you can use one of the custom scripts in meta-rauc-community or those in meta-leda: meta-leda/meta-leda-bsp/recipes-bsp/uboot/files/qemuarm64/boot.cmd.in.\nIMPORTANT: Be consistent with your slot names!\nAs it can be seen in the examples, after the “slot retries counting” part of the script is done, the root= and rauc.slot= kernel command line arguments are set, the U-Boot environment is saved in the VFAT boot partition as uboot.env and the selected image is booted.\nThen, from userspace RAUC reads the kernel command line (/proc/cmdline) and the uboot.env (through the fw_printenv/fw_setenv utilities) and decides (based on that) whether a slot is “Good” or “Bad”.\nfw_env.config For RAUC to be able to read and modify the U-Boot environment from userspace, it needs the fw_printenv/fw_setenv utilities to be installed in the distro. This can be done by installing the packages: \"u-boot-fw-utils u-boot-env libubootenv\" through IMAGE_INSTALL or RDEPENDS in a recipe.\nThis, however, is not enough since these utilities, in turn, have to be configured to know where U-Boot stores its environment. This is done through the /etc/fw_env.config file. An example of such a file with all of its different variations can be seen here fw_env.config.\nThe one for Leda quickstart contains a single line:\n/boot/uboot.env 0x0000 0x4000 If you are unsure of the environment size and the device offset, check the defconfig file for your machine, where these two values should be specified as U-Boot compile-time configuration parameters.\nConclusion (U-Boot \u003c-\u003e RAUC integration checklist) Given the following:\nYou have setup U-Boot to save its environment in a known place (device, partition, etc).\nThis device storing the U-Boot environment is mounted in userspace.\nYou have provided a custom boot-script that does the RAUC slot counting.\nYou have installed fw_printenv/fw_setenv and properly setup the /etc/fw_env.config file.\nThe RAUC mark-good-service should successfully go through and mark both RAUC slots as “GOOD”.\nFinal steps - building and flashing Build:\nRun kas build in the root of your repository and wait for the build to finish successfully. If any BitBake errors occur during the build process, you need to fix them before a final flashable image can be built.\nFlash:\nObtain the image-name.wic.bz2 and image-name.wic.bmap files from the tmp/deploy/images directory. Mount your storage and use bmaptool to quickly flash the built image. More information on flashing can be found in the Running on Raspberry Pi section of this documentation.\nIf everything works as intended you should see in the U-Boot output that it found /boot.scr and started executing it. This should lead to a successful boot of your custom Linux distro image with the RAUC-mark-good.service being marked as [OK] in the systemd logs (if your distro uses systemd).\n","categories":"","description":"","excerpt":"The Leda project aims to provide a BitBake meta-layer for building …","ref":"/leda/docs/customization/custom-images/","tags":"","title":"Custom Distros"},{"body":"","categories":"","description":"","excerpt":"","ref":"/leda/docs/build/dev-and-maintenance/rust/","tags":"","title":"Rust Utils"},{"body":"A general utility for monitoring the status of important sdv services/containers/devices.\nChecking the status of kanto-cm containers Kanto CM containers are split into two groups - required and optional. Both groups are checked, but only a warning is issued when an optional container is missing/not working.\nGeneral code for checking the status is:\nif [ -n \"$KANTO_CM_CONTAINERS_OPT\" ]; then printf -- \"$SEPARATOR\\n\" printf -- \"${COL_WHITE}[Kanto CM Containers (OPTIONAL)]${COL_NC}\\n\" if [[ ${CM_STATUS} != *\"inactive\"* ]]; then # \"Optional containers\" KANTO_CM_LIST=$(${KANTO_CMD} list) # removes tabs, splits on pipe and takes the container name column ($2) FOUND_CONTAINERS=($(echo \"$KANTO_CM_LIST\" | awk -F'|' '{gsub(/\\t/, \"\"); print $2}')) # array with all kanto container names # removes tabs, splits on pipe and takes the container status colum ($4) FOUND_CONTAINERS_STATES=($(echo \"$KANTO_CM_LIST\" | awk -F'|' '{gsub(/\\t/, \"\"); print $4}')) # array with all kanto container states KANTO_CM_CONTAINERS_ARR=( $KANTO_CM_CONTAINERS_OPT ) for expectedCtr in ${KANTO_CM_CONTAINERS_ARR[@]}; do CTR_IDX=$(get_array_element_index ${expectedCtr} ${FOUND_CONTAINERS[@]}) if [ ! -z $CTR_IDX ]; then status=${FOUND_CONTAINERS_STATES[$CTR_IDX]} if [ \"$status\" = \"Running\" ]; then printf \" * %-40s : $TEXT_OK\\n\" \"${expectedCtr}\" else printf \" * %-40s : $TEXT_WARN (%s)\\n\" \"${expectedCtr}\" \"$status\" fi else printf \" * %-40s : $TEXT_WARN (%s)\\n\" \"${expectedCtr}\" \"NOT FOUND\" fi done else printf \" * %-40s : $TEXT_FAIL (%s)\\n\" \"Kanto Container Management\" \"Unavailable\" fi fi Here it is important to know that kanto-cm list outputs the list of containers in a different order every time it’s called. That is why, kanto-cm list is invoked once and its output is stored in a variable:\nKANTO_CM_LIST=$(${KANTO_CMD} list) Output:\nID |Name |Image |Status |Finished At |Exit Code | ------------------------------------- |------------------------------------- |------------------------------------------------------------ |---------- |------------------------------ |---------- | d82a406e-80d7-4d2c-8044-3799544fc39a |vum |ghcr.io/eclipse-leda/leda-contrib-vehicle-update-manager/vehicleupdatemanager:main-1d8dca55a755c4b3c7bc06eabfa06ad49e068a48 |Running | |0 | 0f079856-767c-4e8d-b4df-a2323392849f |cloudconnector |ghcr.io/eclipse-leda/leda-contrib-cloud-connector/cloudconnector:main-47c01227a620a3dbd85b66e177205c06c0f7a52e |Exited |2023-01-31T11:58:01.564126452Z |1 | e4cf317e-c2d3-42c7-8f12-8ecf6f9d5d7a |databroker |ghcr.io/eclipse/kuksa.val/databroker:0.2.5 |Running | |0 | 6440a9b6-4fb8-4735-b3de-484286ac705b |feedercan |ghcr.io/eclipse/kuksa.val.feeders/dbc2val:v0.1.1 |Running | |0 | efbd572b-3331-4f19-9b17-7c69511ec5ca |hvacservice-example |ghcr.io/eclipse/kuksa.val.services/hvac_service:v0.1.0 |Running | |0 | 6d9a6f07-1659-4b51-9ddb-6e9ade64f2fd |seatservice-example |ghcr.io/eclipse/kuksa.val.services/seat_service:v0.1.0 |Running | |0 | 06b0ddf2-7c91-41e4-9a00-4213ee361cdf |sua |ghcr.io/eclipse-leda/leda-contrib-self-update-agent/self-update-agent:build-12 |Running | |0 | So we use awk to split on pipe (column), strip unecessary tabs. print $2 then gives us the container name and print $4 - its status.\nsdv-health then proceeds to check if every container specified in the list is available and if its status is Running.\nNote: Exited is considered a fail-state.\nChecking kanto-cm socket This is a simple test -s check for the default socket path.\n","categories":"","description":"","excerpt":"A general utility for monitoring the status of important sdv …","ref":"/leda/docs/build/dev-and-maintenance/shell/sdv-health/","tags":"","title":"sdv-health"},{"body":"","categories":"","description":"","excerpt":"","ref":"/leda/docs/build/dev-and-maintenance/shell/","tags":"","title":"Shell Utils"},{"body":"The Eclipse Leda project will provide system image “recipes” to deliver a functional Linux-based image/distribution in the context of SDV (Software Defined Vehicle), by pulling together individual contributons from the SDV and the larger OSS community.\nThe Eclipse Leda distribution will work with build scripts, package definitions, image build pipelines, etc, with the goal to pull SDV projects and dependecies from the larger OSS community into a working Linux system. Such system images (or other useful forms of delivery, as determined by the project) will be made available for consumption for anyone who is interested in working with the SDV tech stack. These deliveries take the form of container (base) images, installable/flashable image files, etc (again to be evolved by the project team according to community needs). Also in scope is concise and useful documentation for consumers of the project’s deliverables, plus a method of delivering that documentation.\nIn the context described above - the ambition of SDV to build a technology ecosystem for software-defined vehicle concern - a prime challenge will be the combination of these initially diverse components into a coherent and useful whole: all the software components in the world will not have the impact needed to transform the automotive industry unless we can make them play together coherently an form a functional portfolio. As a first step towards that goal, this proposal (Eclipse Leda) is for a “SDV distribution” project that pulls together individual contributor pieces from SDV and the larger OSS community, to deliver a functional and always-available Linux-based image/distribution with two primary goals:\nbe the crystalization point for functionally integrating diverse SDV-scope projects into a working whole\ndeliver a continually available, always working starting image for developers interested in getting going with the SDV tech stack\n","categories":"","description":"","excerpt":"The Eclipse Leda project will provide system image “recipes” to …","ref":"/leda/docs/about/","tags":"","title":"About Leda"},{"body":"Git Authentication For private repositories, we need to separately authenticate against the submodule repositories, as GitHub Codespaces will only inject a token with access rights to the current repository.\nChange to the users home directory\ncd ~ Install Git Credential Manager\ncurl -LO https://raw.githubusercontent.com/GitCredentialManager/git-credential-manager/main/src/linux/Packaging.Linux/install-from-source.sh \u0026\u0026 sh ./install-from-source.sh \u0026\u0026 git-credential-manager-core configure Configure a credential store typ, e.g. git config --global credential.credentialStore plaintext\nVerify with git config --global -l, it should show git-credential-manager-core as the credential helper.\nUpdate the submodules Run git submodule update --recursive\nSee VSCode Issue #109050 for details.\nSetup skopeo Skopeo is needed to download various files during the build:\nsudo mkdir -p /run/containers/1000 sudo chmod a+w /run/containers/1000 skopeo login ghcr.io --authfile ~/auth.json --username \u003cyour GitHub User\u003e Enter your token when asked for the password.\n","categories":"","description":"","excerpt":"Git Authentication For private repositories, we need to separately …","ref":"/leda/docs/build/devenv/github-codespaces/github-codespaces-advanced/","tags":"","title":"Advanced topics"},{"body":"Note: The configuration mentioned in this chapter is already enabled in the run-leda.sh script.\nQEMU General documentation about using CAN-Bus in Qemu: https://www.qemu.org/docs/master/system/devices/can.html\nEnabling Virtual CAN Bus interfaces (vcan) No special parameters are necessary for qemu, as vcan is virtual:\nrunqemu qemux86-64 nographic slirp qemuparams=\"-m 2048\" Bring interface up:\nip link add dev vcan0 type vcan ip link set vcan0 up Enabling CAN Bus interfaces (can) Standalone CAN within Qemu To run a standalone CAN setup, qemu must be instructed to emulate a specific CAN hardware device. We will be using the kvaser_pci device in this example:\nrunqemu qemux86-64 nographic slirp qemuparams=\"-m 2048 -object can-bus,id=canbus0 -device kvaser_pci,canbus=canbus0\" After the image has booted, load the Linux Kernel Module kvaser_pci device driver and configure the CAN-Bus device (eg bitrate) before bringing the interface up:\nroot@qemux86-64:~# modprobe kvaser_pci root@qemux86-64:~# dmesg | grep kvaser [ 9.565149] kvaser_pci 0000:00:04.0: initializing device 10e8:8406 [ 9.569308] kvaser_pci 0000:00:04.0: reg_base=00000000d5a68095 conf_addr=000000002b3c7ef6 irq=20 [ 9.596942] kvaser_pci 0000:00:04.0: xilinx version=13 number of channels=0 root@qemux86-64:~# ip link show type can 4: can0: \u003cNOARP,ECHO\u003e mtu 16 qdisc noop state DOWN mode DEFAULT group default qlen 10 link/can Configure the interface:\nroot@qemux86-64:~# ip link set can0 type can bitrate 1000000 [ 165.519919] kvaser_pci 0000:00:04.0 can0: setting BTR0=0x00 BTR1=0x14 root@qemux86-64:~# ip link set can0 up [ 186.906065] IPv6: ADDRCONF(NETDEV_CHANGE): can0: link becomes ready root@qemux86-64:~# ip link show type can 4: can0: \u003cNOARP,UP,LOWER_UP,ECHO\u003e mtu 16 qdisc pfifo_fast state UP mode DEFAULT group default qlen 10 link/can Tunneling a CAN Interface from the Host runqemu qemux86-64 nographic slirp qemuparams=\"-m 2048 -object can-bus,id=canbus0 -object can-host-socketcan,id=canhost0,if=can0,canbus=canbus0 -device kvaser_pci,canbus=canbus0\" Bring interface up:\nip link add dev can0 type can ip link set can0 type can bitrate 1000000 ip link set can0 up ip link show type can Raspberry Pi CAN HAT Extensions Supported boards:\nBoards with a Microchip MCP251x based CAN chip, such as Waveshare CAN HAT or PiCAN 2 Verify driver is loaded:\n# dmesg | grep mcp [ 8.23543] mcp251x spi0.0 can0: MCP2515 successfully initialized Verify SocketCAN network interface shows up:\n# ip link show type can 3: can0: \u003cNOARP,ECHO\u003e mtu 16 qdisc noop state DOWN mode DEFAULT roup default qlen 10 Continue with configuring the CAN chip and bring up the SocketCAN network interface:\n# ip link set can0 type can bitrate 1000000 # ip link set can0 up # ip link show type can Linux Kernel Modules The following Linux Kernel modules are available on the quickstart images:\nNote: For QEMU, only kvaser_pci is used\nLeda main Kernel peak_pciefd - Socket-CAN driver for PEAK PCAN PCIe/M.2 FD family cards m_can - CAN bus driver for Bosch M_CAN controller m_can_pci - CAN bus driver for Bosch M_CAN controller on PCI bus m_can_platform - M_CAN driver for IO Mapped Bosch controllers softing - Softing DPRAM CAN driver cc770_platform - Socket-CAN driver for CC770 on the platform bus cc770_isa - Socket-CAN driver for CC770 on the ISA bus cc770 - cc770CAN netdevice driver ifi_canfd - CAN bus driver for IFI CANFD controller kvaser_usb - CAN driver for Kvaser CAN/USB devices etas_es58x - Socket CAN driver for ETAS ES58X USB adapters ucan - Driver for Theobroma Systems UCAN devices peak_usb - CAN driver for PEAK-System USB adapters kvaser_pciefd - CAN driver for Kvaser CAN/PCIe devices kvaser_pci - Socket-CAN driver for KVASER PCAN PCI cards f81601 - Fintek F81601 PCIE to 2 CANBUS adaptor driver sja1000_isa - Socket-CAN driver for SJA1000 on the ISA bus plx_pci - Socket-CAN driver for PLX90xx PCI-bridge cards with the SJA1000 chips sja1000 - sja1000CAN netdevice driver ems_pci - Socket-CAN driver for EMS CPC-PCI/PCIe/104P CAN cards peak_pci - Socket-CAN driver for PEAK PCAN PCI family cards sja1000_platform - Socket-CAN driver for SJA1000 on the platform bus vxcan - Virtual CAN Tunnel c_can_platform - Platform CAN bus driver for Bosch C_CAN controller c_can - CAN bus driver for Bosch C_CAN controller c_can_pci - PCI CAN bus driver for Bosch C_CAN/D_CAN controller slcan - serial line CAN interface can_dev - CAN device driver interface vcan - virtual CAN interface can-isotop - PF_CAN isotp 15765-2:2016 protocol can-gw - PF_CAN netlink gateway can-j1939 - PF_CAN SAE J1939 can-raw - PF_CAN raw protocol can-bcm - PF_CAN broadcast manager protocol can - Controller Area Network PF_CAN core Raspberry Pi The following Linux Kernel modules are available on the quickstart image for Raspberry Pi:\ncan - Controller Area Network PF_CAN core vxcan - Virtual CAN Tunnel can-dev - CAN device driver interface can-bcm - PF_CAN broadcast manager protocol can-gw - PF_CAN netlink gateway can-raw - PF_CAN raw protocol can-isotop - PF_CAN isotp 15765-2:2016 protocol can-j1939 - PF_CAN SAE J1939 vcan - virtual CAN interface slcan - serial line CAN interface mcp251x - Microchip 251x/25625 CAN driver mcp251xfd - Microchip 251xFD Family CAN controller driver ems_usb - CAN driver for EMS Dr. Thomas Wuensche CAN/USB interfaces gs_usb - Socket CAN device driver for Geschwister Schneider UG peak_usb - CAN driver for PEAK-System USB adapters ","categories":"","description":"","excerpt":"Note: The configuration mentioned in this chapter is already enabled …","ref":"/leda/docs/general-usage/running-qemu/canbus/","tags":"","title":"CAN Bus"},{"body":"Simulation of cloud connectivity Container Management starts and subscribes to edge/thing/response Cloud Connector starts and publishes the following message to edge/thing/response as soon as the connection is online: { \"deviceId\":\"\u003cnamespace\u003e:\u003cgatewayId\u003e:\u003cdeviceId\u003e\", \"tenantId\":\"\u003ctenantId\u003e\" } namespace is azure.edge for Kanto’s Azure Cloud Connector gatewayId indicates the hostname of the Azure IoT Hub deviceId is the identifier for the device, this can either be part of the Azure Connection String or part of the device authentication certificate (CN) tenantId is a configuration setting in the cloud connector Note: You can simulate the cloud connector trigger by issueing the MQTT message manually on command line:\nmosquitto_pub -t 'edge/thing/response' -m '{\"deviceId\":\"dummy-namespace:dummy-gateway:dummy-device-id\",\"tenantId\":\"dummy-tenant-id\"}' ","categories":"","description":"","excerpt":"Simulation of cloud connectivity Container Management starts and …","ref":"/leda/docs/device-provisioning/container-management/cloud-connectivity/","tags":"","title":"Cloud Connectivity"},{"body":"Setting up Development Environment in GitHub Codespaces Install the GitHub Codespaces Extension Note: When using our DevContainer, the GitHub Codespaces extension is pre-installed.\nStart VSCode Go to Extensions Search for “GitHub Codespaces” Click Install Alternatively, create a new codespace via the GitHub web interface:\nSelect a big enough machine type for Yocto/BitBake, e.g. 16 CPU. You need at leasst 50GB disk space.\nBuilding Leda in a Github Codespace After successfully obtaining and connecting to a codespace you can build Leda either with kas or manually:\nTo build with kas follow the instructions at: Building with kas\nTo build manually: Building manually\nPrivate Repositories When using GitHub Codespaces with submodules and private repositories, a separate tool for git authentication is required (see VSCode issue #109050), as the authentication token provided to the GitHub Codespaces virtual machine only allows access to the main repository.\nGit Credential Manager: https://aka.ms/gcm\nInstallation:\ncurl -LO https://raw.githubusercontent.com/GitCredentialManager/git-credential-manager/main/src/linux/Packaging.Linux/install-from-source.sh \u0026\u0026 sh ./install-from-source.sh \u0026\u0026 git-credential-manager-core configure ","categories":"","description":"","excerpt":"Setting up Development Environment in GitHub Codespaces Install the …","ref":"/leda/docs/build/devenv/github-codespaces/","tags":"","title":"Codespaces"},{"body":"Thanks for your interest in this project and in our community.\nhttps://projects.eclipse.org/projects/automotive.leda Contact Contact the project developers via the project’s “dev” list.\nhttps://accounts.eclipse.org/mailing-list/leda-dev Developer resources Information regarding source code management, builds, coding standards, and more.\nhttps://projects.eclipse.org/projects/automotive.leda/developer The project maintains the following source code repositories\nhttps://github.com/eclipse-leda/leda https://github.com/eclipse-leda/leda-distro https://github.com/eclipse-leda/meta-leda Publications Eclipse Leda Introduction Video - YouTube, 2min Eclipse SDV - First Contribution Day (June 2022) - Session Recording, YouTube, 26min Eclipse SDV - Second Contribution Day (September 2022) - Session Recording, YouTube, 30min Eclipse Leda - Slides SDV Contribution Day - September 2022 - Slides (PDF) ","categories":"","description":"","excerpt":"Thanks for your interest in this project and in our community. …","ref":"/leda/docs/project-info/community/","tags":"","title":"Community"},{"body":"The example build configurations in this repository are based on the official BitBake Quickstart tutorial and have been extended to include Leda SDV components.\nBuild Setup To set up your own BitBake build configuration, follow the BitBake documentation and include meta-leda in your bblayers.conf and add the SDV packages into your local.conf.\nThe Leda build is mainly using the kas tool for a simplified maintenance of the BitBake Configuration files. The kas configuration files are located in kas/\nheader: version: 12 distro: leda machine: qemux86-64 target: sdv-image-all repos: ... meta-leda: url: \"https://github.com/eclipse-leda/meta-leda\" refspec: main layers: meta-leda-bsp: meta-leda-components: meta-leda-distro: Leda Metalayer The meta-leda layer conatins the BitBake Classes and Recipes to integrate SDV Components into a BitBake based build setup.\nPlease see https://github.com/eclipse-leda/meta-leda for more information.\nRecipes for containerized components The SDV.EDGE stack is based on a containerized architecture and the intention is to have as much components containerized as possible, to ensure a high degree of isolation and updateability. To ensure some degree of flexibility and control, certain core components may also be installed as native services.\nTo automatically deploy the containers of the SDV reference implementation and example applications and services, the build configurations will deploy a couple of deployment specifiction files into the auto-deployment folder /data/var/containers/manifests.\nAt start time, these containers will be automatically deployed:\nCloud Connector Self Update Agent Vehicle Update Manager Vehicle API / Vehicle Abstraction Layer Data Broker (Eclipse Kuksa) Example Seat Service (CAN-bus implementation) For a full list of containers, see meta-leda-components/recipes-sdv/eclipse-leda/kanto-containers\nRecipes for containerized applications OpenEmbedded’s meta-virtualization already contains some recipes and reusabled classes for building virtualization and containerized applications.\nmeta-leda extends that functionality by using skopeo to package container images. To minimize the runtime requirements (dependencies, disk usage), an approach to pre-load container images and its layers directly into the content storage of the container runtime is followed.\nBuilding containers with Yocto For components which can either be installed natively or as container, it can be beneficial to build these containers using Yocto as well. An example is in meta-leda-distro-container/recipes-sdv/sdv-containers/cyclonedds-example-container_0.1.bb.\n","categories":"","description":"","excerpt":"The example build configurations in this repository are based on the …","ref":"/leda/docs/build/concepts/","tags":"","title":"Concepts"},{"body":" Milestone 0.1.0-M1 Latest Release Artifacts Note: There are no official releases yet. The artifacts available on the Release page are for testing the build and release workflows. They should be considered as unstable nightly builds from the main branch. Releases marked -M1, -M2, -RC1 etc. are preparations for official releases according to the Eclipse Release process.\nGo to the Eclipse Leda Releases page and download the release archive for the respective machine. The release archives container the disk image and the respective Linux kernel:\nMachine Filename Description QEMU x86_64 eclipse-leda-qemu-x86_64.tar.xz For running QEMU x86 64-Bit QEMU ARM 64 eclipse-leda-qemu-arm64.tar.xz For running QEMU ARM 64-Bit Raspberry Pi 4 eclipse-leda-raspberrypi.tar.xz For running on Raspberry Pi 4 (SD-Card Image) Using GitHub CLI tool To download all files of the latest release using the GitHub CLI:\nInstall GitHub CLI, e.g. for Ubuntu:\ncurl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main\" | sudo tee /etc/apt/sources.list.d/github-cli.list \u003e /dev/null sudo apt update sudo apt install gh Authenticate to GitHub:\ngh auth login Download Leda latest release:\nOn Linux:\nmkdir leda \u0026\u0026 cd leda gh release download \\ --pattern '*.zip' \\ --pattern 'eclipse-leda-*' \\ --repo eclipse-leda/leda-distro On Windows:\ngh release download --pattern \"*.zip\" --pattern \"eclipse-leda-*\" --repo eclipse-leda/leda-distro Continue with Running Eclipse Leda on QEMU or Running Eclipse Leda on Raspberry Pi 4\n","categories":"","description":"","excerpt":" Milestone 0.1.0-M1 Latest Release Artifacts Note: There are no …","ref":"/leda/docs/general-usage/download-releases/","tags":"","title":"Download latest release"},{"body":" Publish/Subscribe messaging infrastructure for cloud connectivity by Eclipse Kanto local messaging for applications and SDV system components via an MQTT message broker connection to a backend messaging hub, such as Azure IoT Hub or the IoT Suite device identification and authentication for cloud connectivity by using TLS device certificates Container runtime An OCI-compliant container orchestration for vehicle applications and services by Eclipse Kanto containerd.io as the default container runtime. Both layers of container runtimes can be exchanged with other implementations A Vehicle Update Manager to orchestrate deployments of Vehicle Applications, configurations and base operating system updates An example Vehicle Seat Service implementation to showcase the Eclipse Velocitas programming model, the Eclipse Kuksa.VAL vehicle databroker and the Covesa Vehicle Signal Specification the communication with basic vehicle communication networks such as CAN-Bus (CAN Feeder) A Self Update Agent for firmware-over-the-air (FOTA) updates, using an A/B deployment strategy Integration with RAUC An OpenTelemetry collector and example configurations to collect and publish logs and metrics of containerized Vehicle Applications to the cloud backend for further processing The features of the reusable build recipes implemented as an OpenEmbedded metalayer meta-leda are:\nBuild recipes for a Yocto-based distribution to build SDV-related software components Build recipes for customizations of the target device’s storage structure to enable A/B system updates Build recipes for pre-packaging container images into the device during the manufacturing process to minimize initial online provisioning time A customized minimal setup for use on constrained devices and a full setup with convenient developer tools Ready images for virtual devices, for automated testing and evaluation purposes, eg QEMU ARM-64 Ready images for physical devices, for evaluation and demo purposes, eg Raspberry Pi 4 ","categories":"","description":"","excerpt":" Publish/Subscribe messaging infrastructure for cloud connectivity by …","ref":"/leda/docs/about/features/","tags":"","title":"Features"},{"body":"Create a new GitHub Runner for this repo Start with creating a new azure VM:\nUbuntu Server Latest, currently 20.04 Size Standard D16ds v5 The admin user should be called “runner” Once the VM is ready:\nStop the VM Go to “Disk” and resize the OS disk to 512 GB Start the VM again Run the script to setup the runner Log on to the VM as runner. Either copy the scripts/PrepVMasGHRunner.sh onto the VM or create a new script:\nnano prep.sh Copy the content of the PrepVMasGHRunner.sh from this repo into the new file, save it and make it executable:\nchmod 755 prep.sh Call it with the token and the next available nummer, see below how to get this items:\n./prep.sh \"ASYVOMU........DTCFCMBA\" 3 In the Azure portal go the VM, go to the “network” section and delete the rule opening port 22. Congratulation, you are done!\nHow to get the token and the number to call the script In the repo, go to “Settings” -\u003e “Actions”. You see the currently provisioned runners:\nPick the next number and pass it to the script.\nTo get the token press the green button in the above screenshot. The token is in the command:\n","categories":"","description":"","excerpt":"Create a new GitHub Runner for this repo Start with creating a new …","ref":"/leda/docs/build/devenv/github-runner/","tags":"","title":"GitHub Runner"},{"body":"The project aims to provide an integration point for Open Source components for the Software Defined Vehicle. For vehicle software systems, there are a lot of requirements to consider. Some of these requirements are taken into account for Leda’s quickstart setups, thereas some other requirements can only be met once in a production environment and by customizing the target device image.\nThe following document will list some of these requirements and give an explanation on why they are met in the Leda quickstart distribution.\nOverview Provide an example operating system and configuration for constrained in-vehicle devices Integrate software-defined-vehicle Open Source components to showcase the available features and their state of maturity Demonstrate the use and interaction of open protocols and specifications, such as the specifications from the The Connected Vehicle Systems Alliance (COVESA) OpenTelemetry specs and components Eclipse IoT related specifications for software rollouts and digital twin representations ","categories":"","description":"","excerpt":"The project aims to provide an integration point for Open Source …","ref":"/leda/docs/about/goals/","tags":"","title":"Goals"},{"body":"Baseline (no apps installed) Baseline: 400 MB\nInstalled components:\nContainer Management: runc, containerd, container-management System Services: systemd, openssh, mosquitto, mosquitto-clients No cloud connector or edgecontainerd yet. No self update agent No containers, no vehicle apps etc. root@qemux86-64:/bin# df -h Filesystem Size Used Avail Use% Mounted on /dev/root 2.5G 506M 1.8G 22% / Memory Usage: 200MB root@qemux86-64:/# free total used free shared buff/cache available Mem: 1.9G 329.6M 1.4G 16.6M 222.2M 1.6G Swap: Component Details Dependencies\ncni (container networking): 51 MB containerd-ctr: 26 MB containerd: 46 MB dapr cli: 38 MB helm cli: 43 MB runc: 10 MB SDV Components\nvehicle-data-cli: 2.3 MB (dynamic) Medium:\nLinux Kernel (Poky minimal / Leda distro kernel): 8 MB / 21 MB oci-image-tool: 9 MB oci-runtime-tool: 7 MB sshd: 1 MB libstdc++: 2 MB lic: 2 MB libcrypto: 3 MB containerd-shim: 7 MB containerd-shim-runc-v1: 9 MB containred-shim-runc-v2: 9 MB libsystemd: 1 MB busybox: 1 MB ","categories":"","description":"","excerpt":"Baseline (no apps installed) Baseline: 400 MB\nInstalled components: …","ref":"/leda/docs/build/misc/diskusage/","tags":"","title":"Resource Consumptions"},{"body":" Initial Open Source contribution expected by Q2 2022 (Done) A first milestone build is expected end of 2022 (Done) Plan for the first release cycle to be created in Q1/2023 Release cycles are planned every 3-6 months Release planning will be conducted together with corresponding Eclipse projects Backlog The Leda team maintains a backlog roadmap using GitHub projects at https://github.com/orgs/eclipse-leda/projects/1/views/1\nFuture Work The project intends to be the integration and collaboration platform for Software defined Vehicle functionality.\nExemplary future work:\nMigrate to official Eclipse Kanto releases for Cloud Connector, Container Management and Vehicle Update Manager Include reference implementations from the Eclipse Software Defined Vehicle working group projects: Eclipse Velocitas Eclipse Kuksa Eclipse SommR Eclipse Chariott Eclipse Backend function Bindings (BfB) Include and showcase more features regarding development, operation and monitoring of Vehicle Services and Vehicle Applications If you have feedback, please use GitHub Issues\n","categories":"","description":"","excerpt":" Initial Open Source contribution expected by Q2 2022 (Done) A first …","ref":"/leda/docs/about/roadmap/","tags":"","title":"Roadmap"},{"body":"By using the dockerized Leda quickstart images, the SDV vehicle edge components can be evaluated on emulated X86-64 and ARM64 system images, without having to install QEMU and dependencies manually.\nIf you want to execute the image without Docker, please see Running on QEMU. This allows to tweak the startup script, modify the emulated hardware device and set up advanced network configurations.\nThere are two options to run Leda:\nDocker: Simple, restricted to one container Docker Compose: Advanced, allows networking between containers Recommendations A Linux host with 4 vCPUs, 8GB of RAM and SSD storage is recommended Docker Docker The easiest way is to run a single instance of Leda in a Docker container:\nRun the Leda Docker quickstart image:\ndocker run -it ghcr.io/eclipse-leda/leda-distro/leda-quickstart-x86 Login with root\nStop the Docker container\nshutdown now Note: If you need to stop the container from outside, use the docker stop \u003cid\u003e command from a separate terminal.\nPrivileged Containers When run as a privileged container, QEMU will try to set up a TAP network and use KVM acceleration. Network and CPU will be faster, depending on the host system.\nTo be able to use these host devices, QEMU needs access to the following devices:\n/dev/kvm /dev/net/tun Example command:\ndocker run -it --privileged --device=/dev/kvm:/dev/kvm --device=/dev/net/tun:/dev/net/tun ghcr.io/eclipse-leda/leda-distro/leda-quickstart-x86 Exposing additional ports To also expose ports to connect to ssh, mqtt or the Kuksa databroker, add the port mappings to the Docker command:\ndocker run -it --privileged -p 2222:2222 -p 1883:1883 -p 30555:30555 ghcr.io/eclipse-leda/leda-distro/leda-quickstart-x86:latest The following ports are specifically of interest and exposed by the docker container:\n2222 for SSH (mapped internally to 22) 1880 - free slot reserved for user service 1883 for MQTT 8888 - free slot reserved for user service 30555 for Kuksa Databroker Note: In unprivileged mode, only these ports are mapped by the Docker container and forwarded to the Leda system. In privileged mode, all TCP ports can be exposed, which will then be forwarded from the Docker container into the Leda system. See the entrypoint script for details.\nRunning ARM-64 version To run the ARM-64 bit version of the image, use the leda-quickstart-arm64 container image:\ndocker run -it ghcr.io/eclipse-leda/leda-distro/leda-quickstart-arm64:latest Docker Compose A more convenient way to setup advanced scenarios is to use Docker Compose. There is a Docker Compose configuration for an Eclipse Leda setup in resources/docker-compose.\nIt will start up the following containers:\nLeda Quickstart image (QEMU x86-64) Leda Quickstart image (QEMU ARM-64) Web Server with pre-built RAUC Update Bundles MQTT Bridge DNS Proxy (Allows QEMU to use the Docker DNS) Usage Get the Docker Compose configuration file and additional dockerfiles from the https://github.com/eclipse-leda/leda-distro repository:\ngit clone --filter=blob:none https://github.com/eclipse-leda/leda-distro cd leda-distro/resources/docker-compose Starting the containers with:\ndocker compose up --detach --wait Log in to a development shell inside of the docker network:\ndocker compose run --rm devshell Stopping the containers:\ndocker compose down Docker Compose Services Checking all containers are running or exited successfully:\n$ docker compose ps NAME COMMAND SERVICE STATUS PORTS leda-arm64 \"/docker/leda-quicks…\" leda-arm64 running (healthy) 1883/tcp, 0.0.0.0:2002-\u003e2222/tcp, 0.0.0.0:30556-\u003e30555/tcp leda-bundle-server \"/docker-entrypoint.…\" leda-bundle-server running (healthy) 0.0.0.0:8080-\u003e80/tcp leda-dns-proxy \"dnsmasq -k\" dns-proxy running 53/tcp, 0.0.0.0:5353-\u003e53/udp leda-initializer \"/bin/sh -c /root/le…\" leda-initializer exited (0) leda-mqtt-broker \"/docker-entrypoint.…\" mqtt-broker running (healthy) 0.0.0.0:1883-\u003e1883/tcp leda-x86 \"/docker/leda-quicks…\" leda-x86 running (healthy) 1883/tcp, 0.0.0.0:30555-\u003e30555/tcp, 0.0.0.0:2001-\u003e2222/tcp Network setup As the networking is a bit more complicated to set up with emulated network inside of QEMU, the following explanation is helpful to understand networking better.\nAll docker compose containers are attached to a network called leda-bridge and leda-network and can see each other The QEMU instances use a TAP network inside of each leda-quickstart-xxx container and do a NAT network translation to their own container The Docker internal DNS server is being used. This is implemented by a DNS Proxy container, which will forward incoming DNS requests to the Docker DNS running on the 127.0.0.x network. In unprivileged mode: Only the exposed ports are forwarded from the docker container into the QEMU process: mosquitto 1883, ssh 2222 and kuksa.val databroker 30555. In privileged mode, all TCP ports are forwarded from the Docker container into the QEMU process and the special port 2222 is forwarded to ssh port. Developer Shell Developer Shell:\ndocker compose run --rm devshell From there, you can log in to either Leda on QEMU x86-64, or log in to Leda on QEMU ARM-64.\nssh leda-x86 ssh leda-arm64 To run an additional terminal in the developer shell, execute this:\ndocker compose exec devshell /bin/bash Interacting with Eclipse Leda Check the general system status sdv-health Device Provisioning Run the provisioning script:\nsdv-provision Copy the fingerprints\nGo to Azure IoT Hub, create a new device\nUse the certificate’s common name (CN) as Device Id - on Leda, this defaults to a part of the MAC Address\nSelect X.509 Self-Signed authentication type and enter both fingerprints\nClick Save\nMQTT Broker Bridge graph LR; A[\"MQTT Container on docker host localhost:1883\"] -- Bridge --\u003e B[leda-x86:31883]; A -- Bridge --\u003e C[leda-arm64:31883]; B--\u003eB1[mosquitto service leda-x86:1883]; C--\u003eC1[mosquitto service leda-arm64:1883]; The Docker Compose setup will also start an Eclipse Mosquitto message broker as a bridge to both Leda instances. This allows a user or developer to monitor messages sent by or received by both virtual devices.\nConnect your MQTT client to mqtt-broker.leda-network by using the exposed port 1883 on the host:\nmosquitto_sub -h localhost -p 1883 -t '#' -v Docker Networking You need to enable IP forwarding from Docker containers to make networking work. The containers (leda-arm64, leda-x86) need to run with --privileged as they change iptables rules for proper forwarding of network packets.\nSee Docker documentation for bridge networking for details.\nsudo sysctl net.ipv4.conf.all.forwarding=1 sudo iptables -P FORWARD ACCEPT Each Eclipse Leda instance (ARM64, x86_64) is running within a QEMU emulated network (192.168.7.2), which itself is contained in a containerized network called leda-network (192.168.8.x).\nThe containers wrapping the QEMU instances will forward the following ports to the respective QEMU process:\nSSH on port 2222 Mosquitto on port 1883 DHCP and DNS setup Each Leda-QEMU container is running a local DHCP on the tap0 network interface and listens for DHCP requests by the Leda Distro running inside of QEMU. The DHCP server will respond with the same IP address (192.168.7.2) to the request from QEMU.\nThe DHCP response contains a DNS nameserver pointing to the dns-proxy.leda-network (192.168.8.14) IP, which in turn forwards to Docker’s internal 127.0.0.11 nameserver. This allows the QEMU guests to resolve Docker Compose Services by their service name, e.g. leda-bundle-server.leda-network.\nVolumes The /root path inside of the Leda containers is mounted as a volume and contains the raw disk image and runner scripts for the QEMU Leda distribution. Changes on the QEMU filesystem are made persistent on a copy of the QCOW2 disk image, so that restarting the device will keep any changes.\nTo reset to the original state, delete the respective docker volumes and restart the containers:\ndocker compose down docker compose rm --force --stop --volumes docker volume rm leda-arm64 docker volume rm leda-x86 Profiles Profiles can be used to determine which containers (services) docker compose should be starting by default. This is mostly used to have the devshell container not start up by default.\ntools: Contains docker containers which are not essential at runtime, must useful for testing and development purposes ","categories":"","description":"","excerpt":"By using the dockerized Leda quickstart images, the SDV vehicle edge …","ref":"/leda/docs/general-usage/docker-setup/","tags":"","title":"Running on Docker"},{"body":"If you want to execute the image without building first, grab the latest release or build artifacts from https://github.com/eclipse-leda/leda-distro/\nRecommendations A Linux host with 8 vCPUs, 16GB of RAM and SSD storage is recommended Your Linux user should be sudoer to allow TAP network interfaces to be set up QEMU x86_64 Install Qemu, e.g. for Ubuntu:\nsudo apt-get update -y sudo apt-get install -y xz-utils qemu-system-x86-64 Download latest Eclipse Leda release\nUncompress the archive\ntar xf eclipse-leda-qemu-x86_64.tar.xz Run QEMU on Linux:\n./run-leda.sh Run QEMU on Windows:\nrun-leda.cmd Login as root without password on login prompt\nVerify and wait until container runtime is started: systemctl status container-management\nOptional: Check the system health: sdv-health\nNote: The status of some containers (e.g. cloud connector) are expected to stay in FAILED status as long as the Device Provisioning steps are not completed.\nContinue with Device Provisioning\nQEMU ARM 64-Bit Install Qemu, e.g. for ARM 64-Bit: sudo apt install qemu-system-aarch64\nDownload latest Eclipse Leda release\nUncompress the archive\ntar xf eclipse-leda-qemu-arm64.tar.xz Run QEMU on Linux:\n./run-leda.sh Run QEMU on Windows:\nrun-leda.cmd Login as root without password on login prompt\nVerify and wait until container runtime is started: systemctl status container-management\nOptional: Check the system health: sdv-health\nNote: The status of some containers (e.g. cloud connector) are expected to stay in FAILED status as long as the Device Provisioning steps are not completed.\nContinue with Device Provisioning\n","categories":"","description":"","excerpt":"If you want to execute the image without building first, grab the …","ref":"/leda/docs/general-usage/running-qemu/","tags":"","title":"Running on QEMU"},{"body":"What you need:\nA Raspberry Pi 4B (64 Bit) with 2 GiB of RAM or more, recommended is 8 GiB Network connection (Ethernet or Wifi) with transparent internet access Optional keyboard and display (makes it easier to troubleshoot) Steps:\nDownload the latest released SD-Card Image: eclipse-leda-raspberrypi.tar.xz\nUncompress the SD Card image:\napt-get install -y xz-utils tar xf eclipse-leda-raspberrypi.tar.xz bzip2 -d -f sdv-image-all-raspberrypi4-64.wic.bz2 Flash the sdv-image-all-raspberrypi4.wic file to an SD-Card\nOn Linux: Install bmap tools: sudo apt-get install -y bmap-tools Insert SD Card and check which device is mounted: sudo fdisk -l Unmount the device: sudo umount /dev/mmcblk[X] sudo bmaptool copy --bmap sdv-image-all-raspberrypi4-64.wic.bmap sdv-image-all-raspberrypi4-64.wic /dev/mmcblk[X] Note: Using bmap is much faster but works the same as with plain dd if=\u003cwic-file\u003e of=dev/mmcblk[x]. On Windows: Raspberry Pi Imager Balena Etcher Optional: If you need to adapt the network configuration eg Wifi credentials, edit the configuration files on the boot partition.\nShutdown the Raspberry and insert the SD-Card into the Raspberry Pi SD-Card slot at the bottom\nPower on your Raspberry to boot the image\nLogin with root\nCheck disk space:\nThe raspberry-growdisk system service will do this automatically on first boot.\nTo manually enlarge the available disk space on the SD-Card, resize the disk partition: parted /dev/mmcblk0 resizepart 6 100% \u0026\u0026 resize2fs /dev/mmcblk0p6.\nNote: Due to changes in the disk partition, the partition number (6 in the example) may have changed.\nVerify with df -h.\nVerify and wait until container runtime is started: systemctl status container-management\nOptional: Check the system health: sdv-health\nContinue with Device Provisioning\n","categories":"","description":"","excerpt":"What you need:\nA Raspberry Pi 4B (64 Bit) with 2 GiB of RAM or more, …","ref":"/leda/docs/general-usage/raspberry-pi/","tags":"","title":"Running on Raspberry Pi"},{"body":"Health check The sdv-health utility displays a status overview of some important dependencies and device configurations for the SDV edge stack. The sdv health utility can be configured using the sdv.conf configuration file.\nUsage:\n# sdv-health Example output:\nVersion Information The Leda image version and build time will be displayed at the top in the first category:\nOS Release: Year and codename of the release version Image Version: Name of the image and more specific version information, such as the git tag or commit id Build timestamp in yyyMMddHHmmss notation Note: The information is read from the system base file in /etc/os-release:\nroot@qemux86-64:~# cat /etc/os-release BUILD_ID=\"20230309083051\" DISTRO_CODENAME=\"Dracon\" ID=leda IMAGE_VERSION=\"0.0.5-41-g82f2b12\" NAME=\"Eclipse Leda\" PRETTY_NAME=\"Eclipse Leda 2023 (Dracon)\" VERSION=\"2023 (Dracon)\" VERSION_CODENAME=\"Dracon\" VERSION_ID=0.0.5-41-g82f2b12 Bus networks If available, vehicle bus network information, such as the status of the CAN-Bus, will be displayed iin its own category. This helps to quickly identify if there problems with the hardware connectivity for automotive bus networks.\nPorts The health utility checks the TCP ports of specific services. This helps to identify if these services are up and running and potentiallyconnectable via external network interfaces.\nServices and Containers The services category shows the status of required and optional containers. The required containers are supposed to be up and running for the SDV.EDGE stack to be correctly up and running. If any of these core components have a failed state, the functionality is impacted.\nThe optional containers are for additional features and for example applications. These containers may not be necessary for each use case and hence will be marked as a warning if they are not up and running. The overview still helps to identify which containers are working properly.\nErrors, Warnings, Failed states When there are errors or warnings related to the status of SDV related components, the health utility will print out these error states, and if available also a more detailed error message.\nIn the following example, the health utility helps the troubleshooting process:\nThe container runtime is properly started: “Kanto CM” is OK in the “SDV Ports” section and the “container-management” service is OK in the “SDV Services” section\". Some containers are in state “OK”, which means there is no general issue with the container runtime. The cloud connector is in a “Stopped” state, which indicates that the user manually stopped the container by using “kanto-cm stop -n cloud-connector”. The sua container is in a “Exited” state, which indicates the process exited with an error code. ","categories":"","description":"","excerpt":"Health check The sdv-health utility displays a status overview of some …","ref":"/leda/docs/general-usage/utilities/sdv-health/","tags":"","title":"SDV Health"},{"body":"The Seat Adjuster use case has been derived from the Eclipse Velocitas Seat Adjuster example, the Eclipse Kuksa.VAL Seat Service example and the Digital.Playground Prototyping Library\nDescription The Seat Adjuster use cases demonstrates the idea of having a cloud-based driver profile hosted by a third-party web service, which is then used by a custom application to move the driver seat to certain positions.\nExamples range from simple personalization to dynamic seat control (such as periodic back massages) based on personal health treatment plans by an health insurance organization. A driver could opt-in to such a service, which would lead to the ad-hoc installation of a seat adjuster vehicle application when the driver enters the vehicle. The seat adjuster application would then contact the health insurance webservice to retrieve the driver’s personal treatment plan for back massage and start moving the motors in certain intervals, to ease or prevent back pain during long driving periods.\nThis example focuses on the technical aspect, namely the interface between such an assumed application with the high-level logic, and the lower-level backend service (Seat Service), which controls the communication to the underlying hardware (Seat ECU). In this case, the Vehicle Signal Specification and the simple signal for seat position is used exemplarily. Safety considerations are not in scope for this example, but would probably be handled within the Seat ECU as the lowest level component to guard against non-safe use of the seat motors.\nArchitecture Overview Cloud or mobile trigger: not part of the Leda image, but can be simulated by issueing MQTT messages Eclipse Velocitas - Vehicle Application: Seat Adjuster (to be deployed by user as part of the Velocitas tutorial) Eclipse Kuksa.VAL - Example Service: Seat Service (pre-installed) Eclipse Kuksa.VAL - Data Broker (pre-installed) Seat ECU and the separate Seat Motor hardware: not part of the Leda image, but can be emulated using virtual CAN-Bus. The Kuksa Seat Service container contains configuration options to use VCAN Getting started Follow the Velocitas tutorial: build and deploy your clone of the seat adjuster example\nDownload and run the Leda quickstart image\nDeploy your seat adjuster application to the container runtime, either manually by using kanto-cm create or by providing a deployment descriptor in /var/containers/manifests. An example deployment descriptor can be found in meta-leda-components. Details on the deployment can be found in Leda Vehicle Applications\nEnsure the databroker and the seat service containers are running and you know how to check their log files\nPublish an MQTT message for the seat adjuster application, e.g. mosquitto_pub -t seatadjuster/setPosition/request -f seat-request.json and a possible payload like this:\n{\"position\": 300, \"requestId\": \u003crequest_id\u003e} Prototyping The pseudo-code for the Seat Adjuster application could look like the following excerpt from the Digital.Playground example:\nfrom sdv_model import Vehicle vehicle = Vehicle() on_user_profile_changed(Issuer): vehicle.Cabin.Seat.Row1.Pos1.Height.set(profile[Issuer][\"VerticalHeight\"]) vehicle.Cabin.Seat.Row1.Pos1.Position.set(profile[Issuer][\"HorizontalPosition\"]) In the next step, this prototype can be taken to the next level of implementation by using the Velocitas example to express it like this, with possibly more logic:\n@subscribe_topic(\"seatadjuster/setPosition/request\") async def on_set_position_request_received(self, data_str: str) -\u003e None: vehicle_speed = (await self.Vehicle.Speed.get()).value ... if vehicle_speed == 0: try: await self.Vehicle.Cabin.Seat.Row1.Pos1.Position.set(position) response_data[\"result\"] = { \"status\": 0, \"message\": f\"Set Seat position to: {position}\", } except ValueError as error: response_data[\"result\"] = { \"status\": 1, \"message\": f\"Failed to set the position {position}, error: {error}\", } except Exception: response_data[\"result\"] = { \"status\": 1, \"message\": \"Exception on set Seat position\", } else: error_msg = f\"\"\"Not allowed to move seat because vehicle speed is {vehicle_speed} and not 0\"\"\" response_data[\"result\"] = {\"status\": 1, \"message\": error_msg} await self.publish_event(response_topic, json.dumps(response_data)) Testing the seatservice-example container on the Leda Quickstart Image Whether you are running the seatservice-example with a simulated CAN or a physical (emulated) CAN, the seatservice-example container included in the Leda Quickstart image provides a simple client application that can be used to test the GRPC interface.\nTo set the seat position to x%, take x*10 and truncate it to an integer \u003cposition\u003e. Then using the sdv-ctr-exec script issue the following command:\n$ sdv-ctr-exec -n seatservice-example /app/bin/seat_svc_client \u003cposition\u003e You can now check the logs (either with kanto-cm logs or kantui) and you should be able to see the seatservice-example application responding to the command by moving the seat to the desired position.\nSpecifics for physical (emulated) CAN You will have to generate initial CAN frames that will emulate the car ECU responding to the service:\n$ cangen -v can0 -L 8 -I 712 -D r -n 5 can0 712#4F.BF.B0.6B.5F.2D.54.09 can0 712#13.2E.98.7E.77.11.99.5B can0 712#15.70.87.07.73.24.3A.7A can0 712#99.7F.F5.3F.FB.99.00.04 can0 712#FE.1C.D5.55.22.86.3A.1F You can now start tracing CAN frames written to the bus with candump can0\nFrom now on when a request to change the seat position is issued you will be able to see the corresponding CAN frames in the trace.\nNote: On QEMU you can tunnel the host CAN bus to the guest: Tunneling a CAN Interface from the Host.\nHardware CAN-Bus The default configuration of the Seat Service is using simulated VCAN. If you want to switch to a physical CAN-Bus interface, the container needs to have access to the CAN-Bus hardware.\nSuch a CAN-Bus device might be a Raspberry Pi setup with an MCP251x-based CAN-Hat extension or a QEMU image with an emulated kvaser_pci device (enabled on the Leda QEMU Quickstart images by default).\nThis setup would require some adjustments to the container manifest in order for the container to have access to the physical CAN-Bus.\nMake Seat Service container privileged and run on the host network interface:\n\"host_config\": { ... \"network_mode\": \"host\", \"privileged\": true, ... } Remove all port mappings and extra hosts (set \"extra_hosts\": [] and \"port_mappings\": []) for the container as it’s now running in host-networking mode (host_ip variable no longer available) and all ports are directly exposed.\nSet the address to the databroker to localhost:30555:\n\"config\": { \"env\": [ ... \"BROKER_ADDR=127.0.0.1:30555\", ... ], ... } Reconfigure the seat controller application to use the physical CAN interface, please see Eclipse Kuksa.VAL seat_controller/README.md for details:\nSC_CAN=can0 CAN=can0 All the necessary changes combined for clarity as a single diff can be found below:\n--- ../meta-leda-fork/meta-leda-components/recipes-sdv/eclipse-leda/kanto-containers/example_dev/seatservice.json\t2023-03-06 11:32:00.771754434 +0200 +++ seatservice-new.json\t2023-03-06 11:37:12.967182044 +0200 @@ -14,26 +14,16 @@ \"hooks\": [], \"host_config\": { \"devices\": [], - \"network_mode\": \"bridge\", - \"privileged\": false, + \"network_mode\": \"host\", + \"privileged\": true, \"restart_policy\": { \"maximum_retry_count\": 0, \"retry_timeout\": 0, \"type\": \"unless-stopped\" }, \"runtime\": \"io.containerd.runc.v2\", - \"extra_hosts\": [ - \"databroker-host:host_ip\" - ], - \"port_mappings\": [ - { - \"protocol\": \"tcp\", - \"container_port\": 50051, - \"host_ip\": \"localhost\", - \"host_port\": 30051, - \"host_port_end\": 30051 - } - ], + \"extra_hosts\": [], + \"port_mappings\": [], \"log_config\": { \"driver_config\": { \"type\": \"json-file\", @@ -58,9 +48,11 @@ }, \"config\": { \"env\": [ - \"BROKER_ADDR=databroker-host:30555\", - \"RUST_LOG=info\", - \"vehicle_data_broker=info\" + \"CAN=can0\", + \"SC_CAN=can0\", + \"BROKER_ADDR=127.0.0.1:30555\", + \"RUST_LOG=info\", + \"vehicle_data_broker=info\" ], \"cmd\": [] }, ","categories":"","description":"","excerpt":"The Seat Adjuster use case has been derived from the Eclipse Velocitas …","ref":"/leda/docs/app-deployment/seat-adjuster/","tags":"","title":"Seat Adjuster"},{"body":"This chapter describes the steps necessary to perform a local (without cloud) self update of the operating system.\nSelf-Update using RAUC Update Bundles On host: Update bundle sdv-rauc-bundle-qemux86-64.raucb is in current folder\nNote: In the development environment, the update RAUC Update Bundle is located in the BitBake machine-specific output folder Example location is tmp/deploy/images/qemux86-64\nOn host: Start a dummy web server for serving the update file\npython3 -m http.server --bind 192.168.7.1 On host: open two new terminals - one for monitoring and one for triggering the self-update\nTerminal 1: To view the progress, watch the MQTT topics selfupdate/desiredstate and selfupdate/desiredstatefeedback:\nmosquitto_sub -h 192.168.7.2 -p 1883 -t \"selfupdate/#\" Terminal 2: Trigger the actual self update process by publishing an MQTT message to selfupdate/desiredstate:\nmosquitto_pub -h 192.168.7.2 -p 1883 -t \"selfupdate/desiredstate\" -f start-update-example.json Switch to a terminal in the guest\nOn guest: After the self update process completed, check the status:\nrauc status --detailed Self-Update Trigger Message start-update-example.json file:\n{ \"activityId\": \"random-uuid-as-string\", \"timestamp\": 123456789, \"payload\": { \"domains\": [ { \"id\": \"self-update\", \"components\": [ { \"id\": \"os-image\", \"version\": \"${VERSION_ID}\", \"config\": [ { \"key\": \"image\", \"value\": \"http://leda-bundle-server/sdv-rauc-bundle-qemux86-64.raucb\" } ] } ] } ] } } Example Message Flows Current State Initial response message on startup from self update agent in topic selfupdate/currentstate, or upon request by sending message to selfupdate/currentstate/get\nRequest:\n{ \"activityId\": \"\u003cuuid\u003e\", \"timestamp\": 1676332092 } Response:\n{ \"activityId\": \"\u003cuuid\u003e\", \"timestamp\": 1675345910, \"payload\": { \"domains\": [ { \"id\": \"self-update\", \"components\": [ { \"id\": \"os-image\", \"version\": \"bundle_version_not_available\" } ] } ] } } Desired State External trigger to update via desired state on topic selfupdate/desiredstate:\n{ \"activityId\": \"\u003cuuid\u003e\", \"timestamp\": 123456789, \"payload\": { \"domains\": [ { \"id\": \"self-update\", \"components\": [ { \"id\": \"os-image\", \"version\": \"${VERSION_ID}\", \"config\": [ { \"key\": \"image\", \"value\": \"http://leda-bundle-server/sdv-rauc-bundle-qemux86-64.raucb\" } ] } ] } ] } } Initialization response by Self Update Agent on topic selfupdate/desiredstatefeedback\n{ \"activityId\": \"\u003cuuid\u003e\", \"timestamp\": 1675347152, \"payload\": { \"status\": \"IDENTIFIED\", \"message\": \"Self-update agent is about to perform an OS image update.\", \"actions\": [] } } Responses while downloading by Self Update Agent on topic selfupdate/desiredstatefeedback\n{ \"activityId\": \"\u003cuuid\u003e\", \"timestamp\": 1675347152, \"payload\": { \"status\": \"RUNNING\", \"message\": \"Self-update agent is performing an OS image update.\", \"actions\": [ { \"component\": { \"id\": \"self-update:os-image\", \"version\": \"${VERSION_ID}\" }, \"status\": \"DOWNLOADING\", \"progress\": 0, \"message\": \"Downloading 0.0 MiB...\" } ] } } Response by Self Update Agent on topic selfupdate/desiredstatefeedback when download successfully finished:\n{ \"activityId\": \"\u003cuuid\u003e\", \"timestamp\": 1675347154, \"payload\": { \"status\": \"RUNNING\", \"message\": \"Self-update agent is performing an OS image update.\", \"actions\": [ { \"component\": { \"id\": \"self-update:os-image\", \"version\": \"${VERSION_ID}\" }, \"status\": \"DOWNLOAD_SUCCESS\", \"progress\": 100, \"message\": \"Downloaded 108.9 MiB...\" } ] } } Response by Self Update Agent on topic selfupdate/desiredstatefeedback while performing the installation:\n{ \"activityId\": \"\u003cuuid\u003e\", \"timestamp\": 1675347159, \"payload\": { \"status\": \"RUNNING\", \"message\": \"Self-update agent is performing an OS image update.\", \"actions\": [ { \"component\": { \"id\": \"self-update:os-image\", \"version\": \"${VERSION_ID}\" }, \"status\": \"UPDATING\", \"progress\": 20, \"message\": \"RAUC install...\" } ] } } Response by Self Update Agent on topic selfupdate/desiredstatefeedback when installation completed successfully:\n{ \"activityId\": \"\u003cuuid\u003e\", \"timestamp\": 1675347186, \"payload\": { \"status\": \"COMPLETED\", \"message\": \"Self-update completed, reboot required.\", \"actions\": [ { \"component\": { \"id\": \"self-update:os-image\", \"version\": \"${VERSION_ID}\" }, \"status\": \"UPDATE_SUCCESS\", \"progress\": 100, \"message\": \"Writing partition completed, reboot required.\" } ] } } ","categories":"","description":"","excerpt":"This chapter describes the steps necessary to perform a local (without …","ref":"/leda/docs/device-provisioning/self-update/self-update-tutorial/","tags":"","title":"Self Update Tutorial"},{"body":"Sharing a directory with the guest When you want to copy files between the host and the guest, an easy way is to use an SFTP tunnel. With sshfs, you can mount a local directory to a remote directory via SSH.\nPre-Requisites Installation of needed packages:\nRun apt-get install sshfs on your host Enable CORE_IMAGE_EXTRA_INSTALL += \" openssh-sftp-server\" in local.conf of your image (e.g. in the local_conf_header section in your kas file) Verify SFTP connection working with sftp -P 2222 root@localhost Transfering files from host to guest When you want to copy files from the host to the guest, an easy way is to use an SFTP tunnel. With sshfs, you can mount a local directory to a remote directory via SSH.\nCreate a mount point on your host: mkdir remote Open the SSH Filesystem tunnel: sshfs root@localhost:/ remote/ -p 2222 Check files: ls -al remote/ - you should see the root filesystem of the device now You can now easily copy files: cp foo.txt remote/home/root/ Transfering files from guest to host Note: The reverse direction, e.g. initiating an SSH tunnel from within the device to the host, is currently not supported by the installed software on the image.\n","categories":"","description":"","excerpt":"Sharing a directory with the guest When you want to copy files between …","ref":"/leda/docs/general-usage/running-qemu/filetransfer/","tags":"","title":"Transferring Files"},{"body":"Validating the release Steps to validate if a release is properly working:\nCreate a new pre-release from your branch\nDownload the release artifacts onto a clean system.\nDo not use your build environment, to minimize the impact of existing environment configuration from BitBake etc.\nRun the run-leda scripts to execute Qemu\nNote: You should test each of the release archives, for each target machine.\nFollow the Device Provisioning guide\nPerform some verification tests (see below)\nCleanup: Delete the pre-release and the git tag:\ngit push --delete origin \u003ctagname\u003e Ideas for manual verification steps Note: These are just for manual testing, as we intend to extend the automated tests as much as possible.\nOperating system level Run sdv-health on the shell Verify disk partitions and RAUC status, e.g. rauc status Verify network interface and CAN-Bus with ip addr Container runtime Check status of containers with kantui ","categories":"","description":"","excerpt":"Validating the release Steps to validate if a release is properly …","ref":"/leda/docs/build/release/validation/","tags":"","title":"Validating"},{"body":"Preparation Obtain the Docker Engine for your distribution and add your non-privileged user to the docker group (sudo usermod -aG docker $USER ) Install Visual Studio Code Visual Studio Code: Development Containers Open Visual Studio Code Open Command Palette (F1) and select Clone repository in Container Volume Select eclipse-leda/meta-leda and the main branch. Adapt proxy configurations if necessary (.devcontainer/proxy.sh) For a clean remote build machine, you may want to set up a development environment on GitHub CodeSpaces\nBuilding Leda in a VSCode DevContainer: After successfully setting up your DevContainer you can build Leda either with kas or manually:\nTo build with kas follow the instructions at: Building with kas\nTo build manually: Building manually\nAuthentication The build process requires online connection and you must be authenticated to access private repositories.\nCreate a GitHub Personal Access Token (PAT) at https://github.com/settings/tokens and grant read:packages permission Use Configure SSO and authorize your PAT for the organization On the build host, authenticate to ghcr.io: skopeo login ghcr.io --authfile ~/auth.json --username \u003cusername\u003e and enter the PAT as password You may need to create the folder where skopeo is storing authentication information beforehand: sudo mkdir -p /run/containers/1000 sudo chmod a+w /run/containers/1000 Start the bitbake build process ","categories":"","description":"","excerpt":"Preparation Obtain the Docker Engine for your distribution and add …","ref":"/leda/docs/build/devenv/vscode-devcontainer/","tags":"","title":"VSCode DevContainer"},{"body":"Note: This part of the Leda meta-layer and quickstart image is currently under active development and this documentation page may not represent the actual state of VUM\nThe vehicle update manager container requires the following configuration:\nContainer needs to run in privileged mode to enable automatic reboot.\nNote: This is enabled by default on the Leda Quickstart images to simplify automated testing.\nConnection to MQTT broker, defaults to THINGS_CONN_BROKER=tcp://mosquitto:1883\nEnable container orchestration feature: THINGS_FEATURES=ContainerOrchestrator\nOptional configuration options are:\nSELF_UPDATE_ENABLE_REBOOT=true Enable automatic reboot after a successfull application of the update bundle. SELF_UPDATE_TIMEOUT=30m Timeout for downloading and installing an update bundle. Example Deployment Specification apiVersion: v1 kind: ServiceAccount metadata: name: vehicle-update-manager --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: vehicle-update-manager rules: - apiGroups: - '*' resources: - '*' verbs: - '*' --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: vehicle-update-manager subjects: - kind: ServiceAccount name: vehicle-update-manager namespace: default roleRef: kind: ClusterRole name: vehicle-update-manager apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: name: vehicle-update-manager spec: selector: matchLabels: component: vehicle-update-manager template: metadata: labels: component: vehicle-update-manager spec: serviceAccountName: vehicle-update-manager containers: - name: vehicle-update-manager image: \u003crepository\u003e/vehicleupdatemanager:\u003ctag\u003e imagePullPolicy: IfNotPresent securityContext: privileged: true env: - name: SELF_UPDATE_TIMEOUT value: 30m - name: SELF_UPDATE_ENABLE_REBOOT value: \"true\" - name: THINGS_CONN_BROKER value: tcp://mosquitto:1883 - name: THINGS_FEATURES value: ContainerOrchestrator volumeMounts: - mountPath: /proc name: proc volumes: - hostPath: path: /proc name: proc imagePullSecrets: - name: ghcr-io ","categories":"","description":"","excerpt":"Note: This part of the Leda meta-layer and quickstart image is …","ref":"/leda/docs/device-provisioning/vehicle-update-manager/vehicle-update-manager-configuration/","tags":"","title":"Configuration"},{"body":"Kanto-auto-deployer (KAD) This service has been implemented as a stopgap solution by the Leda team since as of now the Kanto CM-native mechanism for initial container deployment does not suit the needs of the Leda Quickstart image well.\nIn-depth documentation on its operation and development can be found in Kanto Auto deployer.\nIn the Leda Distro KAD runs as a service that continuously monitors the directory defined with the BitBake variable KANTO_MANIFESTS_DEV_DIR in the distro config recipes. By default, the Leda quickstart images use KANTO_MANIFESTS_DEV_DIR=/data/var/containers/manifests\". So, these manifests are again stored in the persistent data-partition and can be modified after the image has been deployed.\nImportant: KAD supports the Kanto Container Management Manifest Template AND the Container Internal State Representation. For future compatiblity reasons it is recommended that you choose Kanto Container Management Manifest Template.\nA standard set of containers is deployed through meta-leda, the manifests for which can be found in meta-leda/meta-leda-components/recipes-sdv/eclipse-leda/kanto-containers/core and meta-leda/meta-leda-components/recipes-sdv/eclipse-leda/kanto-containers/example.\nThe advantage of this service is that it can be restarted very quickly (systemctl restart kanto-auto-deployer), without having to restart the whole container-management service. The implemented filewatcher (the --daemon flag) allows you to quickly create/edit (or even touch them) container manifests on the device that would be on-the-fly deployed after saving. This allows for rapid testing when creating new container manifests.\nKanto Container Management init-dir This feature of Kanto Container Managament by the Kanto Teams is currently under development and hence unstable. That is why it is not used as of the test-0.0.6 release of Leda Distro and 0.1.0-M1 release of meta-leda.\n","categories":"","description":"","excerpt":"Kanto-auto-deployer (KAD) This service has been implemented as a …","ref":"/leda/docs/customization/deploying-containers/","tags":"","title":"Deploying Containers"},{"body":"TLDR: To deploy a container in the final Leda image, all you generally need to do is add the manifest in the kanto-containers directory and re-build.\nKanto-CM does not provide (currently) a stable feature that allows for the automatic deployment of containers through manifest files similar to k3s’ automated deployment of k8s-manifests found in the /var/lib/rancher/k3s/server/manifests directory.\nThis can be worked around via a bash script for each container that runs on boot and makes sure it’s deployed. Even though this approach is functional it is not very structured and would require a lot repeating code.\nThat is why the “Kanto Auto deployer” tool was developed. It directly implements the ideas in Communicating with Кanto-CM via gRPC.\nThe compiled binary takes a path to a directory containing the json manifests, parses them into Rust structures and sends gRPC requests to kanto container management to deploy these containers.\nManifest structure Because Kanto CM uses different JSON formats for the interal state representation of the container (from the gRPC API) and for the deployment via the Container Management-native init_dir-mechanism, KAD supports both through the “manifests_parser” module. The conversion between formats is automatic (logged as a warning when it’s attempted) so you do not need to provide extra options when using one or the other.\nContainer Management Manifests Format This is the CM-native format, described in the Kanto-CM documentation. It is the recommended format since KAD is supposed to be replaced by native CM deployment modules in the future and this manifest format will be compatible with that.\nIt also allows you to ommit options (defaults will be used). KAD will issue a log warning when the “manifests_parser” attempts to convert this manifest format to the gRPC message format (internal state representation).\nInternal State Representation The KAD “native” manifests format uses the exact same structure for its manifests as the internal representation of container state in kanto container management. This manifest format does not allow keys in the json to be ommited, so these manifests are generally larger/noisier. For example:\n{ \"id\": \"\", \"name\": \"databroker\", \"image\": { \"name\": \"ghcr.io/eclipse/kuksa.val/databroker:0.2.5\", \"decrypt_config\": null }, \"host_name\": \"\", \"domain_name\": \"\", \"resolv_conf_path\": \"\", \"hosts_path\": \"\", \"hostname_path\": \"\", \"mounts\": [], \"hooks\": [], \"host_config\": { \"devices\": [], \"network_mode\": \"bridge\", \"privileged\": false, \"restart_policy\": { \"maximum_retry_count\": 0, \"retry_timeout\": 0, \"type\": \"unless-stopped\" }, \"runtime\": \"io.containerd.runc.v2\", \"extra_hosts\": [], \"port_mappings\": [ { \"protocol\": \"tcp\", \"container_port\": 55555, \"host_ip\": \"localhost\", \"host_port\": 30555, \"host_port_end\": 30555 } ], \"log_config\": { \"driver_config\": { \"type\": \"json-file\", \"max_files\": 2, \"max_size\": \"1M\", \"root_dir\": \"\" }, \"mode_config\": { \"mode\": \"blocking\", \"max_buffer_size\": \"\" } }, \"resources\": null }, \"io_config\": { \"attach_stderr\": false, \"attach_stdin\": false, \"attach_stdout\": false, \"open_stdin\": false, \"stdin_once\": false, \"tty\": false }, \"config\": { \"env\": [ \"RUST_LOG=info\", \"vehicle_data_broker=debug\" ], \"cmd\": [] }, \"network_settings\": null, \"state\": { \"pid\": -1, \"started_at\": \"\", \"error\": \"\", \"exit_code\": 0, \"finished_at\": \"\", \"exited\": false, \"dead\": false, \"restarting\": false, \"paused\": false, \"running\": false, \"status\": \"\", \"oom_killed\": false }, \"created\": \"\", \"manually_stopped\": false, \"restart_count\": 0 } The only difference to the actual internal state representation is that fields in the manifest can be left empty (\"\") if they are not important for the deployment. These values will be filled in with defaults by kanto-cm after deployment.\nFor example, you do not need to specify the container “id” in the manifest, as an unique uuid would be assigned automatically after deployment.\nContainer deployment in Leda Kanto-auto-deployer can run as a one-shot util that goes through the manifest folder (default: /data/var/containers/manifests) and deploys required containers.\nWhen you pass the --daemon flag it would enable the “filewatcher” module that would continuously monitor the provided path for changes/creation of manifests.\nThe Bitbake recipe for building and installing the auto deployer service can be found at kanto-auto-deployer_git.bb.\nThis recipe also takes all manifests in the kanto-containers directory and installs them in the directory specified by the KANTO_MANIFESTS_DIR BitBake variable (weak default: /var/containers/manifests).\nImportant: To deploy a container in the final Leda image, all you generally need to do is add the manifest in the kanto-containers directory and re-build.\nConditional compilation of the filewatcher module To reduce binary bloat the --daemon option is namespaced under the filewatcher conditional compilation flag (enabled by default). To compile KAD without the filewatcher module run: cargo build --release --no-default-features. (Implemented in Leda Utils PR#35)\n","categories":"","description":"","excerpt":"TLDR: To deploy a container in the final Leda image, all you generally …","ref":"/leda/docs/build/dev-and-maintenance/rust/kanto-auto-deployer/","tags":"","title":"Kanto Auto deployer (KAD)"},{"body":"The Eclipse Leda quickstart image has CAN-Bus kernel modules and some CAN hardware drivers pre-installed. However, some hardware configuration needs to be adapted at boot time depending on the specific CAN-Extension being attached to the device.\nOn Raspberry Pi, there is a /boot/config.txt file where you can configure the dtoverlay options accordingly.\nNote: After modification, the device requires rebooting for the changes to take effect.\nMCP2515 based modules Products:\nWaveshare RS485 CAN HAT for Raspberry Pi dtoverlay=mcp2515-can0,oscillator=12000000,interrupt=25,spimaxfrequency=2000000 MCP2518FD based modules Products:\nWaveshare 2-Channel Isolated CAN FD Expansion HAT for Raspberry Pi, Multi Protections dtoverlay=2xMCP2517FD #dtoverlay=2xMCP2518FD-spi0 ","categories":"","description":"","excerpt":"The Eclipse Leda quickstart image has CAN-Bus kernel modules and some …","ref":"/leda/docs/general-usage/raspberry-pi/raspberry-can-hats/","tags":"","title":"CAN-Bus extensions"},{"body":"The Dog Mode use case has been derived from the Eclipse Velocitas Dog Mode example and the Eclipse Kuksa.VAL HVAC Service example.\nDescription In the Dog Mode use case, an existing vehicle service (such as HVAC for Heating, Ventilation and Air Conditioning) is used for another use case. When a driver enables the dog mode, the vehicle will keep the climate in a status to accomodates the pet while the driver is gone for a few minutes.\nThe focus on this example is to show how an additional application can reuse existing functionality for new use cases. The vehicle service being used does not need to be adapted for the new use case. The new use case is deployed as an additional functionality, separated from the rest of the existing system.\nArchitecture Overview Cloud or mobile trigger: not part of the Leda image, but can be simulated by issueing MQTT messages Eclipse Velocitas - Vehicle Application: Dog Mode (to be deployed by user as part of the Velocitas tutorial) Eclipse Kuksa.VAL - Example Service: HVAC Service (pre-installed) Eclipse Kuksa.VAL - Data Broker (pre-installed) Getting started Follow the Velocitas tutorial: build and deploy your clone of the dog mode example\nDownload and run the Leda quickstart image\nDeploy the application to the container runtime, either manually by using kanto-cm create or by providing a deployment descriptor in /var/containers/manifests. An example deployment descriptor can be found in meta-leda-components. Details on the deployment can be found in Leda Vehicle Applications\nEnsure the databroker and the service containers are running and you know how to check their log files\nMonitor vehicle data: mosquitto_sub -t dogmode/display\nEnable dog mode by using the databroker-cli\nconnect set Vehicle.Cabin.DogMode 1 ","categories":"","description":"","excerpt":"The Dog Mode use case has been derived from the Eclipse Velocitas Dog …","ref":"/leda/docs/app-deployment/dog-mode/","tags":"","title":"Dog Mode"},{"body":" Download latest Eclipse Leda release Run Eclipse Leda on emulated Qemu devices or on Raspberry Pi 4 Configure device, e.g. provision the device Explore the device tools Develop your first Vehicle App using Eclipse Velocitas template Deploy a Vehicle App to the device ","categories":"","description":"","excerpt":" Download latest Eclipse Leda release Run Eclipse Leda on emulated …","ref":"/leda/docs/general-usage/","tags":"","title":"Getting Started"},{"body":"The KantoUI tool is a text-based user interface for conveniently managing containers in the Kanto Container Management. It supports simple navigation using keyboard and mouse to select a specific container. Commands to start, stop, remove and re-deploy containers are available along with a functionality to retrieve the application logs of a selected container.\nKanto User Interface Usage:\nkantui Example output:\nCommand Line Options Print command line help:\nroot@qemux86-64:~# kantui --help kantui 0.2.0 A TUI for Kanto CM that allows easier management of deployed containers. Requires root. USAGE: kantui [OPTIONS] OPTIONS: -c, --config-file-path \u003cCONFIG_FILE_PATH\u003e Set a custom path for the kantui configuration file [default: /etc/kantui/kantui_conf.toml] -h, --help Print help information -V, --version Print version information Note: All config values can be overridden through env variables prefixed with KANTUI_, e.g. KANTUI_STOP_TIMEOUT=5 overrides the timeout before SIGKILL is sent to be 5 seconds. Keyboard commands Arrow keys Up and Down to select a container Arrow keys Left and Right to select a column Enter to change the sort ordering of the currently selected column S to start the selected container which is currently not running P to stop the selected container R to remove a container L to show the log output of a container D to redeploy an existing container (rereads deployment descriptor) Q to quit kantui Note: The mouse can be used to select ui items when holding the Shift key.\nStarting, Stopping, Removing containers To use the lifecycle commands on a container:\nSelect a container by using the mouse or the arrow keys. The selected container is highlighted. Press one of the lifecycle commands, e.g. s, p or r Wait for the value in the State column to change, before issueing the next command. Note: When using the lifecycle commands for containers (Start, Stop, Remove), it may take a few seconds before the UI is updated with the status changes. The amount of time before a container is forcefully killed is determined by the stop_timeout configuration option.\nShow container Logs To view the container’s log output:\nSelect a container by using the mouse or the arrow keys. Press the L key Log output will be displayed (tail, oldest messages first) Use the arrow keys Up and Down or drag the scrollbar using your mouse to scroll through the log Close the Log viewer by pressing Enter or clicking on the OK button Note: The log output is not followed automatically. Close the dialog and reopen to see new output.\nRedeploying containers To redeploy a container, e.g. when a deployment descriptor has changed on disk:\nSelect a container by using the mouse or the arrow keys. Press the P key to stop the container Press the R key to remove the container Press the D key to invoke the Kanto Auto Deployer, which will redeploy the missing containers Note: Only containers managed by Kanto Auto Deployer will be redeployed.\nColumn Sorting To sort a column:\nSelect the column using the Left and Right arrow keys Press Enter to activate sorting Press Enter again on a sorted column to invert sort order (Ascending -\u003e Descending -\u003e Ascending) Note: The selected sort order is not persisted and will reset to the default on restart: By ID, ascending\nConfiguration File The default location of the configuration file is /etc/kantui/kantui_conf.toml:\n# General Configuration Options socket_path = \"/run/container-management/container-management.sock\" # Path to kanto-cm unix socket stop_timeout = 5 # timeout (integer) in seconds before a SIGKILL is sent after a SIGTERM [keyconfig] start_btn_name = \"[S]tart\" start_kbd_key = \"s\" stop_btn_name = \"Sto[P]\" stop_kbd_key = \"p\" remove_btn_name = \"[R]emove\" remove_kbd_key = \"r\" logs_btn_name = \"[L]ogs\" logs_kbd_key = \"l\" quit_btn_name = \"[Q]uit\" quit_kbd_key = \"q\" redeploy_btn_name = \"Re[D]eploy\" redeploy_kbd_key = \"d\" # Includes a shell lexer so anything that would be a valid shell command can be used # No pipes/redirects allowed. # N.B.: Command inherits kantui's privileges (root) redeploy_command = \"systemctl restart kanto-auto-deployer.service\" ","categories":"","description":"","excerpt":"The KantoUI tool is a text-based user interface for conveniently …","ref":"/leda/docs/general-usage/utilities/kantui/","tags":"","title":"KantUI"},{"body":"Follow these steps to do a manual device provisioning:\nGenerate the device certificate (eg using openssl) and sign it with your CA. Log in to Azure Portal, Go to Azure Iot Hub and create a new device Select the proper authentication type, e.g. X.509 Self-signed or X.509 CA Signed Copy the device certificate (cert file and key file) to the device to /data/var/certificate Restart cloud connector service or container. Create a device in Azure IoT Hub For the device to be connectable, it needs to be known to the cloud service first. In these steps, we will create a new device identity by using Azure IoT Hub.\nPre-Requisites:\nVirtual device must already be started with runqemu ... or leda\nNote: For Raspberry Pi, please follow the manual steps below and adapt the SSH connection options to the IP of your Raspbery Pi.\nThe virtual device needs to be remotely accessible via ssh port 2222 on the host’s localhost (Qemu port forwarding in userspace) or via ssh port 22 on the IP address 192.168.7.2 (Qemu virtual networking using TAP network interface)\nThe container runtime needs to have started successfully, check with sdv-health\nA Device has been created in Azure IoT Hub\nNote: Do NOT create an “edge” device.\nConfigure authentication on device For the proper device authentication, the device management backend authority needs to issue a device-specific certificate and sign it. This is a complex process and subject to the specific situation.\nFor the Leda quickstart images, the software configuration is prepared with dummy certificates which need to be replaced.\nATTENTION: The Leda example device certificates are public and insecure, they only serve demonstration purposes. You need to replace the intermediate certificates and device certificates with your own.\nGenerate a device certificate using openssl Sign it with your intermediate CA certificate Put it into /data/var/certificate/ Restart the cloud connector service or container: systemctl restart cloud-connector or kanto-cm stop -n cloudconnector --force; kanto-cm start -n cloudconnector When finished, continue with\nDeploying a Vehicle App Performing a Self Update Private container registries Please refer to the Container Registries on how to configure private container registries.\n","categories":"","description":"","excerpt":"Follow these steps to do a manual device provisioning:\nGenerate the …","ref":"/leda/docs/device-provisioning/manual-provisioning/","tags":"","title":"Manual Provisioning"},{"body":"Initializing BitBake environment Initialize the build environment and start the build for QEMU:\nkas build kas/leda-qemux86-64.yaml Building specific recipes General usage:\nkas build kas/leda-qemux86-64.yaml --target \u003crecipename\u003e Metalayer Structure meta-leda-bsp \\-- classes // Reusable BitBake Classes, eg for offline container image pre-caching \\-- conf // Distribution specific configurations, eg version numbers, release codename \\-- recipes-bsp // Board Support Packages, eg specifics for QEMU and Raspberry Pi meta-leda-components \\-- classes // Reusable BitBake Classes, eg for offline container image pre-caching \\-- conf // Distribution specific configurations, eg version numbers, release codename \\-- recipes-sdv |-- eclipse-leda // Build recipes for Eclipse Leda Incubator components |-- eclipse-kuksa // Build recipes for Eclipse Kuksa |-- eclipse-cyclonedds // Build recipes for Eclipse CycloneDDS |-- northstar // Build recipes for Northstar Container Runtime |-- packagegroups // Grouping packages \\-- sdv-base // SDV Base Bundle: fstab, can0.network |--- base-files |--- SDV Core Utilities \\--- SDV Utilities |-- sdv-containers // Container images recipes for pre-caching / airgap installation |--- Cloud Agent |--- Data Broker |--- Feeder CAN |--- OTel Collector |--- Self Update Agent |--- Vehicle Update manager |--- Example Seat Service \\--- ... |-- sdv-core // SDV Core Bundle |--- SDV RAUC Bundle // RAUC Update Bundle Manifest \\-- tools // Convenience tools for the \"full\" image, eg nerdctl and kantui meta-leda-distro \\-- classes // Reusable BitBake Classes, eg for offline container image pre-caching \\-- conf // Distribution specific configurations, eg version numbers, release codename \\-- recipes-containers // Container related configuration recipes (containerd, nerdctl) \\-- recipes-core // Core recipes (base-files, systemd) \\-- recipes-kernel // Kernel configuration, eg kernel modules, logging, virtio \\-- recipes-sdv-distro // Image definitions \\-- wic // WIC Kickstarter files - Partition layouts meta-leda-distro-container \\-- classes // Reusable BitBake Classes, eg for offline container image pre-caching \\-- conf // Distribution specific configurations, eg version numbers, release codename \\-- recipes-sdv // Build containers with Yocto Base Bundle Contains the recipes to build and install the minimal set of dependencies for the SDV stack on the edge device. With these minimal components, the SDV stack should be able to bootstrap itself.\nCAN-Bus Kernel Configuration To enable support for CAN bus related modules, the kernel needs to be reconfigured. This is done by the sdv-canbus-modules.inc include file in the recipes-kernel/linux folder, which patches Poky’s linux-yocto recipe.\nVerifying and displaying the current kernel configuration: bitbake -e virtual/kernel\nTo verify the recipe and the kernel configuration: bitbake linux-yocto -c kernel_configcheck -f\nThe kernel config file can be found in: ./tmp/work/qemux86_64-poky-linux/linux-yocto/*/linux-qemux86_64-standard-build/.config\nCore Bundle Contains the recipes to build and install additional SDV components, which are required for a proper runtime setup.\nContainers Contains the recipes for pre-installing specific containers into the container management at runtime. This is mainly for pre-caching container image layers onto the device to speed up the initial deployment but can also be used to enable offline usecases.\nBuild Host System Requirements Yocto Project 4.0 (kirkstone) or higher 100GB+ free disk space per build configuration Online connection for fetching sources and container images ","categories":"","description":"","excerpt":"Initializing BitBake environment Initialize the build environment and …","ref":"/leda/docs/build/metalayer/","tags":"","title":"Metalayer"},{"body":"Container Management The container management will respond to the cloud connectivity status message by initially sending a list of containers:\nContainer Management responds with a list of containers in the topic e/\u003ctenantId\u003e/\u003cgatewayId\u003e:\u003cdeviceId\u003e:edge:containers { \"topic\": \"\u003ctenantId\u003e/\u003cgatewayId\u003e:\u003cdeviceId\u003e:edge:containers/things/twin/commands/modify\", \"headers\": { \"response-required\": false }, \"path\": \"/features/SoftwareUpdatable\", \"value\": { \"definition\": [ \"org.eclipse.hawkbit.swupdatable:SoftwareUpdatable:2.0.0\" ], \"properties\": { \"status\": { \"softwareModuleType\": \"oci:container\", \"installedDependencies\": { \"ghcr.io%2Feclipse%2Fkuksa.val%2Fdatabroker.\u003cuuid\u003e:0.2.5\": { \"group\": \"ghcr.io/eclipse/kuksa.val/databroker\", \"name\": \"\u003cuuid\u003e\", \"version\": \"0.2.5\" }, \"ghcr.io%2Feclipse%2Fkuksa.val.services%2Fseat_service.\u003cuuid\u003e:v0.1.0\": { \"group\": \"ghcr.io/eclipse/kuksa.val.services/seat_service\", \"name\": \"\u003cuuid\u003e\", \"version\": \"v0.1.0\" }, \"ghcr.io%2Feclipse-leda%2Fleda-contrib-cloud-connector%2Fcloudconnector.\u003cuuid\u003e:latest\": { \"group\": \"ghcr.io/eclipse-leda/leda-contrib-cloud-connector/cloudconnector\", \"name\": \"\u003cuuid\u003e\", \"version\": \"latest\" }, \"ghcr.io%2Feclipse-leda%2Fleda-contrib-self-update-agent%2Fself-update-agent.\u003cuuid\u003e:build-20\": { \"group\": \"ghcr.io/eclipse-leda/leda-contrib-self-update-agent/self-update-agent\", \"name\": \"\u003cuuid\u003e\", \"version\": \"build-20\" }, \"ghcr.io%2Feclipse-leda%2Fleda-contrib-vehicle-update-manager%2Fvehicleupdatemanager.\u003cuuid\u003e:latest\": { \"group\": \"ghcr.io/eclipse-leda/leda-contrib-vehicle-update-manager/vehicleupdatemanager\", \"name\": \"\u003cuuid\u003e\", \"version\": \"latest\" } } } } } } Container Management answers with an additional message for each container in the topic e/\u003ctenantId\u003e/\u003cgatewayId\u003e:\u003cdeviceId\u003e:edge:containers { \"topic\": \"\u003ctenantId\u003e/\u003cgatewayId\u003e:\u003cdeviceId\u003e:edge:containers/things/twin/commands/modify\", \"headers\": { \"response-required\": false }, \"path\": \"/features/Container:\u003cuuid\u003e\", \"value\": { \"definition\": [ \"com.bosch.iot.suite.edge.containers:Container:1.5.0\" ], \"properties\": { \"status\": { \"name\": \"seatservice-example\", \"imageRef\": \"ghcr.io/eclipse/kuksa.val.services/seat_service:v0.1.0\", \"config\": { \"domainName\": \"seatservice-example-domain\", \"hostName\": \"seatservice-example-host\", \"env\": [ \"VEHICLEDATABROKER_DAPR_APP_ID=databroker\", \"BROKER_ADDR=databroker-host:30555\", \"RUST_LOG=info\", \"vehicle_data_broker=info\" ], \"restartPolicy\": { \"type\": \"UNLESS_STOPPED\" }, \"extraHosts\": [ \"databroker-host:host_ip\" ], \"portMappings\": [ { \"proto\": \"tcp\", \"hostPort\": 30051, \"hostPortEnd\": 30051, \"containerPort\": 50051, \"hostIP\": \"localhost\" } ], \"networkMode\": \"BRIDGE\", \"log\": { \"type\": \"JSON_FILE\", \"maxFiles\": 2, \"maxSize\": \"1M\", \"mode\": \"BLOCKING\" } }, \"createdAt\": \"2023-02-02T08:46:33.792687313Z\", \"state\": { \"status\": \"RUNNING\", \"pid\": 13627, \"startedAt\": \"2023-02-02T09:51:08.049572297Z\", \"finishedAt\": \"2023-02-02T09:50:51.752255799Z\" } } } } } ","categories":"","description":"","excerpt":"Container Management The container management will respond to the …","ref":"/leda/docs/device-provisioning/container-management/orchestration/","tags":"","title":"Orchestration"},{"body":"The bootable Leda Quickstart images are disk image with multiple partitions:\nTwo bootloader partitions, to enable A/B update process A separate partition to hold bootloader status information An optional rescue partition with basic operating system and tooling Two rootfs partitions with a full installation of the operating system, to enable A/B update process A data partition for storing container runtime data Partition Layout for QEMU x86-64 The x86_64 image uses GRUB as a bootloader and the partition layout is as follows:\nPartition Layout for QEMU ARM-64 The partition layout for QEMU ARM-based images are comparable, except:\nBootloader is replaced with U-Boot Partition Layout for Raspberry Pi The partition layout for Raspberry Pi images are comparable, except:\nBootloader is replaced with U-Boot The last partition (the data partition) is marked as growable, to allow the use of the larger SD-Card capacities for container runtime data ","categories":"","description":"","excerpt":"The bootable Leda Quickstart images are disk image with multiple …","ref":"/leda/docs/build/misc/partitioning/","tags":"","title":"Partition Layout"},{"body":"Leda integrates RAUC as a reference implementation and example configuration. It allows the evaluation of the concepts, mechanisms and involved software components in an emulated, virtual environment or on physical devices.\nChecking the RAUC Status Get the current RAUC boot status:\nrauc status Example output:\nroot@qemux86-64:~# rauc status === System Info === Compatible: Eclipse Leda qemu86-64 Variant: Booted from: rootfs.1 (SDV_B) === Bootloader === Activated: rootfs.1 (SDV_B) === Slot States === o [rootfs.1] (/dev/sda5, ext4, inactive) bootname: SDV_B mounted: / boot status: good x [rootfs.0] (/dev/sda4, ext4, booted) bootname: SDV_A boot status: good Forcing to boot the other slot To manually force the device to boot into another slot, mark the current booted slot as bad, mark the other partitions as active and perform a reboot:\nrauc status mark-bad booted rauc status mark-active other reboot now Testing the rescue system By marking both root slots as bad, the bootloader is supposed to boot the rescue system:\nrauc status mark-bad rootfs.0 rauc status mark-bad rootfs.1 reboot now Example output of rauc:\no [rootfs.1] (/dev/sda5, ext4, inactive) bootname: B boot status: bad o [rootfs.0] (/dev/sda4, ext4, booted) bootname: A mounted: / boot status: bad Customizations The configurations can be customized by applying or patching the following files:\nRAUC Configuration file: meta-leda/recipes-bsp/rauc/files/qemux86-64/system.conf Bootloader Configuration file: meta-leda/recipes-bsp/grub/files/grub.cfg The physical disk partition configuration: meta-leda/recipes-sdv/wic/qemux86-grub-efi.wks RAUC System Configuration The RAUC System Configuration is the central configuration of the RAUC Update system.\nExample:\n[system] compatible=Eclipse Leda qemu86-64 bootloader=grub grubenv=/grubenv/grubenv statusfile=/data/rauc.status [keyring] path=ca.cert.pem [slot.efi.0] device=/dev/sda type=boot-gpt-switch region-start=4M region-size=100M [slot.rescue.0] device=/dev/sda3 type=ext4 readonly=true [slot.rootfs.0] device=/dev/sda4 type=ext4 bootname=SDV_A [slot.rootfs.1] device=/dev/sda5 type=ext4 bootname=SDV_B GRUB Bootloader Configuration The GRUB bootloader has a configuration file which describes which partitions are bootable, which partition they are located at and a reference to RAUC’s slot name.\nThe configuration also contains RAUC specific logic and variables required for a proper integration. Please see the full grub.cfg in the source repository and RAUC Documentation - Integration - GRUB for details.\nExcerpt:\n... menuentry \"SDV Slot A (OK=$SDV_A_OK TRY=$SDV_A_TRY)\" { linux (hd0,4)/boot/bzImage root=/dev/vda4 $CMDLINE rauc.slot=SDV_A } menuentry \"SDV Slot B (OK=$SDV_B_OK TRY=$SDV_B_TRY)\" { linux (hd0,5)/boot/bzImage root=/dev/vda5 $CMDLINE rauc.slot=SDV_B } U-Boot Bootloader Configuration Similarly to GRUB, integration of RAUC with U-Boot requires custom boot scripting. A highly detailed explaination can, again, be found in the official RAUC Documentation - Integration - U-Boot.\nMeta-Leda provides such integration recipes and scripts for all U-boot based targets, for which a Leda Quickstart image is available (qemuarm64, qemuarm and rpi4-64). For example:\nMain uboot_%.bbappend Qemuarm custom script integration recipe Qemuarm custom boot.scr Note: A custom U-Boot device defconfig might be required for some devices to be integrated with RAUC. Leda Quickstart images patch the default defconfigs for qemuarm64 and qemuarm to save the U-Boot environment in a VFAT BOOT partition.\nDisk Partitioning with OpenEmbedded Image Creator (WIC) The OpenEmbedded Image Creator is used in BitBake to actually create full disk images with multiple partitions.\nThese disk images are machine specific and the structure of the partitions are configured in OpenEmbedded Kickstart files (*.wks).\nExcerpt qemux86-grub-efi.wks Note: The excerpt is exemplary, please see the sources for a full representation and documentation.\nbootloader --ptable gpt part --fixed-size 50M --source rawcopy --sourceparams=\"file=efi-boot.vfat\" --fstype=vfat --label boot --active part --fixed-size 10M --source rawcopy --sourceparams=\"file=grubenv.vfat\" --fstype=vfat --label grubenv part /rescue --source rootfs --fstype=ext4 --label rescue part / --source rootfs --fstype=ext4 --label root_a part / --source rootfs --fstype=ext4 --label root_b part /data --fixed-size 4G --fstype=ext4 --label data ","categories":"","description":"","excerpt":"Leda integrates RAUC as a reference implementation and example …","ref":"/leda/docs/device-provisioning/self-update/rauc-integration/","tags":"","title":"RAUC Integration"},{"body":"There are multiple variants on how to set up a build environment:\nwith GitHub Codespaces - recommended for developers with restricted internet access, such as corporate proxies, or with Windows hosts with VSCode DevContainer - recommended for Linux hosts Custom setup - for teams ","categories":"","description":"","excerpt":"There are multiple variants on how to set up a build environment:\nwith …","ref":"/leda/docs/build/devenv/","tags":"","title":"Setup development environment"},{"body":"The following message flow is an example for the Self Update use case. The message is triggered by command line via Azure IoT Hub.\nsequenceDiagram autonumber actor flops as Fleet Operations participant backend as Digital Twin participant vum as Vehicle Update Manager participant sua as Self Update Agent participant device as Device flops -\u003e\u003e backend: Rollout Campaign backend --\u003e\u003e vum: Device Command Live Message: yamlApply Note left of backend: C2D Message vum --\u003e\u003e sua: selfupdate/desiredstate SelfUpdateBundle loop Download sua --\u003e\u003e sua: Downloading sua --\u003e\u003e vum: selfupdate/desiredstatefeedback vum --\u003e\u003e backend: progress end loop Installation sua --\u003e\u003e device: Installing Note right of device: RAUC Update Bundle sua --\u003e\u003e vum: selfupdate/desiredstatefeedback vum --\u003e\u003e backend: progress end sua --\u003e\u003e vum: Installed rect rgb(100, 255, 150) sua --\u003e\u003e backend: FINISHED_SUCCESS end opt Reboot vum --\u003e\u003e device: SysRq Reboot end Messages The following describes the message flow with example messages in more detail. The following variables are used for dynamic parts of the messages:\n\u003ccuid\u003e - A Correlation ID in form of a UUID \u003cselfUpdateRequestYaml\u003e or \u003cpayload\u003e- The Desired State Self Update Request message in YAML, as defined by the Self Update Agent API \u003chub\u003e - The name or identifier of the message hub \u003cdevice\u003e - The device identifier used by the message hub or other components to identify the device. Cloud backend sends the Self-Update Request Message as YAML embedded into an Azure IoT Hub C2D Message Envelope:\nPayload:\n{ \"appId\": \"mc-ota-update\", \"cmdName\": \"desiredstate.update\", \"cId\": \"\u003ccuid\u003e\", \"eVer\": \"2.0\", \"pVer\": \"1.0\", \"p\": \u003cselfUpdateRequestYaml\u003e } Cloud Connector validates envelope and transforms request message into a ContainerOrechestrator message:\nTopic: command//azure.edge:\u003chub\u003e:\u003cdevice\u003e:edge:containers/req/\u003ccuid\u003e/yamlApply\nBody (json):\n{ \"topic\": \"azure.edge/\u003chub\u003e:\u003cdevice\u003e:edge:containers/things/live/messages/yamlApply\", \"headers\": { \"content-type\": \"application/json\", \"correlation-id\": \"\u003ccuid\u003e\"}, \"path\": \"/features/ContainerOrchestrator/inbox/messages/yamlApply\", \"value\": { \"correlationId\": \"\u003ccuid\u003e\", \"payload\": \"\u003cpayload\u003e\" } } } Note: Payload (Yaml encoded in JSON) omitted here for clarity, see next step.\nVehicle Update manager extracts payload and forward the message to the Self Update Agent message inbox:\nTopic: selfupdate/desiredstate\nMessage:\napiVersion: sdv.eclipse.org/v1 kind: SelfUpdateBundle metadata: name: self-update-bundle-example spec: bundleDownloadUrl: http://leda-bundle-server/sdv-rauc-bundle-qemux86-64.raucb bundleName: swdv-arm64-build42 bundleTarget: base bundleVersion: v1beta3 The Self Update Agent response with status messages during download and installation phases.\nTopic: selfupdate/desiredstatefeedback\nMessage:\napiVersion: sdv.eclipse.org/v1 kind: SelfUpdateBundle metadata: name: \"self-update-bundle-example\" spec: bundleDownloadUrl: \"http://leda-bundle-server/sdv-rauc-bundle-qemux86-64.raucb\" bundleName: \"swdv-arm64-build42\" bundleTarget: base bundleVersion: v1beta3 state: message: Entered Downloading state name: downloading progress: 0 techCode: 0 Once finished, the Vehicle Update Manager will also return a FINISHED_SUCCESS message for the conversation with the backend.\nTopic: e/defaultTenant/azure.edge:\u003chub\u003e:\u003cdevice\u003e:edge:containers\nMessage:\n{ \"topic\": \"azure.edge/\u003chub\u003e:\u003cdevice\u003e:edge:containers/things/twin/commands/modify\", \"headers\": { \"response-required\":false }, \"path\": \"/features/ContainerOrchestrator/properties/status/state\", \"value\": { \"manifest\": [], \"status\": \"FINISHED_SUCCESS\", \"correlationId\":\"\u003ccuid\u003e\" } } ","categories":"","description":"","excerpt":"The following message flow is an example for the Self Update use case. …","ref":"/leda/docs/device-provisioning/vehicle-update-manager/message-flow/","tags":"","title":"Message Flow"},{"body":"The k8s ecosystem comes with a lot of utilies that allow for the easier management of containers (such as k9s). The kantui util aims to be a “nice” text user interface that lets the user start/stop/remove/get logs of deployed containers in kanto-cm.\nDevelopment notes This tool is again based on the ideas in Communicating with Кanto-CM via gRPC.\nIt spins up two threads - an UI thread (drawing/updating UI) and an IO thread (communicating with kanto-cm via gRPC). The communication between these two threads happens over an async-priority-channel with ListContainers request having a lower priority than Start/Stop/Remove/Get Logs (“user interaction”) requests.\nThis in an “eventually fair” mechanism of communication. That way even if kanto-cm is handling a slow request (such as stopping a container that does not respect SIGTERM) the UI thread is never blocked, allowing for a responsive-feeling UI. The size of the channel is 5 requests and the UI is running at 30 fps. Thus even if the UI gets out-of-sync with the actual state of container management it would be “only” for 5 out 30 frames.\nCursive and ncurses-rs The cursive crate is used as a high level “framework” as it allows very easy handling of UI events via callbacks, though this might be prone to callback hell.\nThe default backend for cursive is ncurses-rs which a very thin Rust wrapper over the standart ncurses library. This in theory would be the optimal backend for our case as ncurses is a very old and stable library that has buffering (other backends lead to flickering of the UI on updates) and is dynamically linked (smaller final binary size).\nThe ncurses-rs wrapper however is not well-suited to cross-compilation as it has a custom build.rs that generates a small C program, compiles it for the target and tries to run it on the host. The only reason for this C program to exist is to check the width of the char type. Obviously, the char type on the host and the target might be of different width and this binary might not even run on the host machine if the host and target architectures are different.\nAfter coming to the conclusion that the ncurses-rs backend was not suitable, kantui was migrated to the termion backend + the cursive_buffered_backend crate which mitigates the flickering issue.\n[dependencies] ... cursive_buffered_backend = \"0.5.0\" [dependencies.cursive] default-features=false version = \"0.16.2\" features = [\"termion-backend\"] This completely drops the need for ncurses-rs but results in a slightly bigger binary (all statically linked).\nBitbake Recipe The recipe was created following the guidelines in Generating bitbake recipes with cargo-bitbake and can be found in meta-leda/meta-leda-components/recipes-sdv/eclipse-leda/.\nFuture improvement notes The gRPC channel can get blocked thus effectively “blocking” the IO-thread until it is freed-up again. Maybe open a new channel for each request (slow/resource heavy)?\nReorganize the code a bit, move all generic functionally in the lib.rs.\n","categories":"","description":"","excerpt":"The k8s ecosystem comes with a lot of utilies that allow for the …","ref":"/leda/docs/build/dev-and-maintenance/rust/kantui/","tags":"","title":"Kantui"},{"body":"BSP packages for emulated WiFi devices (QEMU) and hardware device drivers are provided in meta-leda for supported hardware. They usually do not require extra configuration.\nFor a guide on how to connect to a wireless network check the Connecting to Wi-Fi networks page.\nRaspberry Pi 4B 64 The required kernel modules and binary blobs are provided with the sdv-wifi-kernel-config.inc config file and the packagegroup-sdv-rpi4wifi packagegroup. These are included in sdv-image-full.bb and sdv-image-minimal.bb by default.\nIf you, however, decide to define your own custom image based on Custom Distros, you would have to make sure the packagegroup is installed to enable WiFi connectivity.\nQEMU QEMU images provide the kernel modules necessary to set-up a virtual wlan interface and connect it to a virtual wifi network. This can be useful in various testing scenarios.\nTo create and link a virtual WiFi interface to eth0, boot your QEMU image and run:\n$ ip link add link eth0 name wlan0 type virt_wifi The SSID of the virtual network you can connect to is VirtWifi.\nNote: Leda Quickstart QEMU images set the name of the default virtual ethernet interface to eth0 through kernel CMDLINE configurations (net.ifnames=0 biosdevname=0). If you are building a custom image with a different kernel CMDLINE replace eth0 with the name of your virtual interface (check ifconfig).\n","categories":"","description":"","excerpt":"BSP packages for emulated WiFi devices (QEMU) and hardware device …","ref":"/leda/docs/general-usage/wifi-configuration/","tags":"","title":"Wifi Configuration"},{"body":"The self update agent (SUA) is a component responsible for the OS Update process. SUA is communicating on MQTT interface via usage of defined messages. Internally, SUA uses RAUC to perform the update.\nFollowing sequence diagram shows the happy path example of communication between components.\nProcess Overview sequenceDiagram participant m as MQTT Broker participant s as SUA participant r as RAUC s --\u003e\u003e m: connect loop Wait for message: selfupdate/desiredstate Note left of s: Initial start s -\u003e\u003e m: selfupdate/currentstate Note left of s: Trigger for OTA m -\u003e\u003e s: selfupdate/desiredstate s -\u003e\u003e m: selfupdate/desiredstatefeedback: downloading 0% s -\u003e\u003e s: download bundle s -\u003e\u003e m: selfupdate/desiredstatefeedback: downloading 51% s -\u003e\u003e r: install s -\u003e\u003e m: selfupdate/desiredstatefeedback: installing 0% r -\u003e\u003e r: install r -\u003e\u003e s: share progress: e.g. 51% s -\u003e\u003e m: selfupdate/desiredstatefeedback: installing 51% r -\u003e\u003e s: installation ready s -\u003e\u003e m: selfupdate/desiredstatefeedback: installed s -\u003e\u003e m: selfupdate/desiredstatefeedback: idle end MQTT Message Definitions MQTT messages are specified as follows:\nselfupdate/desiredstate Topic Direction Description selfupdate/desiredstate IN This message triggers the update process. The payload shall contain all data necessary to obtain the update bundle and to install it. apiVersion: \"sdv.eclipse.org/v1\" kind: SelfUpdateBundle metadata: name: self-update-bundle-example spec: bundleName: swdv-arm64-build42 bundleVersion: v1beta3 bundleDownloadUrl: https://example.com/repository/base/ bundleTarget: base selfupdate/currentstate Topic Direction Description selfupdate/currentstate OUT This message is being sent once, on SUA start. It contains information about currently installed OS version. apiVersion: \"sdv.eclipse.org/v1\" kind: SelfUpdateBundle metadata: name: self-update-bundle-example spec: bundleVersion: v1beta3 selfupdate/desiredstatefeedback Topic Direction Description selfupdate/desiredstatefeedback OUT This message is being sent by SUA to share current progress of triggered update process. This is the OUT counterpart of selfupdate/desiredstate input message. apiVersion: \"sdv.eclipse.org/v1\" kind: SelfUpdateBundle metadata: name: self-update-bundle-example spec: bundleName: swdv-arm64-build42 bundleVersion: v1beta3 bundleDownloadUrl: https://example.com/repository/base/ bundleTarget: base state: name: \"idle|installing|etc.\" progress: 0|51|99|etc., techCode: 0|1|5|etc., message: \"Cannot download from url|Bundle already installed|etc.\" state enum State name field can have one of following values:\nState Description Additional payload data uninitialized When the SUA is not configured yet - idle Configured and waiting for messages - downloading Downloading the bundle file progress installing Performing installation progress installed Installation process was successful, new OS version is installed on inactive disc Slot. Important: to finish the OTA process, reboot is required, and it shall be performed by another component, such as the Vehicle Update Manager. - failed Error occurred techCode techCode values techCode field is providing additional details to the state value. It is especially useful for the failed state, as it can specify the reason of failure.\nValue Description 0 OK, no error 1001 Download failed 2001 Invalid Bundle 3001 Installation failed 4001 Update rejected, bundle version same as current OS version 5001 Unknown Error ","categories":"","description":"","excerpt":"The self update agent (SUA) is a component responsible for the OS …","ref":"/leda/docs/device-provisioning/self-update/api/","tags":"","title":"API Reference"},{"body":"After setting up your VSCode DevContainer or GitHub Codespace you can proceed with the actual build process. Here you have two choices - either using the kas-build system or setting up the build manually.\nBuilding with kas This is the easiest way to build leda semi-automatically\ncd /workspaces/meta-leda-fork/ Open the VSCode terminal and run kas build Note: you can alter the build options by modifying the .config.yaml file in the trunk of the repository Building manually You can also build Leda manually if more customization of the build process is required.\nexport LEDA_WORKDIR=/workspaces/meta-leda-fork/\ncd ${LEDA_WORKDIR}\nClone the Poky repository with the required release, e.g. kirkstone and pull updates if necessary:\ngit clone git://git.yoctoproject.org/poky cd poky git checkout -t origin/kirkstone -b kirkstone git config pull.rebase false git pull Prepare the build environment:\nsource oe-init-build-env Dry-run a build of the Linux Kernel recipe using BitBake:\nbitbake --dry-run linux-yocto Checkout the meta-layer dependencies for Leda:\ncd $LEDA_WORKDIR git clone -b kirkstone https://github.com/rauc/meta-rauc.git meta-rauc git clone -b kirkstone https://github.com/rauc/meta-rauc-community.git meta-rauc-community git clone -b kirkstone https://git.yoctoproject.org/meta-virtualization meta-virtualization git clone -b kirkstone https://git.openembedded.org/meta-openembedded meta-openembedded Change to the poky/build directory (generated from the oe-init-build-env script automatically)\nAdd all the necessary meta-layers:\nbitbake-layers add-layer ${LEDA_WORKDIR}/meta-rauc bitbake-layers add-layer ${LEDA_WORKDIR}/meta-rauc-community/meta-rauc-qemux86 bitbake-layers add-layer ${LEDA_WORKDIR}/meta-openembedded/meta-oe bitbake-layers add-layer ${LEDA_WORKDIR}/meta-openembedded/meta-filesystems bitbake-layers add-layer ${LEDA_WORKDIR}/meta-openembedded/meta-python bitbake-layers add-layer ${LEDA_WORKDIR}/meta-openembedded/meta-networking bitbake-layers add-layer ${LEDA_WORKDIR}/meta-virtualization bitbake-layers add-layer ${LEDA_WORKDIR}/meta-leda-components bitbake-layers add-layer ${LEDA_WORKDIR}/meta-leda-bsp bitbake-layers add-layer ${LEDA_WORKDIR}/meta-leda-distro Dry run:\nDISTRO=leda bitbake --dry-run sdv-image-all Real build:\nDISTRO=leda bitbake sdv-image-all You can also build one of the target recipies this way:\nDISTRO=leda bitbake kanto-container-management Note: in this case you can set the target architecture and other build options in the build/local.conf file\n","categories":"","description":"","excerpt":"After setting up your VSCode DevContainer or GitHub Codespace you can …","ref":"/leda/docs/build/devenv/build-kas-manually/","tags":"","title":"Building with kas/manually"},{"body":"Container Metrics Container-level metrics, such as CPU utilization or memory usage, can be retrieved on-demand and continuously by enabling the container metrics events. This is done by sending a request to the topic command//\u003cnamespaceId\u003e:\u003cgatewayId\u003e:\u003cdeviceId\u003e:edge:containers/req/\u003ccorrelationId\u003e/request:\nEnable Container Metrics with a frequency of 5s: { \"topic\": \"dummy-namespace/dummy-gateway:dummy-device-id:edge:containers/things/live/messages/request\", \"headers\": { \"timeout\": \"10\", \"response-required\": true, \"content-type\": \"application/json\", \"correlation-id\": \"3fdc463c-293c-4f39-ab19-24aef7944550\" }, \"path\": \"/features/Metrics/inbox/messages/request\", \"value\": { \"frequency\": \"5s\" } } Example command line:\nmosquitto_pub -t command//dummy-namespace:dummy-gateway:dummy-device-id:edge:containers/req/3fdc463c-293c-4f39-ab19-24aef7944550/request -m '{\"topic\":\"dummy-namespace/dummy-gateway:dummy-device-id:edge:containers/things/live/messages/request\",\"headers\":{\"timeout\":\"10\",\"response-required\":true,\"content-type\":\"application/json\",\"correlation-id\":\"3fdc463c-293c-4f39-ab19-24aef7944550\"},\"path\":\"/features/Metrics/inbox/messages/request\",\"value\":{\"frequency\":\"5s\"}}' Container Metrics answers with an additional message for each container in the topic e/\u003ctenantId\u003e/\u003cgatewayId\u003e:\u003cdeviceId\u003e:edge:containers { \"topic\": \"\u003ctenantId\u003e/\u003cgatewayId\u003e:\u003cdeviceId\u003e:edge:containers/things/twin/commands/modify\", \"headers\": { \"response-required\": false }, \"path\": \"/features/Container:\u003cuuid\u003e\", \"value\": { \"definition\": [ \"com.bosch.iot.suite.edge.containers:Container:1.5.0\" ], \"properties\": { \"status\": { \"name\": \"seatservice-example\", \"imageRef\": \"ghcr.io/eclipse/kuksa.val.services/seat_service:v0.1.0\", \"config\": { \"domainName\": \"seatservice-example-domain\", \"hostName\": \"seatservice-example-host\", \"env\": [ \"VEHICLEDATABROKER_DAPR_APP_ID=databroker\", \"BROKER_ADDR=databroker-host:30555\", \"RUST_LOG=info\", \"vehicle_data_broker=info\" ], \"restartPolicy\": { \"type\": \"UNLESS_STOPPED\" }, \"extraHosts\": [ \"databroker-host:host_ip\" ], \"portMappings\": [ { \"proto\": \"tcp\", \"hostPort\": 30051, \"hostPortEnd\": 30051, \"containerPort\": 50051, \"hostIP\": \"localhost\" } ], \"networkMode\": \"BRIDGE\", \"log\": { \"type\": \"JSON_FILE\", \"maxFiles\": 2, \"maxSize\": \"1M\", \"mode\": \"BLOCKING\" } }, \"createdAt\": \"2023-02-02T08:46:33.792687313Z\", \"state\": { \"status\": \"RUNNING\", \"pid\": 13627, \"startedAt\": \"2023-02-02T09:51:08.049572297Z\", \"finishedAt\": \"2023-02-02T09:50:51.752255799Z\" } } } } } To disable Container Metrics, send a request with frequency of 0s:\nmosquitto_pub -t command//dummy-namespace:dummy-gateway:dummy-device-id:edge:containers/req/3fdc463c-293c-4f39-ab19-24aef7944550/request -m '{\"topic\":\"dummy-namespace/dummy-gateway:dummy-device-id:edge:containers/things/live/messages/request\",\"headers\":{\"timeout\":\"10\",\"response-required\":true,\"content-type\":\"application/json\",\"correlation-id\":\"3fdc463c-293c-4f39-ab19-24aef7944550\"},\"path\":\"/features/Metrics/inbox/messages/request\",\"value\":{\"frequency\":\"0s\"}}' ","categories":"","description":"","excerpt":"Container Metrics Container-level metrics, such as CPU utilization or …","ref":"/leda/docs/device-provisioning/container-management/container-metrics/","tags":"","title":"Container Metrics"},{"body":" Dependency Type License URL poky OpenEmbedded Metalayer MIT https://www.yoctoproject.org/software-item/poky/ meta-virtualization OpenEmbedded Metalayer MIT https://git.yoctoproject.org/meta-virtualization meta-networking OpenEmbedded Metalayer MIT https://git.yoctoproject.org/meta-networking meta-rauc OpenEmbedded Metalayer MIT https://github.com/rauc/meta-rauc meta-openembedded OpenEmbedded Metalayer MIT https://git.openembedded.org/meta-openembedded meta-security OpenEmbedded Metalayer MIT https://git.yoctoproject.org/meta-security meta-rauc-community OpenEmbedded Metalayer MIT https://github.com/rauc/meta-rauc-community meta-raspberrypi OpenEmbedded Metalayer MIT https://git.yoctoproject.org/meta-raspberrypi ","categories":"","description":"","excerpt":" Dependency Type License URL poky OpenEmbedded Metalayer MIT …","ref":"/leda/docs/project-info/dependencies/","tags":"","title":"Dependencies"},{"body":"The device needs to be configured before it can make a connection to the cloud.\nThe following initial configuration steps are required:\nCreate a device in the cloud backend, such as Azure IoT Hub Configure authentication on device Configure credentials for accessing private container registries ","categories":"","description":"","excerpt":"The device needs to be configured before it can make a connection to …","ref":"/leda/docs/device-provisioning/","tags":"","title":"Device Provisioning"},{"body":"Run the full build To setup the environment and build the Leda image, please refer to: Setup development environment.\nRunning QEMU from existing build Use kas shell -c \"runqemu qemux86-64 ovmf kvm nographic\" \u003ckas-configs\u003e to execute the image. Replace qemux86-64 with one of the other qemu machines, such as qemuarm64 Use the keyword slirp to enable user-networking which does not require root privileges on the host. tun is default but requires setup on the host. Continue with Device Provisioning Variations of runqemu command line Use runqemu ovmf ovmf will enable the UEFI support for IA32 (x86) and X64 (x86-64) guests, for testing the dual-boot capabilities and SDV Self-Update mechanisms All other options are now part of the default Leda distribution configuration (see leda-qemu-settings.inc) Continue with Device Provisioning Running QEMU in the background To start QEMU in the background enter, use nohup and bring the process into the background.\nnohup runqemu qemux86-64 nographic qemuparams=\"-m 2048 -pidfile qemu.pid\" \u0026 The image is then reachable via ssh root@192.168.7.2 This will write a file qemu.pid in the current directory including the process ID of QEMU. Once done, kill -9 \u003cqemu.pid\u003e kills the process.\nRunning with kas-shell If you’ve chosen to build the Leda image with kas, you can use the kas-shell to run QEMU, with kas setting up the environment for you. To do that change to the main working directory and run:\nkas shell -c 'runqemu slirp nographic ovmf sdv-image-full' DHCP Configuration As the Leda Quickstart image will try to retrieve its IP address via DHCP broadcast request, it is good to run a DHCP daemon on the host, listening on the respective TAP network interface of QEMU. This will then simulate a LAN with DHCP server and let’s us control which IP address gets assigned to multiple QEMU instances.\nThe run-dhcp.sh utility will run an ISC-DHCP server on the host. The default configuration has a couple of MAC addresses preconfigured.\n","categories":"","description":"","excerpt":"Run the full build To setup the environment and build the Leda image, …","ref":"/leda/docs/build/run-build/","tags":"","title":"Run the build"},{"body":"Displays the current device configuration, such as Device ID.\nNote: Requires the Cloud Connector component to be configured and running.\nUsage:\nsdv-device-info Usage Synposis: ./sdv-device-info [options] [command]\nFull help:\nroot@qemux86-64:~# sdv-device-info --help sdv-device-info v0.2 Usage: /usr/bin/sdv-device-info [options] [command] Show SDV device configuration information Example: /usr/bin/sdv-device-info show Commands: show : Display configuration (default command) help : This message env : Format output for use in scripts Options: --ansi | -a : Don't use colored output. --norestart | -n : Do not automatically restart services --verbose | -v : Enable verbose mode. --help | -h : This message. Use in scripts To use device information on other scripts, it may be useful to source the device information variables into the current environment variable context:\nSynposis: source ./sdv-device-info env\nExample:\n$ source ./sdv-device-info env $ echo $DEVICE_ID exampledevice1 ","categories":"","description":"","excerpt":"Displays the current device configuration, such as Device ID.\nNote: …","ref":"/leda/docs/general-usage/utilities/sdv-device-info/","tags":"","title":"SDV Device Info"},{"body":"Eclipse Leda is using the Robot Framework for black box tests and system tests.\nThe black box tests are supposed to only use public API from SDV components, for example the MQTT interface of the Self Update Agent. The system tests are supposed to test on Leda Distro level and can use shell and SSH commands to verify system behavior, e.g. performing a reboot.\nTest Execution: an external trigger, such as the ./test-docker.sh shell script, starts the leda-tests container. Docker Compose ensures that the needed containers are built and started. The test cases and test resources are copied into the leda-tests container at build time. The Robot process is started and performs the execution of all test cases Black box test cases use the MQTT interface to connect to the test target and publish messages System level test cases use SSH to connect to the test target and execute commands Test reports are written to a mounted volume, so that they are available on the host for further processing Run the tests The easiest way to run the test cases is to run it in the Docker Compose setup:\nClone the leda-distro repository:\ngit clone https://github.com/eclipse-leda/leda-distro Optional: Build both images (qemuarm64 and qemux86-64) using kas / BitBake. If you omit this step, docker compose will download the latest container images from the Eclipse Leda Container Registry on ghcr.io.\nkas build kas/leda-qemux86-64.yml kas build kas/leda-qemuarm64.yml Switch to the docker-snapshot directory:\ncd resources/docker-snapshot/ Run the Leda Tests\n./test-docker.sh Test Reports The output of test-docker.sh will show the test results from Robot.\nThe test reports and debug logs are available on the host’s filesystem in the path resources/docker-snapshot/leda-tests-reports\noutput.xml - The main Robot output report report.html - A Robot HTML summary report leda-tests-xunit.xml - A xUnit report file suitable for rendering with various tools log.html - A Robot HTML report with the test execution log leda-tests-debug.log - Debug log file of the test execution, helpful during implementation of test cases and troubleshooting of failed tests The xunit report is being used to visualize the test execution results in the GitHub Workflow:\nExample Test Report:\nAdding new tests The tests are located in the following locations of the leda-distro repository:\nresources/docker-snapshot/dockerfiles/leda-tests - Robot Tests which are executed inside of a Docker Compose setup tests/src/robot - Robot Tests which can be executed on the build host with a Leda Runqemu instance running General steps are:\nDecide whether to implement a system-level test or a black-box integration test Add the test case to an existing, matching .robot file. If no matching test suite can be found, create a new .robot file. Prefix with the order number, e.g. 33__my-new-test.robot Check if a refactoring of new keywords may be worthwhile for better reusability. ","categories":"","description":"","excerpt":"Eclipse Leda is using the Robot Framework for black box tests and …","ref":"/leda/docs/build/tests/","tags":"","title":"Automated Tests"},{"body":"When deploying containerized applications, the container runtime will pull container images from a (remote) container registry.\nThe pulled container images and their layers are then stored in a local storage.\nPrivate Container Registries To be able to pull container images, the container runtime needs access to the container registry. Some container registries require authentication. The Kanto Container Manager can be configured to use credentials when accessing remote container registries.\nIn the Leda images, the sdv-kanto-ctl tools allows to easily add authentication to the container manager configuration:\nsdv-kanto-ctl add-registry -h \u003cregistryhostname\u003e -u \u003cyour_username\u003e -p \u003cyour_password\u003e For example, to access container images from GitHub Packages in a private repository, you need a GitHub Personal Access Token (PAT) with the read: packages scope. Then, add the repository as shown below:\nsdv-kanto-ctl add-registry -h ghcr.io -u github -p \u003cYour_GitHub_PersonalAccessToken\u003e sdv-kanto-ctl will make the necessary modifications to /etc/container-management/config.json and restarts the container-management.service systemd unit, so that the changes take effect. You may need to recreate or restart the container if a previous pull failed.\nPlease see the Eclipse Kanto Container Manager Configuration reference for details.\n","categories":"","description":"","excerpt":"When deploying containerized applications, the container runtime will …","ref":"/leda/docs/device-provisioning/container-management/container-registries/","tags":"","title":"Container Registries"},{"body":"In the Leda Robot Tests, keywords are used for reusable functionality. Common keywords are defined in resources/docker-snapshot/dockerfiles/leda-tests/leda_keywords.resource\nThe goal is to treat the Leda Quickstart image as a black box, utilizing as much as possible with public APIs.\nInteraction with Self Update Agent Trigger to start update: Send a “Desired State Request” to the target, to install a RAUC Update Bundle Connect and Subscribe to Listen: Wait for the asynchronous messages which indicate a successful installation of an update Arbitrary Commands Nevertheless, during implementation of test cases, it may be necessary to execute lower level processes for system level tests. For that, a fallback is possible to execute arbitrary test commands via remote SSH connection. These commands are executed through another docker container running in the same Docker network (leda-network) and allow access to the target QEMU instances:\nLeda Execute: Execute an arbitrary shell command via SSH on the test target ","categories":"","description":"","excerpt":"In the Leda Robot Tests, keywords are used for reusable functionality. …","ref":"/leda/docs/build/tests/robot-keywords/","tags":"","title":"Robot Keywords"},{"body":"Note: The Rust tests are being replaced with test cases implemented in Robot.\nCross Compiling to X86_64 on Ubuntu 20.04 There is currently a step to cross-compile tests to X86_64. In order to successfully run the step, you need to make sure that the following artifacts are available on the runner:\nrustc + cargo: curl https://sh.rustup.rs -sSf | sh docker: follow https://docs.docker.com/engine/install/ubuntu/ and afterwards https://docs.docker.com/engine/install/linux-postinstall/ build-essential: sudo apt-get install build-essential cross (0.1.16): cargo install cross --version 0.1.16 jq: sudo apt-get install jq -y You may restart your current shell so that all components are available as env vars.\n","categories":"","description":"","excerpt":"Note: The Rust tests are being replaced with test cases implemented in …","ref":"/leda/docs/build/tests/rust-tests/","tags":"","title":"Rust Tests"},{"body":"The provisioning helper script can be used to manually perform a device provisioning with a cloud backend.\nIt is meant as a convenient tool for developers, who wish to connect their device to a selfmanaged cloud backend directly. In a production environment, the device provisioning functionality is implemented either by the Cloud Connector component.\nThe provisioning script currently supports the following backend and authentication options:\nAzure IoT Hub Connection String Device Certificates Azure IoT Device Provisioning Service Device Certificates Usage The sdv-provision script is interactive and asks for the type of backend and authentication option:\nroot@qemux86-64:~# sdv-provision Checking Eclipse Leda Device Provisioning configuration... - Certificates directory exists Checking Device ID - Based on network device: eth0 - Device ID: 52-54-21-4c-f9-5a Checking whether either IdScope or ConnectionString is configured - Neither Id Scope file nor ConnectionString found, needs manual configuration Do you want to use the global Azure IoT Device Provisioning Service (DPS) by using an Id Scope, or do you want to use a direct connection to a specific Azure IoT Hub using a Connection String? d) Azure IoT Device Provisioning Service (DPS) with Id Scope h) Azure IoT Hub with Connection String Choose:d Please enter your Id Scope of the Azure IoT Device Provisioning Service:example Recreating the Cloud Connector container... Checking device certificates - All device certificates are present - Primary device certificate: /data/var/certificates/device.crt - Primary device private key: /data/var/certificates/device.key - Secondary device certificate: /data/var/certificates/device2.crt - Secondary device private key: /data/var/certificates/device2.key Fingerprints (add these to the Azure IoT Hub Device) - Primary thumbprint: 1B172ED3D06F4E25AFFEF675ADCE519457FFFFFF - Secondary thumbprint: B6CD5EACE96E9D0448BCB0BAED2DEE87AFFFFFFF Once a configuration has been selected, the script will:\nGenerate a random Device Id (based on physical network address) Store the Device Id in /etc/deviceid Generate a primary self-signed device certificate pair Generate a secondary self-signed device certificate pair Store the certificates in /data/var/certificates/ Reconfigure the container descriptor in /data/var/containers/manifests_dev/cloudconnector.json Restarting the Cloud Connector container Print the key fingerprints, used for onboarding the device in Azure IoT Device Provisioning Service Reconfiguration Note: Re-running the script will only print the existing configuration.\nTo reconfigure the device and use different options, perform the following steps:\nDelete the generated files\nrm /data/var/certificates/azure.idscope rm /data/var/certificates/azure.connectionstring rm /data/var/certificates/device*.crt rm /data/var/certificates/device*.key Rerun the script\nsdv-provision ","categories":"","description":"","excerpt":"The provisioning helper script can be used to manually perform a …","ref":"/leda/docs/general-usage/utilities/sdv-provision/","tags":"","title":"SDV Provision"},{"body":"This chapter contains use cases and examples for SDV applications.\nSome of these example are taken from the following sources:\nDigital.Auto and its playground Eclipse Velocitas tutorials Eclipse Kuksa.VAL examples ","categories":"","description":"","excerpt":"This chapter contains use cases and examples for SDV applications. …","ref":"/leda/docs/app-deployment/","tags":"","title":"Vehicle Applications"},{"body":"To generically interact with the Vehicle Signals which are managed in the Eclipse Kuksa.VAL Databroker, you can either use the natively installed databroker-cli command line tool, or use the CLI tool from an updated and released containerized image.\nKuksa Databroker CLI To install a new version of the Kuksa Databroker CLI container, follow these steps:\nCreate the container in Kanto:\nkanto-cm create --i --t --network=host --name=databroker-cli ghcr.io/eclipse/kuksa.val/databroker-cli:master Start the command line tool within a container in interactive mode:\nkanto-cm start --a --i --name databroker-cli In the databroker shell, connect to the databroker via the URL http://127.0.0.1:30555/:\nconnect http://127.0.0.1:30555/ Kuksa Client To install a new version of the Kuksa Client container, follow these steps:\nPull the container image\nctr --namespace kanto-cm image pull ghcr.io/eclipse/kuksa.val/kuksa-client:master Create the container in Kanto Namespace:\nctr --namespace kanto-cm container create --net-host --tty ghcr.io/eclipse/kuksa.val/kuksa-client:master kuksa-client Start the container in detached mode:\nctr --namespace kanto-cm tasks start --detach kuksa-client Start the command line tool with additional command line options:\nctr --namespace kanto-cm tasks exec --tty --exec-id sometask kuksa-client /kuksa-client/bin/kuksa-client --port 30555 --protocol grpc --insecure ","categories":"","description":"","excerpt":"To generically interact with the Vehicle Signals which are managed in …","ref":"/leda/docs/app-deployment/kuksa-databroker/","tags":"","title":"VSS \u0026 Kuksa.VAL"},{"body":"\n","categories":"","description":"","excerpt":"\n","ref":"/leda/docs/about/architecture/","tags":"","title":"Architecture"},{"body":"The cloud connector is used in the context of Device Provisioning and cloud backend connectivity.\nOverview Leda Cloud Connector for Azure IoT Hub is a fork (extended and adapted) of the generic Eclipse Kanto’s Azure connector that is being able to process cloud-to-device and device-to-cloud messages as defined for the Software-Defined Vehicle cloud backend.\nRuntime The SDV cloud connector will come up with pluggable architecture that will allow easy transformation of the incoming cloud-to-device command messages (SDV message envelope) to a format suitable and understandable by the rest of the in-vehicle components and vice-versa. It shall be possible to map SDV messages to and from Eclipse Hono and Eclipse Ditto messages using simple configuration, rules written in JSON; thus allowing this component to work together with other Eclipse Kanto components too.\nSource Repository Source Repository: https://github.com/eclipse-leda/leda-contrib-cloud-connector\nBuilding and Deployment The Cloud Connector can either be installed natively into the system image by using the respective Yocto recipe leda-contrib-cloud-connector_git.bb, or the cloud connector can be deployed as a container.\nNative Installation A native installation has the advantage that no additional container runtime is required. In some vehicle system architectures, there are separate devices for connectivity and for general computation. The actual physical device for connectivity may hence have less ressources available which then requires a native installation of such core components.\nAdd the recipe to the image in your Yocto configuration: IMAGE_INSTALL += \"leda-contrib-cloud-connector\" Override the configuration in /etc/cloud-connector/config.json Create the device certificate The cloud-connector can be managed (start, stop, restart) using systemd: systemctl restart cloud-connector Container Installation When the connectivity components can be deployed on the more generic compute module, where a container runtime is available, the cloud connector can also be deployed as a container. This is the default for the Eclipse Leda quickstart images.\nInitial deployment and configuration steps are:\nOn first start, the auto deployment will deploy and start the cloud connector container automatically, but with invalid default values or missing device certificates. The user needs to adapt the device authentication to suit his needs, e.g. creating and providing a device certificate. Check the /data/containers/cloud-connector.json deployment descriptor for the correct location of the certificate files. After this configuration has been done once, the container needs to be restarted using the Kanto CLI: kanto-cm restart -n cloud-connector ","categories":"","description":"","excerpt":"The cloud connector is used in the context of Device Provisioning and …","ref":"/leda/docs/leda-incubator/cloud-connector/","tags":"","title":"Cloud Connector"},{"body":"Thanks for considering to contribute to Eclipse Leda. We really appreciate the time and effort you want to spend helping to improve the project.\nIn order to get you started as fast as possible we need to go through some organizational issues first, though.\nEclipse Development Process This Eclipse Foundation open project is governed by the Eclipse Foundation Development Process and operates under the terms of the Eclipse IP Policy.\nhttps://eclipse.org/projects/dev_process https://www.eclipse.org/org/documents/Eclipse_IP_Policy.pdf Eclipse Contributor Agreement Before your contribution can be accepted by the project team contributors must electronically sign the Eclipse Contributor Agreement (ECA).\nhttp://www.eclipse.org/legal/ECA.php Commits that are provided by non-committers must have a Signed-off-by field in the footer indicating that the author is aware of the terms by which the contribution has been provided to the project. The non-committer must additionally have an Eclipse Foundation account and must have a signed Eclipse Contributor Agreement (ECA) on file.\nFor more information, please see the Eclipse Committer Handbook: https://www.eclipse.org/projects/handbook/#resources-commit\nMaking Your Changes Fork the repository on GitHub. Create a new branch for your changes. Note: When forking multiple repositories (eg most of the time, you also need to make modifications to meta-leda), please use the same branch name of each repository. Make your changes following the code style guide for the respective type of content: BitBake Recipes: https://www.openembedded.org/wiki/Styleguide Documentation: https://www.docsy.dev/docs/best-practices/ Shell Scripts (Example Style Guide): https://google.github.io/styleguide/shellguide.html When you create new files make sure you include a proper license header at the top of the file (see License Header section below). Make sure you include test cases for non-trivial features. Make sure the test suite passes after your changes. Commit your changes into that branch. Use descriptive and meaningful commit messages. Start the first line of the commit message with the a GitHub Issue number if available and a title e.g. [#9865] Add token based authentication. Squash multiple commits that are related to each other semantically into a single one. Make sure you use the -s flag when committing as explained above. Push your changes to your branch in your forked repository. Once you’re satisfied with your contribution, open a Pull Request and Eclipse Leda Committers will start with the review of your changes. Note: When working with multiple repositories, you need to open separate Pull Requests for each repository. Adding Documentation to Hugo Add the markdown document to the appropriate folder in the path leda/content/en. Add the front-matter --- title: \"title of the file\" date: 2022-05-09T13:43:25+05:30 --- Additional front matter that can be added – url : \"specifying a definite url to the file\" weight : 10 (used for ordering your content in lists. Lower weight gets higher precedence.) The images need to be put in path leda/static/assets. The image reference should be /assets/image.jpg in the markdown file. (Note: Do not use relative paths or url) In case you are creating a new folder, create _index.md file with the front matter only. Running Locally Install hugo version 0.98.0 extended Release v0.98.0 · gohugoio/hugo (github.com) Install Docsy theme in the path leda/themes/docsy – #Run this command from root directory of velocitas-docs git clone https://github.com/google/docsy.git themes/docsy Install pre-requisites cd themes/docsy/userguide/ npm install npm install --save-dev postcss From the leda directory run the command hugo server visit localhost:1313 to see the rendered static site. Submitting the Changes Submit a pull request via the normal GitHub UI.\nAfter Submitting Do not use your branch for any other development, otherwise further changes that you make will be visible in the PR. License Header Please make sure any file you newly create contains a proper license header like this:\n# /******************************************************************************** # * Copyright (c) 2022 Contributors to the Eclipse Foundation # * # * See the NOTICE file(s) distributed with this work for additional # * information regarding copyright ownership. # * # * This program and the accompanying materials are made available under the # * terms of the Apache License 2.0 which is available at # * https://www.apache.org/licenses/LICENSE-2.0 # * # * SPDX-License-Identifier: Apache-2.0 # ********************************************************************************/ You should, of course, adapt this header to use the specific mechanism for comments pertaining to the type of file you create.\nImportant\nPlease do not forget to add your name/organization to the LICENSE file’s Copyright Holders section. If this is not the first contribution you make, then simply update the time period contained in the copyright entry to use the year of your first contribution as the lower boundary and the current year as the upper boundary, e.g.\nCopyright 2017, 2018 ACME Corporation\nBuild On every PR merge a pipeline run will be triggered. This run will trigger the hugo docs build Hugo v0.98.0 extended is set up for the runner Docsy theme is setup for beautification of static site Then dependencies are installed for the theme Static site is generated and stored in a folder \"public\" The contents of public are committed to gh_pages branch which is exposed to host the github pages ","categories":"","description":"","excerpt":"Thanks for considering to contribute to Eclipse Leda. We really …","ref":"/leda/docs/project-info/contribution-guidelines/","tags":"","title":"Contribution Guidelines"},{"body":"The Vehicle Signal Specification has a lot of standard Vehicle Signals already defined.\nHowever, if you want to add additional custom signals or mappings from CAN-Bus for the Kuksa.VAL dbc2val feeder, the VSS data model needs to be customized and regenerated.\nThe following steps can be performed on a development machine to regenerate VSS data model with custom a vspec mapping.\nConcepts Setup workspace Defining the VSS Overlay Regenerate VSpec JSON Generate a candump Databroker and dbc2val containers Testing with data Concepts VSS Overlay The VSS Overlay concept allows to override the standard signals in the specification with custom signals. It defines metadata attributes like signal name, datatype, description etc.\nIn addition, the vspec file allows to add custom metadata, such as the dbc node in the following example:\nVehicle.Speed: type: sensor datatype: float dbc: message: Kombi_01 signal: KBI_angez_Geschw interval_ms: 1000 See Eclipse Kuksa documentation about dbc2val mapping for details.\nThere are also ways to automatically transform the CAN-Bus signal values into VSS values:\nMapping of literal values, such as enums and booleans Mathematical transformations Full and partial transformations Setup workspace Required tools Git Docker Python 3 can-utils a YAML/JSON text editor Depending on your host OS, you may need to install the packages first:\nsudo apt-get update sudo apt-get install can-utils git docker python3 Workspace structure You need the following files in your workspace folder:\n./custom.dbc - The vehicle-specific DBC file. You can use the Tesla Model 3 example or check out the opendbc project. ./custom.vspec - The VSS overlay from the previous step Defining the VSS Overlay The definition of an VSS overlay is a manual process. It requires research, browsing the existing VSS catalog and deep knowledge of the vehicle signals. Especially knowing the semantic of a signal and how to map it between DBC and VSS is a complex task.\nNote: The Leda team is not aware of any advanced tooling to support the automation of this task.\nCreate a new file custom.vspec. Use YAML format For each identified signal: Look up if there already is a corresponding signal in VSS. You may use the Digital.Auto Playground or the spec directly. Add a new top-level entry to the custom.vspec file. The key is the full VSS path name in dot notation. Add the corresponding dbc node Add the signal attribute and enter the name of the CAN-Bus signal name in the DBC Add additional attributes such as interval_ms, message or transform. Note: The attributes signal, interval_ms and transform are evaluated by dbc2val. The attribute message is only evaluated by the below demo script and is optional.\nExample:\n# CAN Message: Kombi_01 Vehicle.Speed: type: sensor datatype: float dbc: message: Kombi_01 signal: KBI_angez_Geschw interval_ms: 1000 # CAN Message: FS_Position_01 Vehicle.Cabin.Seat.Row1.Pos1.Position: type: sensor datatype: float dbc: message: FS_Position_01 signal: FSS_Laenge_Pos interval_ms: 1000 These two extensions will be added to the combined VSS tree:\nRegenerating VSpec The script vspec2json.py from COVESA VSS Tools can then be used to merge the overlay file with the standard specification.\nFor Leda devices, you can use kanto-cm to run the tool as a container:\n# Copy custom.dbc and custom.vspec to /home/root # Run in /home/root kanto-cm create --name vspec2json --rp no --mp=\"/home/root:/data\" ghcr.io/eclipse-leda/leda-vss-vspec2json:main kanto-cm start --a --i --name vspec2json For developer workstations, there is a convenient pre-built Docker container:\ndocker run --name vspec2json --rm -v `pwd`:/data ghcr.io/eclipse-leda/leda-vss-vspec2json:main Or, if you want to run vspec2json from the sources, clone the repository and invoke it like this:\ngit clone \\ --recurse-submodules \\ --branch v3.1 \\ --single-branch \\ --depth 1 \\ https://github.com/COVESA/vehicle_signal_specification python3 vehicle_signal_specification/vss-tools/vspec2json.py \\ -e dbc \\ -o custom.vspec \\ --uuid \\ --json-pretty \\ --json-all-extended-attributes \\ ../vehicle_signal_specification/spec/VehicleSignalSpecification.vspec \\ custom_vss_dbc.json With the resulting file custom_vss_dbc.json and the corresponding DBC file, the dbc2val feeder can relay the signals to the databroker. dbc2val can either read from a CAN-Dump file (generated by candump from can-utils) in an endless loop, or can listen on actual virtual or physical CAN interfaces.\nThe output should look like this:\n$ ls custom.dbc custom.vspec $ docker run --name vspec2json --rm -v `pwd`:/data ghcr.io/eclipse-leda/leda-vss-vspec2json:main INFO Output to json format INFO Known extended attributes: dbc INFO Added 54 units from /vss/spec/units.yaml INFO Loading vspec from /vss/spec/VehicleSignalSpecification.vspec... INFO Applying VSS overlay from /data/custom.vspec... INFO Autocreating implicit branch Vehicle INFO Autocreating implicit branch Cabin INFO Autocreating implicit branch Seat INFO Autocreating implicit branch Row1 INFO Autocreating implicit branch Pos1 INFO Calling exporter... INFO Generating JSON output... INFO Serializing pretty JSON... INFO All done. $ ls custom.dbc custom.vspec custom_vss_dbc.json Databroker Container Ensure the databroker container is up and running:\nOn developer workstation:\ndocker run --name databroker --detach --rm -p 55555:55555/tcp ghcr.io/eclipse/kuksa.val/databroker:0.3.1 On Leda device (if not already running)\nkanto-cm start --name databroker Testing with data To see actual data, we need to use the candump or another CAN source and feed that data into the databroker. After that is running, we can use the databroker-cli tool to subscribe to the example VSS signals.\nGenerate a candump To generate a candump for testing purposes, start the candump tool and listen for some packets.\nIn the following example, the first 20 CAN frames are being recorded and the command will exit.\ncandump -L -n 20 vcan0 \u003e candump.log If you do not have an external CAN source, you may want to simulate CAN frames. There is a Python library for parsing DBC files, which also let’s you encode and send the actual CAN messages.\nAn example script could look like this:\nNote: Due to GPL-3 licensing, the Leda quickstart image does not contain some Python standard libraries. This script will not run on a Leda quickstart image.\nimport random import time from pprint import pprint import yaml import cantools import can can_bus = can.interface.Bus('vcan0', bustype='socketcan') db = cantools.database.load_file('custom.dbc') with open(\"custom.vspec\", \"r\") as stream: try: vspec = yaml.safe_load(stream) for key in vspec: message_name=vspec[key]['dbc']['message'] signal_name=vspec[key]['dbc']['signal'] example_message = db.get_message_by_name(message_name) # Initialize an empty message data = {} for signal in example_message.signals: data[signal.name] = False pprint(\"Message: %s\" % example_message) # Send 10 frames with random data for x in range(10): data[signal_name] = random.uniform(0,100) encoded_data = example_message.encode(data) message = can.Message(arbitration_id=example_message.frame_id, data=encoded_data) pprint(\"CAN Frame: %s\" % message) can_bus.send(message) # Wait up to 1 second time.sleep(random.random()) except yaml.YAMLError as exc: print(exc) Example output:\n\"Message: message('Kombi_01', 0x30b, False, 8, {None: 'MQB'})\" ('CAN Frame: Timestamp: 0.000000 ID: 0000030b X Rx ' 'DL: 8 00 00 00 00 00 00 b9 00') ('CAN Frame: Timestamp: 0.000000 ID: 0000030b X Rx ' 'DL: 8 00 00 00 00 00 00 16 00') ('CAN Frame: Timestamp: 0.000000 ID: 0000030b X Rx ' 'DL: 8 00 00 00 00 00 00 ca 00') ('CAN Frame: Timestamp: 0.000000 ID: 0000030b X Rx ' 'DL: 8 00 00 00 00 00 00 ad 00') ('CAN Frame: Timestamp: 0.000000 ID: 0000030b X Rx ' 'DL: 8 00 00 00 00 00 00 b0 00') ('CAN Frame: Timestamp: 0.000000 ID: 0000030b X Rx ' 'DL: 8 00 00 00 00 00 00 79 00') ('CAN Frame: Timestamp: 0.000000 ID: 0000030b X Rx ' 'DL: 8 00 00 00 00 00 00 1b 01') ('CAN Frame: Timestamp: 0.000000 ID: 0000030b X Rx ' 'DL: 8 00 00 00 00 00 00 19 01') ('CAN Frame: Timestamp: 0.000000 ID: 0000030b X Rx ' 'DL: 8 00 00 00 00 00 00 0a 00') ('CAN Frame: Timestamp: 0.000000 ID: 0000030b X Rx ' 'DL: 8 00 00 00 00 00 00 b0 00') \"Message: message('FS_Position_01', 0x6be, False, 8, None)\" ('CAN Frame: Timestamp: 0.000000 ID: 000006be X Rx ' 'DL: 8 59 00 00 00 00 00 00 00') ('CAN Frame: Timestamp: 0.000000 ID: 000006be X Rx ' 'DL: 8 31 00 00 00 00 00 00 00') ('CAN Frame: Timestamp: 0.000000 ID: 000006be X Rx ' 'DL: 8 29 00 00 00 00 00 00 00') ('CAN Frame: Timestamp: 0.000000 ID: 000006be X Rx ' 'DL: 8 07 00 00 00 00 00 00 00') ('CAN Frame: Timestamp: 0.000000 ID: 000006be X Rx ' 'DL: 8 40 00 00 00 00 00 00 00') ('CAN Frame: Timestamp: 0.000000 ID: 000006be X Rx ' 'DL: 8 55 00 00 00 00 00 00 00') ('CAN Frame: Timestamp: 0.000000 ID: 000006be X Rx ' 'DL: 8 54 00 00 00 00 00 00 00') ('CAN Frame: Timestamp: 0.000000 ID: 000006be X Rx ' 'DL: 8 07 00 00 00 00 00 00 00') ('CAN Frame: Timestamp: 0.000000 ID: 000006be X Rx ' 'DL: 8 62 00 00 00 00 00 00 00') ('CAN Frame: Timestamp: 0.000000 ID: 000006be X Rx ' 'DL: 8 59 00 00 00 00 00 00 00') dbc2val on workstation Get the latest sources:\ngit clone https://github.com/eclipse/kuksa.val.feeders Looping a CAN dump recording:\nkuksa.val.feeders/dbc2val/dbcfeeder.py \\ --dumpfile candump.log \\ --canport vcan0 \\ --server-type kuksa_databroker \\ --dbcfile custom.dbc \\ --mapping custom_vss_dbc.json Listening on CAN interface:\n# CAN must be set up up front, e.g. # modprobe vcan # ip link add dev vcan0 type vcan # ip link set vcan0 up kuksa.val.feeders/dbc2val/dbcfeeder.py \\ --canport vcan0 \\ --use-socketcan \\ --server-type kuksa_databroker \\ --dbcfile /custom/custom.dbc \\ --mapping /custom/custom_vss_dbc.json dbc2val on Leda device To run the dbc2val feeder on Leda, you can use kanto-cm (instead of docker) to create the respective containers.\nNote: Due to Issue #73, the container ghcr.io/eclipse/kuksa.val.feeders/dbc2val:v0.1.1 cannot be used with SocketCAN. Please use a later release or build the container from the sources after fixing the bug. Alternatively, use the candump file instead of SocketCAN.\nRemove any existing dbc2val container:\nkanto-cm remove --force --name dbc2val Create the new container in Kanto Container Management:\nkanto-cm create \\ --name dbc2val \\ --mp=\"/home/root:/data\" \\ --e=VDB_ADDRESS=\"databroker:30555\" \\ --e=USECASE=\"databroker\" \\ --network=host \\ --privileged \\ ghcr.io/eclipse/kuksa.val.feeders/dbc2val:v0.1.1 \\ /dist/dbcfeeder-exe \\ --canport vcan0 \\ --use-socketcan \\ --usecase databroker \\ --dbcfile /data/custom.dbc \\ --mapping /data/custom_vss_dbc.json Note: The host networking and privileged is used to allow the container access to the CAN sockets.\nStart the container:\nkanto-cm start --a --i --name dbc2val Check the logs to verify container is running:\nkanto-cm logs --name dbc2val Querying via CLI To be able to see the changes, you may want to use the Kuksa Databroker CLI:\ndocker run --rm -it --network host ghcr.io/eclipse/kuksa.val/databroker-cli:0.3.1 Now, depending on the way how the dbc2val feeder has been started (candump vs. socketcan), you will either already see data from the candump loop.\n","categories":"","description":"","excerpt":"The Vehicle Signal Specification has a lot of standard Vehicle Signals …","ref":"/leda/docs/app-deployment/kuksa-databroker/custom-vss-mappings/","tags":"","title":"Custom Vehicle Signals"},{"body":"\nManually creating a Release triggers the release workflow. The release workflow calls the build workflow. The build workflow indirectly depends on the sstate cache being prebuilt manually (see optimizations below) The build workflow runs a full build of the SDV-Image-All disk image for all target machines. A separate job is used for each target machine, to ensure an image build for a target machine can finish within 6 hours. Each build contains the creation of SBOM artifacts and the check for CVEs. The SBOM artifacts are in SPDX JSON format and packaged per target machine’s disk image (SDV-Image-Full to include all packages). The OSS License Scanning (using the OSS Review Toolkit) is done asynchronously on a separate fork, as it currently uses a proprietary infrastructure. The ORT-based infrastructure of Eclipse is planned to be used in the future. The web report is attached as a build artifact on the internal fork and not accessible by public currently. Once the build workflow’s jobs are finished, the release workflow will finalize by attaching the release artifacts as assets to the release. Note: While the build workflow and release workflows are in progress, the GitHub release page of that release does not show any other assets besides the source archives. The release artifacts (eclipse-leda-.tar.xz) will only be visible once all workflows have finished.\nLimitations on standard runners As the GitHub-managed runners are optimized for ephemeral build use cases and a Yocto-based build process is very consuming in regards to CPU and disk capacity, a few optimizations need to be done before being able to run a full build or even a release workflow on limited GitHub-managed standard runners.\nPlease see the documentation about GitHub Hosted Runners for current specs.\nResource Standard GitHub Runner Recommended for Yocto CPU 2-core CPU (x86_64) 16-core CPU (x86_64) RAM 7 GB of RAM 16 GB of RAM Disk 14 GB of SSD 128+ GB of SSD Time max. 6 hours / job not limited In general, GitHub recommends to split a build process into smaller chunks, which can then fit into the constraints.\nOptimizations The following optimizations have been implemented for the Eclipse Leda public repository and its build workflow:\nRemote SState Cache: To minimize build time and disk usage, a remote sstate-cache mirror is being used. The mirror is hosted by one of the project sponsors on european Azure infrastructure and available as public HTTP mirror to anonymous Leda builds. The mirror is provided as best-effort, does not provide any kind of service level and may not be available at all times.\nNote: To use the mirror, set the BB_HASHSERVE, MIRROR_SERVER, SSTATE_MIRRORS and related configuration settings. See the mirrors.yaml for a full sample.\nPrebuilding: To fill the remote sstate cache mirror, another build infrastructure is being used. The repository fork has been configured with additional credentials to authenticate against the remote mirror for uploading the built packages. To ensure these steps are not run on the public OSS repository, the workflow steps use additional conditions to check for the owner of the repository. This is a workaround due to a known issue on GitHub Actions.\nChunking of the build steps: To minimize bandwidth transfer, a local GitHub Action Cache is being used. This cache is per target machine and filled with a separate workflow. The Prebuild sstate build jobs will run the BitBake process for 4 hours and then gracefully shut down. The build process will finish the current tasks. The remaining time (max. runtime for GitHub Runners is 6 hours) is used to package and upload the packages to the cache. If the 4 hours of build time are not enough, it may be required to re-run the same job more often.\nNote: The disadvantage of this approach is that each run requires a significant lead time where the remote cache is downloaded, the recipes get parsed again, the build task dependencies are compiled etc. On a persistent runner, this time can be spared.\nRerun on sstate miss: When BitBake is missing a package in the sstate mirror (it may exist in the Hash Equivalence Server though), BitBake will log an Error and continue to run the recipe. However, as the cache-miss is logged as error, BitBake will exit with an error code, indicating a failed build, which in turn would mark the GitHub Job as failed, too. To circumvent this problem, the BitBake build process is executed in a loop (max. 3 retries) to ensure that with the current state, all packages can be built without errors, eventually succeeding with the buid.\nAlways upload GitHub Cache: Under normal circumstances, the GitHub Cache action will update the cache on success of the build job - to not poison the cache with failed builds. However, as the Leda build workflows run for a very long time and may fail due to other reasons, the goal is to still reuse the sstate-cache as much as possible. For that reason, the pat-s/always-upload-cache GitHub action is being used, as it will also upload the cache on failed builds.\n","categories":"","description":"","excerpt":"\nManually creating a Release triggers the release workflow. The …","ref":"/leda/docs/build/github/","tags":"","title":"GitHub Workflow"},{"body":"Download eclipse-leda-logo.zip\nFor Screen White background 2182x794, white, transparent Black background 2182x794, black, opaque Credits: prothesis / kanellos @ 99designs.com | Font: https://www.dafont.com/olney.font\n916x916, black For Print File File Format Color model eclipse-leda-logo_01_cmyk.ai Adobe Illustrator CMYK, Black background eclipse-leda-logo_01_cmyk.eps Encapsulated PostScript CMYK, Black background eclipse-leda-logo_01_cmyk.pdf Portable Document Format CMYK, Black background eclipse-leda-logo_01_rgb.ai Adobe Illustrator RGB, Black background eclipse-leda-logo_01_rgb.jpg JPEG RGB, Black background, 2480x2480 eclipse-leda-logo_01_rgb.png PNG RGB, Black background, 2480x2480 eclipse-leda-logo_02_cmyk.ai Adobe Illustrator CMYK, Transparent background eclipse-leda-logo_02_cmyk.eps Encapsulated PostScript CMYK, Transparent background eclipse-leda-logo_02_cmyk.pdf Portable Document Format CMYK, Transparent background eclipse-leda-logo_02_rgb.ai Adobe Illustrator RGB, Transparent background eclipse-leda-logo_02_rgb.jpg JPEG RGB, White background, 2480x2480 eclipse-leda-logo_02_rgb.png PNG RGB, Transparent background, 2480x2480 eclipse-leda-logo_03_cmyk_cropped.svg Scalable Vector Graphics CMYK, Transparent background, Cropped eclipse-leda-logo_03_cmyk.svg Scalable Vector Graphics CMYK, Transparent background Colors Primary Color: #6daed1 Secondary Color: #804096 ","categories":"","description":"","excerpt":"Download eclipse-leda-logo.zip\nFor Screen White background 2182x794, …","ref":"/leda/docs/project-info/logo/","tags":"","title":"Project Logo"},{"body":"Meta-leda and the Leda-quickstart image provide a utility sdv-provision that simplifies the device provisioning procedure.\nConnecting to the device Since the procedure includes copying and pasting device IDs, it is recommended to connect to the device over a ssh connection. sdv-motd provides an easy way to find your device’s ip. More information on connecting via ssh can be found here.\nProvisioning via a connection string Device-side After connecting via ssh run sdv-provision.\nExample output:\nroot@qemux86-64:~# sdv-provision Checking Eclipse Leda Device Provisioning configuration... - Certificates directory exists Checking Device ID - Based on network device: eth0 - File does not exist, creating: /etc/deviceid - Device ID: XX-XX-XX-XX-XX-XX Checking whether either IdScope or ConnectionString is configured - Neither Id Scope file nor ConnectionString found, needs manual configuration Do you want to use the global Azure IoT Device Provisioning Service (DPS) by using an Id Scope, or do you want to use a direct connection to a specific Azure IoT Hub using a Connection String? d) Azure IoT Device Provisioning Service (DPS) with Id Scope h) Azure IoT Hub with Connection String Choose: Note the generated Device ID (XX-XX-XX-XX-XX-XX).\nType h /Azure IoT Hub with Connection String/ and press Enter.\nPaste your Azure IoT Hub Connection String: HostName=\u003cIoT Hub in Azure\u003e.azure-devices.net;DeviceId=\u003cXX-XX-XX-XX-XX-XX\u003e and press enter. Where \u003cIoT Hub in Azure\u003e is your Azure IoT Hub name.\nAzure Portal Go to https://portal.azure.com/ and to your Azure IoT Hub named \u003cIoT Hub in Azure\u003e. Choose Devices -\u003e Add Device. Enter the Device ID XX-XX-XX-XX-XX-XX generated on the previous step as Device ID. Pick X.509 Self-Signed and paste the two thumbprints generated by sdv-provision. Save. Device-Side After all of the above steps have been completed, connect back to your device and restart the cloudconnector container by running:\nkanto-cm stop -n cloudconnector --force kanto-cm start -n cloudconnector Or alternatively use:\n$ kantui And restart the container from the TUI.\n","categories":"","description":"","excerpt":"Meta-leda and the Leda-quickstart image provide a utility …","ref":"/leda/docs/device-provisioning/script-provisioning/","tags":"","title":"Provisioning with sdv-provision"},{"body":"QEMU’s command line option can get quiet complex. Yocto is providing a convenient wrapper script called runqemu, which takes configurations into account which have been populated during the build process and also takes care of TAP networking setup and teardown. Unfortunately, it can only be used within an OpenEmbedded build environment.\nRunning without runqemu: when you need more control over qemu options\nqemu-system-x86_64 -kernel .\\bzImage-qemux86-64.bin -nographic -m 2G -append \"console=ttyS0 root=/dev/vda1\" -drive file=.../sdv-image-full-qemux86-64.ext4 Running with runqemu: simple and some convenient flags are supported\nrunqemu ovmf sdv-image-full Running with leda: no options, runs the default settings only\nleda Enabling KVM for better performance The performance of qemux86_64 is much better when KVM can be used. Try running with:\nrunqemu ovmf kvm Note: You may need to set up KVM for your host machine first, please refer to How to enable KVM for Poky qemu\n/workspaces/leda-distro/build-sdv-x86_64 (imageformatcleanup ✗) $ ls -al /dev/kvm crw-rw---- 1 root kvm 10, 232 May 16 06:46 /dev/kvm ","categories":"","description":"","excerpt":"QEMU’s command line option can get quiet complex. Yocto is providing a …","ref":"/leda/docs/build/runqemu/","tags":"","title":"Runqemu"},{"body":"The sdv-motd script provides an alternative motd profile, which displays some additional information after login.\nThe script does not have any command line options.\nExample output:\n","categories":"","description":"","excerpt":"The sdv-motd script provides an alternative motd profile, which …","ref":"/leda/docs/general-usage/utilities/sdv-motd/","tags":"","title":"SDV MotD"},{"body":"The quickstart image contains the following utilities. These utility scripts are meant to be convenience tools for users and developers. They help to experiment with the container deployment, device provisioning or vehicle data access.\nsdv-health: Show SDV software components health status kantui: A text user interface for kanto-cm to manage containers (start, stop, logs, redeploy all) sdv-device-info: Show and update device information sdv-provision: Generate device certificates and configuration of cloud connection sdv-motd: Message-of-the-Day shown after login prompt can-forward: Forwarding a CAN-bus network interface into a containerized Vehicle Application kanto-auto-deployer: Automatically deploys containers on boot. Runs as a systemd service, and can also be invoked by a user directly. sdv-ctr-exec: Execute arbitrary commands in existing containers sdv-kanto-ctl: Manage the Kanto Container Management configuration via CLI Note: These scripts are not meant to be reused or called from production software. Their behaviour or command line syntax is not a stable API and should not be trusted for automation purposes. For details, please see leda-utils\n","categories":"","description":"","excerpt":"The quickstart image contains the following utilities. These utility …","ref":"/leda/docs/general-usage/utilities/","tags":"","title":"Utilities"},{"body":"The can-forward help script can be used to forward an existing CAN-Bus interface on the host system to a container process.\nNote: Warning! The script does not yet support Kanto (or containerd) as a container runtime. It has been implemented for k3s.\nUsage root@qemux86-64:~# can-forward --help Usage: /usr/bin/can-forward {-h} {-p PID} {-c container} \u003chw_can\u003e hw_can Host CAN hw interface to forward. Default: can0 -c container Attemmpt to get netns PID from a running container: (docker, ctr). Default: seat_service -p PID Use provided PID for transferring vxcan interface (e.g.: docker inspect -f '{{ .State.Pid }}' container) -h Prints this message The script performs the following steps:\nFind the process ID of the target container process Check and modprobe vxcan Check and modprobe can-gw Create a new virtual CAN interface (vxcanX) Create a new virtual CAN interface (vxcanX+1) Link both interfaces together Move the second interface (vxcanX+1) into the namespace of the target container Set up a bi-directional CAN-Bus packet forwarding between both interfaces using cangw ","categories":"","description":"","excerpt":"The can-forward help script can be used to forward an existing CAN-Bus …","ref":"/leda/docs/general-usage/utilities/can-forward/","tags":"","title":"CAN Forward"},{"body":"Automatically deploys containers to the Kanto Container Management based on deployment descriptors from a given path. All deployment descriptors in the manifests folder will be deployed (created and started) on startup of the service. The directory will then be monitored for creation of/changes to manifests and those changes will be redeployed.\nUsage Usage:\n$ kanto-auto-deployer --help kanto-auto-deployer 0.2.0 Automated deployment of Kanto Container Management Manifests USAGE: kanto-auto-deployer [OPTIONS] [MANIFESTS_PATH] ARGS: \u003cMANIFESTS_PATH\u003e Set the path to the directory containing the manifests [default: .] OPTIONS: -d, --daemon Run as a daemon that continuously monitors the provided path for changes -h, --help Print help information -s, --socket-cm \u003cSOCKET_CM\u003e Set the path to the Kanto Container Management API socket [default: /run/container-management/container-management.sock] -V, --version Print version information Example:\n# Use container manifests from current working directory root@qemux86-64:/data/var/containers/manifests# kanto-auto-deployer [2023-04-18T10:27:21Z INFO kanto_auto_deployer] Running initial deployment of \"/data/var/containers/manifests\" [2023-04-18T10:27:21Z INFO kanto_auto_deployer] Reading manifests from [/data/var/containers/manifests] [2023-04-18T10:27:21Z WARN kanto_auto_deployer::manifest_parser] Failed to load manifest directly. Will attempt auto-conversion from init-dir format. [2023-04-18T10:27:21Z INFO kanto_auto_deployer] Already exists [cloudconnector] [2023-04-18T10:27:21Z WARN kanto_auto_deployer::manifest_parser] Failed to load manifest directly. Will attempt auto-conversion from init-dir format. [2023-04-18T10:27:21Z INFO kanto_auto_deployer] Already exists [databroker] # Use container manifests from specified directory root@qemux86-64:~# kanto-auto-deployer /data/var/containers/manifests/ [2023-04-18T10:27:44Z INFO kanto_auto_deployer] Running initial deployment of \"/data/var/containers/manifests\" [2023-04-18T10:27:44Z INFO kanto_auto_deployer] Reading manifests from [/data/var/containers/manifests] [2023-04-18T10:27:44Z WARN kanto_auto_deployer::manifest_parser] Failed to load manifest directly. Will attempt auto-conversion from init-dir format. [2023-04-18T10:27:44Z INFO kanto_auto_deployer] Already exists [cloudconnector] [2023-04-18T10:27:44Z WARN kanto_auto_deployer::manifest_parser] Failed to load manifest directly. Will attempt auto-conversion from init-dir format. [2023-04-18T10:27:44Z INFO kanto_auto_deployer] Already exists [databroker] Nоte: The warnings from the manifest_parser module are normal and expected when the manifest is in the Container Management Manifests Format\nUsage as systemd service In the Leda quickstart images, kanto-auto-deployer is installed as a systemd service.\nNote the service uses the --daemon flag that asks KAD to continuously monitor the specified directory (see last line of logs).\nThe service unit configuration file is located in /lib/systemd/system/kanto-auto-deployer.service:\n[Unit] Description=Kanto Auto Deployer After=network-online.target container-management.service Wants=network-online.target container-management.service Requires=container-management.service [Install] WantedBy=multi-user.target [Service] Restart=on-failure RestartSec=5s ExecStart=/usr/bin/kanto-auto-deployer /data/var/containers/manifests --daemon Example output:\nroot@qemux86-64:/lib/systemd/system# systemctl status kanto-auto-deployer.service * kanto-auto-deployer.service - Kanto Auto Deployer Loaded: loaded (/lib/systemd/system/kanto-auto-deployer.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2023-04-18 10:22:10 UTC; 3min 55s ago Main PID: 525 (kanto-auto-depl) Tasks: 10 (limit: 4708) Memory: 1.4M CGroup: /system.slice/kanto-auto-deployer.service `- 525 /usr/bin/kanto-auto-deployer /data/var/containers/manifests --daemon Apr 18 10:22:48 qemux86-64 kanto-auto-deployer[525]: [2023-04-18T10:22:48Z INFO kanto_auto_deployer] Creating [sua] Apr 18 10:23:04 qemux86-64 kanto-auto-deployer[525]: [2023-04-18T10:23:04Z INFO kanto_auto_deployer] Created [sua] Apr 18 10:23:04 qemux86-64 kanto-auto-deployer[525]: [2023-04-18T10:23:04Z INFO kanto_auto_deployer] Starting [sua] Apr 18 10:23:05 qemux86-64 kanto-auto-deployer[525]: [2023-04-18T10:23:05Z INFO kanto_auto_deployer] Started [sua] Apr 18 10:23:05 qemux86-64 kanto-auto-deployer[525]: [2023-04-18T10:23:05Z WARN kanto_auto_deployer::manifest_parser] Failed to load manifest directly. Will attempt auto-conversion from init-dir format. Apr 18 10:23:05 qemux86-64 kanto-auto-deployer[525]: [2023-04-18T10:23:05Z INFO kanto_auto_deployer] Creating [vum] Apr 18 10:23:10 qemux86-64 kanto-auto-deployer[525]: [2023-04-18T10:23:10Z INFO kanto_auto_deployer] Created [vum] Apr 18 10:23:10 qemux86-64 kanto-auto-deployer[525]: [2023-04-18T10:23:10Z INFO kanto_auto_deployer] Starting [vum] Apr 18 10:23:11 qemux86-64 kanto-auto-deployer[525]: [2023-04-18T10:23:11Z INFO kanto_auto_deployer] Started [vum] Apr 18 10:23:11 qemux86-64 kanto-auto-deployer[525]: [2023-04-18T10:23:11Z INFO kanto_auto_deployer] Running in daemon mode. Continuously monitoring \"/data/var/containers/manifests\" ","categories":"","description":"","excerpt":"Automatically deploys containers to the Kanto Container Management …","ref":"/leda/docs/general-usage/utilities/kanto-auto-deployer/","tags":"","title":"Kanto Auto Deployer (KAD)"},{"body":"The Vehicle Update Manager delegates two different types of updates:\nThe Desired State on the container layer The Self Update on operating system layer Desired State The Desired State is applied at runtime on the container layer.\nThis type of update mechanism can update vehicle applications, vehicle services and other containers together with configuration resources or data files at runtime. If the applications support it, the rollout can also use high-availability strategies, such as rolling deployments.\nSelf Update The Self Update is applied on reboot of the device only.\nThis type of update mechanism is used for system-level updates which require the operating system to be rebooted to take effect.\n","categories":"","description":"","excerpt":"The Vehicle Update Manager delegates two different types of updates: …","ref":"/leda/docs/device-provisioning/vehicle-update-manager/","tags":"","title":"Vehicle Update Manager"},{"body":"The Leda Incubator project is a place for new software-defined-vehicle related software components which are in incubation state.\nThe goal of the Eclipse Leda Incubator is to foster collaboration in the Eclipse Software Defined Vehicle (SDV) ecosystem, and hopefully results in work proceeding to contribution by their original authors into upstream projects. The Eclipse Leda Incubator would not make any releases itself.\nThe project hosts software-defined vehicle components which are fitting into the general Eclipse Leda scope, but do not fit to either being part of Leda itself, which should be upstreamed into other projects in the future, or which are still highly experimental and should not be considered for any kind of production use.\n","categories":"","description":"","excerpt":"The Leda Incubator project is a place for new software-defined-vehicle …","ref":"/leda/docs/leda-incubator/","tags":"","title":"Incubator"},{"body":"The sdv-ctr-exec wrapper allows to execute arbitrary user commands in existing containers. Kanto Container Management cli tool (kanto-cm) only allows to manage the lifecycle of a container, but does not allow to specify or override the entrypoint or command definitions of an existing container. The ctr command line tool of containerd allows the execution of additional tasks in a running container.\nUsage As a convenient tool, sdv-ctr-exec allows the simple execution of arbitrary commands inside of containers. This is especially useful for non-service-containers, or containers which have additional binaries (e.g. cli tools) embedded.\nUsage:\nroot@qemux86-64:~# sdv-ctr-exec /usr/bin/sdv-ctr-exec -h to print this message Usage: /usr/bin/sdv-ctr-exec \u003ccontainer-id\u003e \u003ccommand\u003e or /usr/bin/sdv-ctr-exec -n \u003ccontainer-name\u003e \u003ccommand\u003e Example:\n# Executing a containerized cli tool using sdv-ctr-exec kanto-cm create --i --t --network=host --name=kuksa-client ghcr.io/eclipse/kuksa.val/kuksa-client:master kanto-cm start --name=kuksa-client sdv-ctr-exec -n kuksa-client /kuksa-client/bin/kuksa-client --port 30555 --protocol grpc --insecure Alternatives containerd: ctr The above commands are equivalent to the following commands:\n# Executing a containerized cli tool using ctr ctr --namespace kanto-cm image pull ghcr.io/eclipse/kuksa.val/kuksa-client:master ctr --namespace kanto-cm container create --net-host --tty ghcr.io/eclipse/kuksa.val/kuksa-client:master kuksa-client ctr --namespace kanto-cm tasks start --detach kuksa-client ctr --namespace kanto-cm tasks exec --tty --exec-id sometask kuksa-client /kuksa-client/bin/kuksa-client --port 30555 --protocol grpc --insecure containerd: nerdctl Note: nerdctl is currently not installed on the Leda Quickstart images.\ncontainerd: ctr and mounting To execute a binary natively (outside of a containerized environment), the container image may be mounted to the host filesystem using the ctr snapshots mount commands.\nThis approach only works if the binary is compatible with the host environment (dependencies, libraries etc.).\n$CONTAINER_IMAGE=\"ghcr.io/my-org/my-project/my-container\" ctr --namespace kanto-cm image pull $CONTAINER_IMAGE ctr --namespace kanto-cm container create --net-host --tty $CONTAINER_IMAGE my-container mkdir my-container ctr --namespace=kanto-cm snapshots mount my-container my-container | $SHELL cd my-container ./bin/my-application --help ","categories":"","description":"","excerpt":"The sdv-ctr-exec wrapper allows to execute arbitrary user commands in …","ref":"/leda/docs/general-usage/utilities/sdv-ctr-exec/","tags":"","title":"SDV Container Exec"},{"body":"In general, the self-update mechanism for operating system level updates is done with two separate partitions. While one partition is the actively booted partition and in use, the other partition can be updated by writing a partition image to it, as it is unused or inactive.\nOnce the download and writing is done, a reboot is triggered and the boot loader will now switch to the newly updated partition.\nIf the booting of the updated partition fails, the self update mechanism can revert back to the previous partition or boot to a rescue partition.\nAs updating the running operating system cannot be done at runtime, the approach requires additional disk space, a second partition and also requires the device to be rebooted for the updates to take effect.\nIn a vehicle, the self-updater cannot decide on its own when to do a reboot, as the vehicle must be in a safe condition (eg parked, state of charge etc.). Hence, the trigger for performaing the switch to another slot and a subsequent reboot is handed over to a higher level component, such as the vehicle update manager, which may in turn rely on driver feedback or other conditions.\nImplementation with RAUC Update Service RAUC is a lightweight update client that runs on your embedded device and reliably controls the procedure of updating your device with a new firmware revision.\nFor general usage of the RAUC tool, please see the RAUC User manual\nReference configuration The project contains an example reference implementation and configuration using RAUC, which allows the evaluation of the concepts, mechanisms and involved software components in an emulated, virtual environment.\nThe Leda quickstart image contains the following disk partitions:\na small rescue partition a full SDV installation with a container runtime, pre-loaded SDV container images and deployment specifications and additional developer tools such as nerdctl and kantui. a minimal SDV installation with a container runtime, but no additional examples or developer tools. This partition is used to demonstrate the self-update functionality. additional boot and data partitions for keeping system state information Note: All three rootfs partitions (rootfs) initially contain the same identical copies of the base operating system. Both SDV Root partitions will use the same shared data partition for the container persistent state.\n","categories":"","description":"","excerpt":"In general, the self-update mechanism for operating system level …","ref":"/leda/docs/device-provisioning/self-update/","tags":"","title":"Self Updates"},{"body":"Running BitBake to build your own images requires some extra setup on the build machine. Please see the following chapters for more information about the build process itself and how to setup a development and build infrastructure.\nIf you are interested to contribute or to get in touch with us, please see our Community pages and Contribution Guidelines.\nFor reporting security vulnerabilities, please follow our Security Policy.\n","categories":"","description":"","excerpt":"Running BitBake to build your own images requires some extra setup on …","ref":"/leda/docs/build/","tags":"","title":"Building Leda"},{"body":"","categories":"","description":"","excerpt":"","ref":"/leda/docs/build/misc/","tags":"","title":"Miscellaneous"},{"body":"Your privacy is important to us. The following Information is to provide you with all information relevant to data protection in order to be able to use the software, in a data protection compliant manner. It is provided as an information source for your solution-specific data protection and data privacy topics. This is not intended to provide and should not be relied on for legal advice.\nYour Role First things first: when you choose and use our software, you are most likely acting in the role of data controller, if personal related data is being processed. Therefore, you must ensure that the processing of personal data complies with the respective local legal requirements, e.g. when processing data within the scope of General Data Protection Regulation (GDPR) the legal requirements for a controller from the GDPR.\nWhere may the processing of personal related data be relevant? When using our software in combination with other software components, personal data or data categories may be collected for the purpose of developing, testing and running in-vehicle applications (Vehicle Apps). Possible examples are the vehicle identification number (VIN), the number plate, GPS data, video data, audio data, or other measurement data. You can determine which data or data categories are collected when configuring the software. These data are stored in volatile memory and are deleted by shutting down the system. You are responsible for the compliant handling of the data in accordance with the applicable local law.\nWhat have we done to make the software data protection friendly? This section describes the measures taken to integrate the requirements of the data protection directly into the software development. The technical measures described below follow a “privacy by design” approach.\nLocal data: The software may save data permanently in local virtual storage (eg when run in QEMU Emulator) or on local physical storage (SD-Card on Raspberry PI). All collected or processed data can be deleted by either deleting the virtual storage file (*.qcow2), or by erasing the SD-Card.\nCloud storage: The software may send data to cloud endpoints controlled by you or your organization. Examples include connectivity data, device identification, device health, device telemetry, application metrics and application logs. Collection and processing of example data on the device is enabled by default. Sending of device data to cloud endpoints must be explicitly enabled by performing the device provisioning process. The actual cloud endpoints are determined and configured during the device provisioning process. All collected or processed data can be deleted on the cloud side in the respective cloud endpoints.\nVulnerabilities: The release process for this software is set up to always update to the newest package updates. The project will continously release new versions of the software. To protect personal data, it is advisable to always use the latest version of the software.\nImportant: When you use the Eclipse Leda quickstart images for non-volatile setups, it is essential to reconfigure the system and harden it, this includes but is not limited to the following configuration items:\nDisable system user (root) password and login Disable SSH login with password Adding a new Linux user with restricted permissions Adding SSH key based authentication Container Resources and Configurations: Secrets, such as Device Identity Certificates for Cloud Connection and Access credentials for private Container Registries ","categories":"","description":"","excerpt":"Your privacy is important to us. The following Information is to …","ref":"/leda/docs/project-info/privacy-information/","tags":"","title":"Privacy Information"},{"body":"The Eclipse Leda project follows the Eclipse Release process. Please see Eclipse Project handbook for details about the process.\nThe release plans overview for Eclipse Leda is available at https://projects.eclipse.org/projects/automotive.leda/governance\n","categories":"","description":"","excerpt":"The Eclipse Leda project follows the Eclipse Release process. Please …","ref":"/leda/docs/about/releases/","tags":"","title":"Releases"},{"body":"When you want to reuse Leda for your custom setup or hardware we do not directly provide BSPs for (see Hardware) you can reference the following chapters. They describe the general approaches for different level of customization - from deploying different sets of containers from those provided by default to building completely custom distros based on meta-leda.\n","categories":"","description":"","excerpt":"When you want to reuse Leda for your custom setup or hardware we do …","ref":"/leda/docs/customization/","tags":"","title":"Customizing"},{"body":"This cheat sheet gives you an overview of common command line commands to interact with the tools available on the quickstart image.\nGeneral Commands Category Task Command General Overall info sdv-health Show device info sdv-device-info Device provisioning sdv-provision Switch Keyboard layout loadkeys de System System load htop Disk free df -h -t ext4 Memory free free -h Network Interfaces summary networkctl Ethernet status networkctl status enp0s2 Routing table route Active listeners netstat -l -n -t Multicast for Some/IP route add -net 224.0.0.0 netmask 240.0.0.0 dev eth0 Kanto-CM Show all containers kanto-cm list User interface kantui Service logs journalctl -f -l -t container-management Auto deployments See /data/var/containers/manifests/ Development deployments See /data/var/containers/manifests_dev/ Restart a container kanto-cm restart -n \u003ccontainername\u003e Manage config sdv-kanto-ctl Shell in container sdv-ctr-exec \u003ccontainerId\u003e \u003ccommand\u003e Add Private Registry sdv-kanto-ctl add-registry -h \u003chost\u003e -u \u003cuser\u003e -p \u003cpass\u003e ContainerD Show images ctr --address /data/run/containerd/containerd/containerd.sock --namespace=kanto-cm i ls Import local archive ctr --address /data/run/containerd/containerd.sock --namespace=kanto-cm i import \u003cdocker.tar\u003e Prune containers nerdctl system prune --all Mosquitto Show all messages mosquitto_sub -v -t '#' -h localhost Send message mosquitto_pub -t '\u003ctarget/topic\u003e' -h localhost -m '{\"foo\":\"bar\"}' Connectivity status mosquitto_rr --quiet -h localhost -t 'edge/thing/request' -e 'edge/thing/response' -m '' RAUC Self Update Current boot status rauc status Switch to other boot slot rauc status mark-active other CAN-Bus CAN Dump candump -l any,0:0,#FFFFFFFF Configuring NTP A failed download of container images may be caused by out-of-sync clock, thus failing the TLS certificate validation. A hardware device without hardware clock (such as the Raspberry Pi) relies on network time. In some (corporate) networks, there might be rogue NTP servers and it might be necessary to override the NTP sources.\nTo reset NTP server to a public server, follow these steps:\n# Check NTP clock sync timedatectl timesync-status # Add your NTP server vi /etc/systemd/timesyncd.conf # Restart container management systemctl restart container-management systemctl restart kanto-auto-deployer Reset container deployment When things go really wrong and the container runtime is unable deploy containers properly, it may be necessary to reset the internal state. This is achieved by stopping the container runtime(s) and deleting the /var/lib/container-management folder:\n# Stop all container runtimes systemctl stop container-management systemctl stop containerd # Delete the persistence of container management # Attention! Here be dragons rm -rf /var/lib/container-management # Restart the container runtimes systemctl start containerd systemctl start container-management # Redeploy the containers systemctl restart kanto-auto-deployer Running custom ad-hoc containers To install arbitrary containers, create the container using the kanto-cm command line tool. If it’s not a background service, but a cli tool, adding the --t --i options allows console access.\nroot@qemux86-64:~# kanto-cm remove --name python root@qemux86-64:~# kanto-cm create --name python --t --i --privileged docker.io/library/python:3.8.16-slim-bullseye bf9deca4-dbf1-4132-9ba7-e0f378bd34a7 root@qemux86-64:~# kanto-cm start --name python --a --i Python 3.8.16 (default, Jan 24 2023, 00:19:05) [GCC 10.2.1 20210110] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. \u003e\u003e\u003e quit() Raspberry Pi Overlays Edit /boot/config.txt and reboot:\n# Disable to fix non-working standard Raspberry Pi 7\" display # dtoverlay=vc4-kms-v3d # PiCAN 2 # dtoverlay=mcp2515-can0,oscillator=16000000,interrupt=25 # Waveshare RS485 CAN Hat # dtoverlay=mcp2515-can0,oscillator=12000000,interrupt=25,spimaxfrequency=2000000 ","categories":"","description":"","excerpt":"This cheat sheet gives you an overview of common command line commands …","ref":"/leda/docs/general-usage/cheatsheet/","tags":"","title":"Cheatsheet"},{"body":"Leda is using Kanto Container Management as the upper-layer container runtime and container orchestration engine.\nBesides the command line tool kanto-cm, Kanto also has remote interfaces to manage containers.\nRemote Interface Kanto’s container-management service offers a remote interface via local messaging (MQTT) to interact with a digital twin on the cloud side. This feature can be easily enabled by setting \"things\": { \"enabled\": true } in /etc/container-management/config.toml.\nWhen one of the cloud connector components, such as leda-contrib-cloud-connector or Kanto’s azure-connector, is connected to a cloud backend, the container-management will publish its own information using Eclipse Ditto and Eclipse Hono messages. For this, container-management only needs the device Id, gateway Id and tenant Id.\n","categories":"","description":"","excerpt":"Leda is using Kanto Container Management as the upper-layer container …","ref":"/leda/docs/device-provisioning/container-management/","tags":"","title":"Container Management"},{"body":"This project implements the Eclipse Foundation Security Policy\nhttps://www.eclipse.org/security Reporting a Vulnerability Please report vulnerabilities to the Eclipse Foundation Security Team at security@eclipse.org\nSupported Yocto Versions Version Supported Yocto 4.x (Kirkstone) Yes Yocto 3.4 (Honister) EOL Yocto 3.3 Untested Yocto \u003c 3.3 No Important: When you use the quickstart images for non-volatile setups, it is essential to reconfigure the system and harden it.\nConfiguration Items Disable system user (root) password and login Disable SSH login with password Adding a new Linux user with restricted permissions Adding SSH key based authentication Container Secrets Device identity certificates for cloud connection Access credentials for private container registries Device Identity for Cloud Connector Method Implementation Intended use Pre-Shared Symmetric Key Azure IoT Hub Connection String Development Certificates X.509 Certificates Production ","categories":"","description":"","excerpt":"This project implements the Eclipse Foundation Security Policy …","ref":"/leda/docs/project-info/security/","tags":"","title":"Security Policy"},{"body":"The self update agent is used in the context of OTA Self Updates.\nOverview The Self Update Agent (SUA) as part of the Eclipse Leda Incubator project proposal is a software component responsible for performing updates of system-level components of the connectivity device, such as\nBoot Loader Operating System Device Firmware Hardware Drivers … other parts of the system, which cannot be deployed as containerized packages or may require a reboot of the device. Implementation and Deployment SUA is using the RAUC framework via D-Bus calls, but it is designed in a way that switching to other updating solution shall be possible. SUA may be controlled by an external higher-level orchestration component via defined MQTT messages, which carry necessary for update data, such as version and URL of the update bundle. Update process feedback and the end result are also communicated via defined MQTT messages. Software Update Agent is implemented in C++ and configured to be running inside of a container.\nBuilding SUA can either be installed natively into the system image by using the respective Yocto recipe leda-contrib-self-update-agent_git.bb, or can be deployed as a container.\nNative Installation A native installation has the advantage that no additional container runtime is required. In some vehicle system architectures, there are separate devices for connectivity and for general computation. The actual physical device for connectivity may hence have less ressources available which then requires a native installation of such core components.\nAdd the recipe to the image in your Yocto configuration: IMAGE_INSTALL += \"leda-contrib-self-update-agent\" The self update agent can be managed (start, stop, restart) using systemd: systemctl restart self-update-agent Container Installation When the component can be deployed on the more generic compute module, where a container runtime is available, the self update agent can also be deployed as a container. This is the default for the Eclipse Leda quickstart images.\nInitial deployment and configuration steps are:\nOn first start, the auto deployment will deploy and start the self update agent container automatically. The self update agent will then connect to the locally running mosquitto server, awaiting update requests. The default configuration will use the location /data/selfupdates/ to write and read update files. Source Repository Source Repository: https://github.com/eclipse-leda/leda-contrib-self-update-agent\n","categories":"","description":"","excerpt":"The self update agent is used in the context of OTA Self Updates. …","ref":"/leda/docs/leda-incubator/self-update-agent/","tags":"","title":"Self Update Agent"},{"body":"For local deployment without a cloud backend, deployment specifications for Eclipse Velocitas Vehicle Apps may be put directly onto the device.\nCreate a Kanto deployment specification for your vehicle app. You may copy one of the existing JSON files in /data/var/containers/manifests as a template Copy the specification file to the device, e.g. scp -P 2222 myapp.json root@localhost:/data/var/containers/manifests/ Apply the specification: systemctl restart kanto-auto-deployer To update an existing container when the configuration has changed, delete the container and restart kanto-auto-deployer:\nkanto-cm remove -n myapp-example # Edit /data/var/containers/manifests/myapp.json kanto-auto-deployer Example Deployment Specification Attention: The current implementation requires all fields to be present in the JSON, even if the values are not set or used. Do not remove any fields, as it may break the functionality.\n{ \"id\": \"\", \"name\": \"myapp-example\", \"image\": { \"name\": \"ghcr.io/my-org/my-repo/my-app:latest\", \"decrypt_config\": null }, \"host_name\": \"\", \"domain_name\": \"\", \"resolv_conf_path\": \"\", \"hosts_path\": \"\", \"hostname_path\": \"\", \"mounts\": [], \"hooks\": [], \"host_config\": { \"devices\": [], \"network_mode\": \"bridge\", \"privileged\": false, \"restart_policy\": { \"maximum_retry_count\": 0, \"retry_timeout\": 0, \"type\": \"unless-stopped\" }, \"runtime\": \"io.containerd.runc.v2\", \"extra_hosts\": [], \"port_mappings\": [ { \"protocol\": \"tcp\", \"container_port\": 30151, \"host_ip\": \"localhost\", \"host_port\": 50151, \"host_port_end\": 50151 } ], \"log_config\": { \"driver_config\": { \"type\": \"json-file\", \"max_files\": 2, \"max_size\": \"100M\", \"root_dir\": \"\" }, \"mode_config\": { \"mode\": \"blocking\", \"max_buffer_size\": \"\" } }, \"resources\": null }, \"io_config\": { \"attach_stderr\": false, \"attach_stdin\": false, \"attach_stdout\": false, \"open_stdin\": false, \"stdin_once\": false, \"tty\": false }, \"config\": { \"env\": [ \"VEHICLEDATABROKER_DAPR_APP_ID=vehicledatabroker\" ], \"cmd\": [] }, \"network_settings\": null, \"state\": { \"pid\": -1, \"started_at\": \"\", \"error\": \"\", \"exit_code\": 0, \"finished_at\": \"\", \"exited\": false, \"dead\": false, \"restarting\": false, \"paused\": false, \"running\": false, \"status\": \"\", \"oom_killed\": false }, \"created\": \"\", \"manually_stopped\": false, \"restart_count\": 0 } ","categories":"","description":"","excerpt":"For local deployment without a cloud backend, deployment …","ref":"/leda/docs/app-deployment/velocitas/","tags":"","title":"Velocitas VApps"},{"body":"The vehicle update manager is used in the context of OTA Software Updates.\nOverview Vehicle Update Manager (VUM) is an extended version of the Eclipse Kanto Container Manager that is being able to handle new desired state for the software on the whole vehicle device.\nImplementation The desired state comes as a multi document YAML content and it includes a list of container resources:\nContainer Configuration VUM detects the system-level update custom resource and passes it for further processing to the Self Update Agent.\nVUM also monitors the self-update agent and the control plane, and compiles and report the current state of the device.\nSource Repository Source Repository: https://github.com/eclipse-leda/leda-contrib-vehicle-update-manager\n","categories":"","description":"","excerpt":"The vehicle update manager is used in the context of OTA Software …","ref":"/leda/docs/leda-incubator/vehicle-update-manager/","tags":"","title":"Vehicle Update Manager"},{"body":"\nIn order to verify that the device and the cloud connector can successfully receive messages sent by the cloud backend, we will send a dummy message to the device.\nPre-Requisites Device is up and running, e.g by running in qemu Eclipse Leda has successfully booted The device has been provisioned and configured, see Device Provisioning Validating configuration First, let’s check that the cloud connection is active.\nLogin as root\nRun sdv-health and check for the SDV Connectivity section:\nsdv-health Start watching the output of the cloud connector:\nkantui Select the cloud-connector container and press L to watch the logs\nNote: When an unknown type of message is received, the cloud connector will log an error:\n2022/04/13 16:04:41.911727 [agent] ERROR Handler returned error err=\"cannot deserialize cloud message: invalid character 'H' looking for beginning of value Start watching on the MQTT message broker:\nmosquitto_sub -h localhost -t '#' --pretty -v Note: When a known type of message is received, the cloud connector will forward the message to the MQTT broker into the corresponding topic $appId/$cmdName\nSending a Device Message Go to the Web Console of Azure IoT Hub Select the device Click on “Send Message” Enter a C2D payload and click “Send” Alternatively, on command line, use the Azure CLI client. Replace DeviceId and IotHubName with the appropriate names of your IoT Hub and device.\naz iot device c2d-message send \\ --device-id ${DeviceID} \\ --hub-name ${IotHubName} \\ --data 'Hello World' ","categories":"","description":"","excerpt":"\nIn order to verify that the device and the cloud connector can …","ref":"/leda/docs/app-deployment/cloud2device-messages/","tags":"","title":"Cloud2Device Messages"},{"body":"","categories":"","description":"","excerpt":"","ref":"/leda/docs/project-info/","tags":"","title":"Project Information"},{"body":"Versioning and Codenames Distribution Versioning is following the x.y.z syntax for the distribution, e.g. Leda 0.0.1 Build Versioning is following the git describe syntax: v0.0.1-blank-168-g7e14c4c Latest tag name: v0.0.1 Branch name: blank Number of commits behind: 168 Short Commit ID: g7e14c4c Codenames are taken from Wikipedia List of Motor Racing Tracks Initial codename is Hockenheim Current distribution version can be taken from /etc/issue: root@qemux86-64:~# cat /etc/issue Eclipse Leda v0.0.8 How to Release A new release is triggered manually from its GitHub web page Releases section. By clicking on the Draft new release button, the release process starts:\nSelect a branch to use as code base for the release Create a tag using the standard pattern vX.Y.Z Write a title Release X.Y.Z and release notes Optionally select if this is a pre-release Publish the release Validate the release With Publish the release action the release workflow located in .github/workflows/release.yml will be triggered. This will start building the distro image files for the supported platforms, running the test cases and generating reports as junit xml and license scanning. If the image generation and the test runs are successful the artifacts: images, test binaries and qa reports will be attached as assets to the release.\nThe build (build.yml) and release (release.yml) workflows share a common reusable workflow (base.yml). In this way the release workflow repeats the build actions without duplicating the code.\nDetailed build information on device Eclipse Leda Version root@qemux86-64:~# cat /etc/issue Eclipse Leda v0.0.8 Exact Build Timestamp root@qemux86-64:~# cat /etc/version 20220408135014 root@qemux86-64:~# cat /etc/timestamp 20220408135230 Details of Build Information root@qemux86-64:~# cat /etc/build ----------------------- Build Configuration: | ----------------------- DISTRO = leda DISTRO_VERSION = 2022 DATETIME = 20220408135014 DISTRO_NAME = Eclipse Leda IMAGE_BASENAME = core-image-minimal MACHINE = qemux86-64 TUNE_PKGARCH = core2-64 MACHINE_FEATURES = alsa bluetooth usbgadget screen vfat x86 pci rtc qemu-usermode DISTRO_FEATURES = acl alsa argp debuginfod ipv4 ipv6 largefile pcmcia usbgadget usbhost wifi xattr zeroconf pci vfat seccomp largefile ptest multiarch vulkan virtualization k8s seccomp raucg COMMON_FEATURES = IMAGE_FEATURES = debug-tweaks TUNE_FEATURES = m64 core2 TARGET_FPU = APP_URI_PREFIX = APP_URI_BRANCH = ----------------------- Layer Revisions: | ----------------------- meta = honister:ee68ae307fd951b9de6b31dc6713ea29186b7749 meta-poky = honister:ee68ae307fd951b9de6b31dc6713ea29186b7749 meta-yocto-bsp = honister:ee68ae307fd951b9de6b31dc6713ea29186b7749 meta-leda = main:30a5ff0a7e04dfa2c9b43175a49ac7a2ae0c64a9 -- modified meta-rauc = honister:3faf4cc4fcf558e99dad5aa8532fef2ecd566653 meta-filesystems = honister:061b7fc74f887454251307ef119b808a90654d3f meta-networking = honister:061b7fc74f887454251307ef119b808a90654d3f meta-oe = honister:061b7fc74f887454251307ef119b808a90654d3f meta-python = honister:061b7fc74f887454251307ef119b808a90654d3f meta-perl = honister:061b7fc74f887454251307ef119b808a90654d3f meta-virtualization = honister:bd7511c53b921c9ce4ba2fdb42778ca194ebc3e8 meta-security = honister:fb77606aef461910db4836bad94d75758cc2688c patch = main:a041dad5be9444d55491b57cb6a669a44196566d -- modified ","categories":"","description":"","excerpt":"Versioning and Codenames Distribution Versioning is following the …","ref":"/leda/docs/build/release/","tags":"","title":"Releasing"},{"body":"Manage the Kanto Container Management configuration via CLI.\nNote: Requires jq to be installed.\nFeatures:\nAdd and remove container registries (for authentication purposes) Set primitive values in configuration Restart container-management.service on configuration changes Automatically back up configuration file Display changes to user Usage Synposis: ./sdv-kanto-ctl \u003ccommand\u003e [\u003coptions\u003e]\nFull help:\n$ ./sdv-kanto-ctl --help Eclipse Kanto Container Manager Configuration Utility See https://websites.eclipseprojects.io/kanto/docs/references/containers/container-manager-config/ Usage: ./sdv-kanto-ctl \u003ccommand\u003e {options} Commands: add-registry -h \u003chostname\u003e -u \u003cusername\u003e -p \u003cpassword\u003e Adds or replaces a container registry authentication configuration -h or --hostname: Configure the hostname of the container registry (e.g. hub.docker.io, ghcr.io, ...) -u or --username: Configure the username -p or --password: Configure the password remove-registry -h \u003chostname\u003e Removes the specified container registry -h or --hostname: The hostname of the container registry remove-all-registries Removes all configured container registries list-registries Prints all configured container registries show-config Print the container management configuration set \u003ckey\u003e \u003cvalue\u003e Set a primitive configuration value. Key in JSON Dot-Notation Examples: ./sdv-kanto-ctl set containers.registry_configurations.MyRegistry.credentials.password foobar ./sdv-kanto-ctl set things.enable true Options: --no-reload : Do not reload the configuration and restart the container-management service automatically --ansi : Don't use colored output. --verbose | -v : Enable verbose mode. --help : This message. Example: Private Container Registries To be able to pull container images, the container runtime needs access to the container registry. Some container registries require authentication. The Kanto Container Manager can be configured to use credentials when accessing remote container registries.\nIn the Leda images, the sdv-kanto-ctl tools allows to easily add authentication to the container manager configuration:\nsdv-kanto-ctl add-registry -h \u003cregistryhostname\u003e -u \u003cyour_username\u003e -p \u003cyour_password\u003e For example, to access container images from GitHub Packages in a private repository, you need a GitHub Personal Access Token (PAT) with the read: packages scope. Then, add the repository as shown below:\nsdv-kanto-ctl add-registry -h ghcr.io -u github -p \u003cYour_GitHub_PersonalAccessToken\u003e sdv-kanto-ctl will make the necessary modifications to /etc/container-management/config.json and restarts the container-management.service systemd unit, so that the changes take effect. You may need to recreate or restart the container if a previous pull failed.\nPlease see the Eclipse Kanto Container Manager Configuration reference for details.\nExample: Enabling Things management Enable the container manager digital twin representation.\nsdv-kanto-ctl set things.enable true Example: Container Stop Timeout Kanto waits for a timeout before forcefully stopping a container. The default is 30 seconds in Kanto, and 10 seconds in the Leda quickstart image.\nTo change this behavior at runtime:\nsdv-kanto-ctl set manager.default_ctrs_stop_timeout 2 ","categories":"","description":"","excerpt":"Manage the Kanto Container Management configuration via CLI.\nNote: …","ref":"/leda/docs/general-usage/utilities/sdv-kanto-ctl/","tags":"","title":"SDV Kanto-Ctl"},{"body":"The Leda teams provides some custom utilities that allow for a more integrated end-user experience. They can be found in the main leda-utils repository on GitHub: eclipse-leda/leda-utils.\nThe following pages are meant to serve as both internal documentation and general guidelines when developing for Leda.\nBash Leda uses the classic Bourne shell as its main shell, thus all scripts should be sh-compatible (use the #!/bin/sh shebang). As a Poky+OE-based distro we use BusyBox for core-utils. To check explicitly for “bashisms” in your scripts, the checkbashisms tool might be useful.\nUtility-Specific Pages\nThe bash-based leda-utils are all deployed with the same recipe under the sdv-base packagegroup: meta-leda/meta-leda-components/recipes-sdv/sdv-base.\nRust The current main branch for leda-distro (and meta-leda) is based on the Kirkstone Yocto/Poky release.\nToolchain version The version of the Rust toolchain available in OE (Kirkstone) is 1.59.0. Make sure to target 1.59.0 (or earlier) when developing Rust-based utils for leda. To set 1.59.0 as your default Rust version on your development machine:\n$ rustup install 1.59.0 $ rustup default 1.59.0 $ cargo --version cargo 1.59.0 (49d8809dc 2022-02-10) Generating bitbake recipes with cargo-bitbake After making sure your toolchain is on version 1.59.0 go trough a clean build of your Rust binary:\n$ cd \u003crust_project_dir\u003e $ cargo clean $ rm Cargo.lock $ cargo build --release This will make sure the Cargo.lock is re-generated with packages matching the Rust version. The cargo.bbclass on which Rust recipes are based, requires all Rust crates + their version (matching the Cargo.toml) to be specified as a “SRC_URI +=”. This can become tedious and error-prone if done by hand. That’s why meta-rust provides a tool called cargo-bitbake that generates a minimal recipe with all the crates it finds in the Cargo.lock files of the project.\n$ cargo install --locked cargo-bitbake $ cd \u003crust_project_dir\u003e $ cargo bitbake This will generate a recipe in the project root which you can use as a starting point.\nExample: kantui_git.bb\nNote this recipe will only build your Rust crate. To deploy/install your binary you have to define a .inc file with the same name as the recipe that would handle the installation.\nExample: kantui.inc\nKnown bugs The built-in proc_macros crate is not being imported properly by meta-rust (Kirkstone), thus breaking all library crates that define a proc_macro (meta-rust issue 266). To fix this create a libstd-rs_%.bbappend file containing the single line:\nS = \"${RUSTSRC}/library/test\" meta-leda already provides this fix here, so it should not be necessary to implement it again.\n“Fat” Link time optimization LTO is a nice feature of LLVM that can optimize even through language boundaries at link-time, but leads to longer overall build times. That is why Rust by default uses “thin” LTO which may result in larger/slower binaries. “Fat” LTO can be enabled when building release binaries by adding to the Cargo.toml file the following section:\n[profile.release] lto = true For kantui this leads to reduction of the final binary size from ~8 MiB to ~5 MiB.\nMore information on profile-optimizations can be found here.\nNote: Stripping the debug symbols completely results in further binary size reduction, but BitBake fails with a QA problem when deploying stripped binaries.\nRust-based utilities Utility-Specific Pages\n","categories":"","description":"","excerpt":"The Leda teams provides some custom utilities that allow for a more …","ref":"/leda/docs/build/dev-and-maintenance/","tags":"","title":"Developing and Maintaining Utils"},{"body":"Frequently Asked Questions Why is the performance very slow? The performance of a system heavily relies on the infrastructure and the general setup.\nOn virtual setup (QEMU): Typical reasons for slow performance are software emulation, e.g. when you run an ARM-based image on an x86-based host machine. This requires QEMU to emulate the execution, which is very slow. When you run an x86-based image on an x86-based host machine, QEMU can leverage acceleration, such as KVM, which greatly improves the performance. For network performance issues, try to use TAP networking instead of SLIRP networking for QEMU. That requires more setup on the host though. On hardware, such as Raspberry Pi, it should be fast. If the system startup takes very long, you may have an outdated image, please update to the latest version. Another cause of performance loss can be networking issues, especially if the system does not have transparent internet access, network requests (e.g. DNS) may time out and slowing down the startup. On startup, the container runtime may download newer versions of containers and unpack them, which takes a considerable amount of time depending on the size of the container images. Why is my Bluetooth/Wifi driver missing? Please check the latest release, as we’re continously adding features. If your driver is still missing, please open a new GitHub Issue. We can add Kernel Modules for supported hardware. If you hardware is not supported out of the box by the Eclipse Leda project, you need to build your own image and customize the Linux Kernel configuration. Please see the Yocto Project documentation on how to do that.\nHow do I connect to a WiFi network? We use iwd/iwctl as high-level interfaces to manage wireless devices. For a guide on how to connect to a WiFi network with iwctl, you can check: Notes on using iwd/iwctl.\nHow can a container access CAN-Bus interfaces? As using a CAN-Bus requires access to the host network interface, the container needs to run in privileged mode and it needs access to the host network interface. For a Kanto-CM container, simply add privileged: true to the container configuration. A more sophisticated setup involves using virtual CAN interfaces for each container and using a CAN gateway to route the traffic accordingly. However, using CAN-Bus is not in the focus of SDV-style applications, as we assume that Vehicle Services would be running on dedicated ECUs, offering higher-level interfaces to Vehicle Applications on other protocols, such as Some/IP.\n\"host_config\": { ... \"network_mode\": \"host\", \"privileged\": true, ... } Why does booting from SD-Card sometimes fail and enters maintenance mode? This may depend on the quality or age of the SD-Card as well. Also, consumer-grade hardware like a Raspberry Pi may fail at times. Please try another SD-Card. Another reason may be high write operations to the SD-Card due to excessive data transfer (logging, container deployment etc.).\nHow can I install additional software in Leda? As we’re using Yocto, there is no central repository for software packages. Thus, we did not include any kind of package manager. If you feel that the software you want to install is valuable for other Leda users as well, please open a new GitHub Issue and request the installation of the software. If the software is specific to your use case, you need to build your own image using Yocto. Alternatively, you may build a container for your software and deploy the container into the container runtime. Using kanto-cm or ctr you would be able to execute your software as a container. See the Cheatsheet for an example.\n","categories":"","description":"","excerpt":"Frequently Asked Questions Why is the performance very slow? The …","ref":"/leda/docs/customization/faq/","tags":"","title":"FAQ"},{"body":"The release plan is available at https://projects.eclipse.org/projects/automotive.leda/releases/0.1.0\nRelease Date Thursday, June 1, 2023\nDeliverables The team plans to deliver the following features:\nLeda Quickstart Images for Qemu (x86_64, arm64), Raspberry Pi and Docker Yocto / OpenEmbedded meta-layer for better reusability of the Eclipse SDV Vehicle.Edge stack in customized distributions for constrained devices User documentation (Reference, Example Use Cases) Automated system tests using Robot Framework IP scanning with Eclipse Oniro Compliance Toolchain Pre-Integrated Eclipse SDV Vehicle.Edge stack The core SDV.EDGE stack in Leda would contain the following components for the first release (based upon availability):\nEclipse Kuksa.VAL: Data Broker Eclipse Kanto: Container Management, Vehicle Update Manager Eclipse Leda Incubator: Cloud Connector (Azure), Self Update Agent, Utilities (e.g kantui) The following components are not yet released in public or did not yet finished the project review, and may only be added to the release on shorter notice:\n(Eclipse Backend-Function-Bindings) (Eclipse SommR) (OTA Client) Compatibility Compatibility to previous versions is not considered for this first release.\nInternationalization No efforts towards i18n are done in this release.\nTarget Environments Linux (virtual, qemu) Raspberry Pi 4 (consumer grade) ","categories":"","description":"","excerpt":"The release plan is available at …","ref":"/leda/docs/about/releases/0.1.0/leda-0.1.0-plan/","tags":"","title":"Release Plan 0.1.0"},{"body":"Here we will describe the basic steps on how to identify and connect to a WiFi network in interactive or headless mode.\nInitial steps (identifying wlan interfaces) Start by running:\n$ ip a ... 3: wlan0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc fq_codel state DOWN group default qlen 1000 link/ether \u003cMAC\u003e brd ff:ff:ff:ff:ff:ff ... Note: Usually the name of the interface would be wlan0. It might however follow the newer “predictable interface names” standart. Then the interface name would start with wl* with the remaining characters identifying a physical bus/chip/number/etc corresponding to the interface. For simplicity we will wlan0 in all examples below.\nMost likely your wlan interface would be in the state DOWN. To bring it up:\n$ ip link set wlan0 up Scanning for wireless networks You can scan for wireless networks using either iw or wpa_cli.\n$ iw wlan0 scan Note: The output of this command can be quite verbose. You can grep for SSID to find the SSIDs of the networks around you.\n$ wpa_cli scan \u0026\u0026 wpa_cli scan_results The output of wpa_cli is cleaner and more readable so it is recommended to use that.\nConnecting to a network Once you have identified the SSID of your network you can connect to it by ussing the following commands:\n$ wpa_passphrase \u003cSSID\u003e \u003cPassphrase\u003e \u003e\u003e /etc/wpa_supplicant.conf $ systemctl restart wpa You should now be connected to the wireless network. To confirm that, you can once again use ip a to check that wlan0 has been configured with an IP.\nThis configuration shall be remembered from now on and your interface will try to connect automaticatically to that network whenever it is available.\nHeadless Wi-Fi configuration Similarly to the Raspbian, meta-leda provides a mechanism for headless configuration of Wi-Fi credentials for your device (Headless Raspberry Pi Setup).\nTo use this feature you have to prepapre your wpa_supplicant.conf ahead of time (check the reference above):\nctrl_interface=/var/run/wpa_supplicant ctrl_interface_group=0 update_config=1 network={ key_mgmt=NONE } network={ ssid=\"\u003cYOUR NETWORK NAME\u003e\" psk=\"\u003cYOUR NETWORK PASSWORD\u003e\" key_mgmt=WPA-PSK } IMPORTANT: It is recommended that if you are creating this configuration file on Windows to use an editor such as Notepad++ that can save files using the “Unix Line endings” (DOS vs. Unix Line Endings), or run it through a tool such as dos2unix that can convert between the two file formats. Otherwise wpa_supplicant.conf might not be read properly in the Linux image.\nOnce you have your wpa_supplicant.conf put your SD-card in your Desktop machine and place the configuration file in the root of the BOOT partion.\nWhen you boot your device it should now automatically connect to your Wi-Fi network.\n","categories":"","description":"","excerpt":"Here we will describe the basic steps on how to identify and connect …","ref":"/leda/docs/general-usage/wifi-configuration/connecting-to-wifi/","tags":"","title":"Connecting to Wi-Fi networks"},{"body":"\nThe Eclipse Leda project provides system image “recipes” to deliver a functional and always-available Linux-based image/distribution in the context of SDV (Software Defined Vehicle), by pulling together individual contributor pieces from Eclipse SDV and the larger OSS community.\nThe quickstart images help to learn how the SDV development, test and deployment lifecycle works from an E2E perspective, including the deployment of applications into the container runtimes on constrained embedded devices.\nThe ready images are also useful for quickly setting up showcases with virtual or real hardware devices.\nEclipse Leda provides a Poky-based reference build pipeline and an OpenEmbedded Metalayer meta-leda for integration into existing Yocto-based projects.\nUsage Download latest Eclipse Leda release Run Eclipse Leda on emulated Qemu devices or on Raspberry Pi 4 Configure device, e.g. provision the device Explore the device tools Develop your first Vehicle App using Eclipse Velocitas template Deploy a Vehicle App to the device Supported Machines / Build Configurations\nEmulated Qemu: x86-64, ARM64, ARM Raspberry Pi 4 Introduction Video Components Overview Features Publish/Subscribe infrastructure for local messaging and cloud connectivity Lightweight container runtime Vehicle Update Manager to orchestrate deployments of Vehicle Applications over the air (SOTA) Self Update Agent for firmware-over-the-air (FOTA) updates Example Vehicle Seat Service implementation Metrics and logs collector for Vehicle Apps See About - Features for more details about current implementation and About - Roadmap for our future work.\nLicense and Copyright This program and the accompanying materials are made available under the terms of the Apache License 2.0 which is available at https://www.apache.org/licenses/LICENSE-2.0\nFor details, please see our license NOTICE\n","categories":"","description":"","excerpt":"\nThe Eclipse Leda project provides system image “recipes” to deliver a …","ref":"/leda/docs/","tags":"","title":"Documentation"},{"body":" Download latest Eclipse Leda release Run Eclipse Leda on emulated Qemu devices or on Raspberry Pi 4 Configure device, e.g. provision the device Explore the device tools Develop your first Vehicle App using Eclipse Velocitas template Deploy a Vehicle App to the device ","categories":"","description":"","excerpt":" Download latest Eclipse Leda release Run Eclipse Leda on emulated …","ref":"/leda/docs/__shared/usage-overview/","tags":"","title":""},{"body":"","categories":"","description":"","excerpt":"","ref":"/leda/categories/","tags":"","title":"Categories"},{"body":" Eclipse Leda - Quickstart images for software-defined vehicle development Get started with Eclipse Leda ","categories":"","description":"","excerpt":" Eclipse Leda - Quickstart images for software-defined vehicle …","ref":"/leda/","tags":"","title":"Leda"},{"body":"","categories":"","description":"","excerpt":"","ref":"/leda/tags/","tags":"","title":"Tags"}]